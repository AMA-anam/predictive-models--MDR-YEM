{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsQ25jVkSLo1+JGeaFJwiI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMA-anam/predictive-models--MDR-YEM/blob/main/MDR_YEM_AMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import Libraries**\n",
        "\n",
        "Install and import all necessary Python libraries for data analysis, modeling, and interpretation."
      ],
      "metadata": {
        "id": "qSFZ_vnZi0fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO396YCvizv3"
      },
      "outputs": [],
      "source": [
        "!pip install optuna \\\n",
        "             numpy \\\n",
        "             scikit-learn==1.5 \\\n",
        "             umap-learn==0.5.7 \\\n",
        "             seaborn --upgrade \\\n",
        "             shap \\\n",
        "             lime \\\n",
        "             xgboost==1.6.1 \\\n",
        "             catboost \\\n",
        "             statsmodels \\\n",
        "             mlxtend \\\n",
        "             joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Data Splitting (Train / Validation / Test Sets)**  \n",
        "Split the dataset into training, validation, and test subsets to ensure unbiased evaluation"
      ],
      "metadata": {
        "id": "4eNMfBHxjKCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def fully_decoupled_split(data, target_column, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
        "\n",
        "    if target_column not in data.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame columns: {data.columns.tolist()}\")\n",
        "\n",
        "\n",
        "    y = data[target_column].values\n",
        "    X = data.drop(columns=[target_column]).values\n",
        "    feature_names = data.drop(columns=[target_column]).columns.tolist()\n",
        "\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y,\n",
        "        train_size=train_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    remaining_ratio = val_size / (val_size + test_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        train_size=remaining_ratio,\n",
        "        stratify=y_temp,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "    train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "    train_df[target_column] = y_train\n",
        "\n",
        "    val_df = pd.DataFrame(X_val, columns=feature_names)\n",
        "    val_df[target_column] = y_val\n",
        "\n",
        "    test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "    test_df[target_column] = y_test\n",
        "\n",
        "\n",
        "    assert train_df.index.tolist() == list(range(len(train_df)))\n",
        "    assert val_df.index.tolist() == list(range(len(val_df)))\n",
        "    assert test_df.index.tolist() == list(range(len(test_df)))\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "original_data = pd.read_csv(\"Original data.csv\")\n",
        "\n",
        "for col in ['Gender', 'Healthcare Sector', 'Institution Type', 'Bacteria type']:\n",
        "    print(f\"{col}:\")\n",
        "    if col in original_data.columns:\n",
        "        orig_dist = original_data[col].value_counts(normalize=True) * 100\n",
        "        for category, pct in orig_dist.items():\n",
        "            print(f\"  {category}: {pct:.2f}%\")\n",
        "    else:\n",
        "        print(\"  Column not found in data.\")\n",
        "\n",
        "\n",
        "train_df, val_df, test_df = fully_decoupled_split(\n",
        "    data=original_data,\n",
        "    target_column=\"MDR status\",\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_df.to_csv(\"train_set.csv\", index=False)\n",
        "val_df.to_csv(\"validation_set.csv\", index=False)\n",
        "test_df.to_csv(\"test_set.csv\", index=False)\n",
        "\n",
        "print(\"Split completed with complete index decoupling:\")\n",
        "print(f\"Train: {len(train_df)} rows, index: {train_df.index.tolist()[:5]}...\")\n",
        "print(f\"Val: {len(val_df)} rows, index: {val_df.index.tolist()[:5]}...\")\n",
        "print(f\"Test: {len(test_df)} rows, index: {test_df.index.tolist()[:5]}...\")"
      ],
      "metadata": {
        "id": "FFqqacrOjLok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Preprocessing & Model Performance (Training Set)**  \n",
        "Clean, encode, scale, and preprocess features, then train candidate models on the training set"
      ],
      "metadata": {
        "id": "STWVsVwIjO_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "import traceback\n",
        "import joblib\n",
        "import json\n",
        "import gc\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple, Callable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm, chi2, bootstrap, t\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import shap\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.exceptions import TrialPruned\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, recall_score, f1_score,\n",
        "    accuracy_score, cohen_kappa_score, matthews_corrcoef, brier_score_loss,\n",
        "    log_loss, roc_curve, precision_recall_curve, confusion_matrix,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from scipy.special import expit as sigmoid_func\n",
        "from scipy.special import expit\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from scipy.special import expit, logit\n",
        "\n",
        "\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels not available - some statistical functions will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    MULTITEST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MULTITEST_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.multitest not available - multiple comparison corrections will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.contingency_tables import mcnemar\n",
        "    MCNEMAR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MCNEMAR_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.contingency_tables not available - McNemar test will be limited\")\n",
        "\n",
        "try:\n",
        "    from mlxtend.evaluate import mcnemar_table\n",
        "    MLXTEND_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLXTEND_AVAILABLE = False\n",
        "    warnings.warn(\"mlxtend not available - McNemar table functionality will be limited\")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, brier_score_loss,\n",
        "    matthews_corrcoef, cohen_kappa_score, roc_curve,\n",
        "    precision_recall_curve, balanced_accuracy_score, log_loss, confusion_matrix\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"KMP_WARNINGS\"] = \"0\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "\n",
        "\n",
        "    random_state: int = 42\n",
        "    n_jobs_parallel: int = field(default_factory=lambda: int(os.cpu_count() or 4))\n",
        "\n",
        "\n",
        "    n_outer_folds: int = 5\n",
        "    n_inner_folds: int = 5\n",
        "\n",
        "    n_trials_optuna: int = 10\n",
        "    early_stopping_rounds: int = 30\n",
        "    eval_metric_objective: str = \"logloss\"\n",
        "\n",
        "\n",
        "    calibration_cv_folds: int = 5\n",
        "    calibration_method: str = 'sigmoid'\n",
        "    n_bins_calibration: int = 10\n",
        "\n",
        "    n_features_select: int = 10\n",
        "    shap_weight: float = 0.5\n",
        "    tree_weight: float = 0.5\n",
        "    max_samples_shap: int = 100\n",
        "\n",
        "\n",
        "    n_bootstrap: int = 1000\n",
        "    confidence_level: float = 0.95\n",
        "\n",
        "\n",
        "    train_csv_path: str = \"train_set.csv\"\n",
        "    target_column: str = \"MDR status\"\n",
        "    output_dir: Path = field(default_factory=lambda: Path(\"nested_cv_output\"))\n",
        "\n",
        "\n",
        "    plotting_settings: Dict[str, Any] = field(default_factory=lambda: {\n",
        "        'figsize': (3.5, 3.0),\n",
        "        'dpi': 450,\n",
        "        'style': 'seaborn-v0_8',\n",
        "        'colors': sns.color_palette(\"colorblind\").as_hex()\n",
        "    })\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "\n",
        "        output_base = self.output_dir / self.eval_metric_objective\n",
        "        object.__setattr__(self, 'output_dir', output_base)\n",
        "        output_base.mkdir(exist_ok=True, parents=True)\n",
        "        for sub in [\"models\", \"plots\", \"metrics\", \"logs\", \"best_params\"]:\n",
        "            (output_base / sub).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(cls) -> \"Config\":\n",
        "\n",
        "        cfg_dict = {}\n",
        "        for key, default_value in cls.__annotations__.items():\n",
        "            env_var = os.getenv(key.upper())\n",
        "            if env_var is not None:\n",
        "\n",
        "                if isinstance(default_value, type):\n",
        "                     if default_value is Path:\n",
        "                         cfg_dict[key] = Path(env_var)\n",
        "\n",
        "                elif isinstance(default_value, bool):\n",
        "                    cfg_dict[key] = env_var.lower() in {\"1\", \"true\", \"yes\"}\n",
        "                elif isinstance(default_value, int):\n",
        "                    cfg_dict[key] = int(env_var)\n",
        "                elif isinstance(default_value, float):\n",
        "                    cfg_dict[key] = float(env_var)\n",
        "                elif isinstance(default_value, str):\n",
        "                    cfg_dict[key] = env_var\n",
        "        return cls(**cfg_dict)\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "\n",
        "        return {\n",
        "            'random_state': self.random_state,\n",
        "            'optimization': {'inner_cv_folds': self.n_inner_folds, 'n_trials': self.n_trials_optuna, 'optuna_n_jobs': 1},\n",
        "            'feature_selection': {'n_features': self.n_features_select, 'shap_weight': self.shap_weight, 'tree_weight': self.tree_weight, 'max_samples_shap': self.max_samples_shap},\n",
        "            'evaluation': {'outer_cv_folds': self.n_outer_folds, 'calibration_cv_folds': self.calibration_cv_folds, 'calibration_method': self.calibration_method, 'n_bins': self.n_bins_calibration},\n",
        "            'early_stopping': {'rounds': self.early_stopping_rounds, 'metric': self.eval_metric_objective},\n",
        "            'output_dirs': {'models': self.output_dir / \"models\", 'plots': self.output_dir / \"plots\", 'metrics': self.output_dir / \"metrics\"},\n",
        "            'plotting': self.plotting_settings,\n",
        "            'data': {'train_csv': self.train_csv_path, 'target': self.target_column}\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def setup_logger(cfg: Config) -> logging.Logger:\n",
        "\n",
        "    log = logging.getLogger(\"nested_cv\")\n",
        "    if log.hasHandlers():\n",
        "        return log\n",
        "    log.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "    log_path = cfg.output_dir / \"logs\" / \"pipeline.log\"\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\"))\n",
        "    log.addHandler(fh)\n",
        "\n",
        "\n",
        "    ch = logging.StreamHandler(sys.stdout)\n",
        "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
        "    log.addHandler(ch)\n",
        "\n",
        "    return log\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_names(pre: ColumnTransformer, X: pd.DataFrame) -> List[str]:\n",
        "\n",
        "    names = []\n",
        "    for nm, pipe, cols in pre.transformers_:\n",
        "        if nm == 'cat':\n",
        "            try:\n",
        "\n",
        "                cats = pipe.named_steps['onehot'].categories_\n",
        "                for c, cl in zip(cols, cats):\n",
        "                    names += [f\"{c}_{v}\" for v in cl]\n",
        "            except Exception:\n",
        "                names += [f\"cat_{c}\" for c in cols]\n",
        "        elif nm == 'num':\n",
        "            names += list(cols)\n",
        "    return names\n",
        "\n",
        "def create_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
        "\n",
        "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    cat_pipe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "    num_pipe = Pipeline([('scale', StandardScaler())])\n",
        "\n",
        "    return ColumnTransformer([\n",
        "        ('cat', cat_pipe, cat_cols),\n",
        "        ('num', num_pipe, num_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "def _decision_to_proba(model: Any, X: pd.DataFrame) -> np.ndarray:\n",
        "\n",
        "    try:\n",
        "        scores = model.decision_function(X)\n",
        "        scores = np.asarray(scores).ravel()\n",
        "        return sigmoid_func(scores)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Could not get probabilities from decision_function: {e}\")\n",
        "        return model.predict(X).astype(float)\n",
        "\n",
        "def z_test_standard(y_true, y_prob):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = np.sum(y_true - y_prob)\n",
        "\n",
        "\n",
        "    denominator = np.sqrt(np.sum(y_prob * (1 - y_prob)))\n",
        "\n",
        "    if denominator < 1e-8:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    z_stat = numerator / denominator\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return z_stat, p_value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HybridFeatureSelector:\n",
        "\n",
        "    def __init__(self, cfg: Dict[str, Any]):\n",
        "        self.n_features = cfg['feature_selection']['n_features']\n",
        "        self.shap_weight = cfg['feature_selection']['shap_weight']\n",
        "        self.tree_weight = cfg['feature_selection']['tree_weight']\n",
        "        self.max_samples_shap = cfg['feature_selection']['max_samples_shap']\n",
        "        self.random_state = cfg['random_state']\n",
        "        self.selected_features_ = None\n",
        "\n",
        "    def _calculate_shap_importances(self, X: pd.DataFrame, model: Any) -> pd.Series:\n",
        "\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            X_samp = X.sample(min(self.max_samples_shap, len(X)), random_state=self.random_state)\n",
        "            X_samp = X_samp.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "            shap_vals = explainer.shap_values(X_samp)\n",
        "\n",
        "            if isinstance(shap_vals, list):\n",
        "                shap_arr = np.abs(shap_vals[1]) if len(shap_vals) > 1 else np.abs(shap_vals[0])\n",
        "            else:\n",
        "                shap_arr = np.abs(shap_vals)\n",
        "\n",
        "            return pd.Series(shap_arr.mean(axis=0), index=X_samp.columns)\n",
        "        except Exception:\n",
        "            logging.warning(\"SHAP calculation failed. Using uniform importances.\")\n",
        "            return pd.Series(1.0, index=X.columns)\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series, model: Any) -> 'HybridFeatureSelector':\n",
        "\n",
        "        if not hasattr(model, 'fit'):\n",
        "            raise ValueError(\"Model must be fitted before feature selection\")\n",
        "\n",
        "\n",
        "        tree_models_supporting_shap = (XGBClassifier, lgb.LGBMClassifier, CatBoostClassifier)\n",
        "        if isinstance(model, tree_models_supporting_shap) and hasattr(model, 'feature_importances_'):\n",
        "            shap_imp = self._calculate_shap_importances(X, model)\n",
        "        else:\n",
        "            shap_imp = pd.Series(1.0, index=X.columns)\n",
        "\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            tree_imp = pd.Series(model.feature_importances_, index=X.columns)\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            coef = model.coef_\n",
        "            if coef.ndim > 1: coef = coef[0]\n",
        "            tree_imp = pd.Series(np.abs(coef), index=X.columns)\n",
        "        else:\n",
        "            tree_imp = pd.Series(1.0, index=X.columns)\n",
        "\n",
        "\n",
        "        shap_imp, tree_imp = shap_imp.align(tree_imp, fill_value=0)\n",
        "\n",
        "        if shap_imp.sum() == 0 and tree_imp.sum() == 0:\n",
        "            combined = pd.Series(np.ones(len(X.columns)), index=X.columns)\n",
        "        else:\n",
        "            shap_rank = shap_imp.rank(ascending=False, method='min')\n",
        "            tree_rank = tree_imp.rank(ascending=False, method='min')\n",
        "            combined = self.shap_weight * shap_rank + self.tree_weight * tree_rank\n",
        "\n",
        "\n",
        "        self.selected_features_ = combined.nsmallest(self.n_features).index.tolist()\n",
        "        return self\n",
        "\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "        if self.selected_features_ is None:\n",
        "            raise ValueError(\"Selector not fitted yet. Call fit() first.\")\n",
        "\n",
        "\n",
        "        sorted_features = sorted(self.selected_features_)\n",
        "\n",
        "\n",
        "        return X[sorted_features]\n",
        "\n",
        "    def fit_transform(self, X: pd.DataFrame, y: pd.Series, model: Any) -> pd.DataFrame:\n",
        "\n",
        "        return self.fit(X, y, model).transform(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_ece(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "    \"\"\"\n",
        "    Robust Expected Calibration Error (ECE) calculation.\n",
        "    Handles duplicate bin edges and edge-case probabilities correctly.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    # Remove invalid probabilities (NaNs, Infs)\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    # Ensure probabilities are clipped to [0, 1]\n",
        "    y_prob = np.clip(y_prob, 0.0, 1.0)\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        # FIX 1: Remove duplicate edges to prevent zero-width bins\n",
        "        bin_edges = np.unique(bin_edges)\n",
        "    else:\n",
        "        # Uniform bins\n",
        "        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "\n",
        "    # Determine actual number of bins (may be less than n_bins if duplicates existed)\n",
        "    actual_n_bins = len(bin_edges) - 1\n",
        "    if actual_n_bins == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ece = 0.0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for i in range(actual_n_bins):\n",
        "        # FIX 2: Correct semi-open intervals [a, b)\n",
        "        if i == actual_n_bins - 1:\n",
        "            # Last bin includes the upper edge (inclusive)\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            # Other bins are exclusive on the right\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            ece += (n_in_bin / total_samples) * np.abs(avg_pred - avg_true)\n",
        "\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def calculate_mce(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "    \"\"\"\n",
        "    Robust Maximum Calibration Error (MCE) calculation.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    y_prob = np.clip(y_prob, 0.0, 1.0)\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        # FIX 1: Remove duplicate edges\n",
        "        bin_edges = np.unique(bin_edges)\n",
        "    else:\n",
        "        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "\n",
        "    actual_n_bins = len(bin_edges) - 1\n",
        "    if actual_n_bins == 0:\n",
        "        return 0.0\n",
        "\n",
        "    max_error = 0.0\n",
        "\n",
        "    for i in range(actual_n_bins):\n",
        "        # FIX 2: Correct semi-open intervals\n",
        "        if i == actual_n_bins - 1:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            error = abs(avg_pred - avg_true)\n",
        "            if error > max_error:\n",
        "                max_error = error\n",
        "\n",
        "    return float(max_error)\n",
        "\n",
        "def classification_extended_metrics(y_true, y_pred):\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
        "        'NPV': npv, 'PPV': ppv, 'FPR': fpr, 'FNR': fnr\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def calibration_slope_intercept(y_true, y_prob, method='logistic'):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob) | np.isnan(y_true))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_prob = np.clip(y_prob, 1e-6, 1 - 1e-6)\n",
        "    X_base = logit(y_prob) if method == 'logistic' else y_prob\n",
        "    X = sm.add_constant(X_base.reshape(-1, 1))\n",
        "\n",
        "    try:\n",
        "        mod = sm.GLM(y_true, X, family=sm.families.Binomial())\n",
        "        res = mod.fit(disp=0)\n",
        "        intercept, slope = float(res.params[0]), float(res.params[1])\n",
        "        return intercept, slope\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "def calibration_slope_intercept_ci(y_true, y_prob, n_bootstrap=100, alpha=0.05, seed=42):\n",
        "\n",
        "\n",
        "    intercept_point, slope_point = calibration_slope_intercept(y_true, y_prob)\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    slopes, intercepts = [], []\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        try:\n",
        "            indices = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "            if len(np.unique(y_true[indices])) < 2:\n",
        "                continue\n",
        "\n",
        "            int_b, slp_b = calibration_slope_intercept(y_true[indices], y_prob[indices])\n",
        "            if not (np.isnan(slp_b) or np.isnan(int_b)):\n",
        "                slopes.append(slp_b)\n",
        "                intercepts.append(int_b)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if slopes and intercepts:\n",
        "        slope_ci = (np.percentile(slopes, 100 * alpha/2), np.percentile(slopes, 100 * (1 - alpha/2)))\n",
        "        intercept_ci = (np.percentile(intercepts, 100 * alpha/2), np.percentile(intercepts, 100 * (1 - alpha/2)))\n",
        "    else:\n",
        "\n",
        "        slope_ci = (np.nan, np.nan)\n",
        "        intercept_ci = (np.nan, np.nan)\n",
        "\n",
        "    return intercept_point, slope_point, intercept_ci, slope_ci\n",
        "\n",
        "\n",
        "\n",
        "def ci95(mean, std, n):\n",
        "    if n < 2:\n",
        "        return np.nan, np.nan\n",
        "    tval = t.ppf(0.975, df=n-1)\n",
        "    err = tval * std / np.sqrt(n)\n",
        "    return mean - err, mean + err\n",
        "\n",
        "def cohens_d_from_summary(mean1, std1, mean2, std2):\n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
        "    if pooled_std == 0:\n",
        "        return np.nan\n",
        "    return (mean1 - mean2) / pooled_std\n",
        "\n",
        "def interpret_effect_size(d):\n",
        "    abs_d = abs(d)\n",
        "    if np.isnan(abs_d):\n",
        "        return \"Unknown\"\n",
        "    elif abs_d < 0.2:\n",
        "        return \"Negligible\"\n",
        "    elif abs_d < 0.5:\n",
        "        return \"Small\"\n",
        "    elif abs_d < 0.8:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Large\"\n",
        "\n",
        "def pairwise_pvalue(mean1, std1, n1, mean2, std2, n2):\n",
        "    se = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
        "    if se == 0:\n",
        "        return np.nan\n",
        "    t_stat = (mean1 - mean2) / se\n",
        "    df_num = (std1**2 / n1 + std2**2 / n2) ** 2\n",
        "    df_den = ((std1**2 / n1) ** 2) / (n1 - 1) + ((std2**2 / n2) ** 2) / (n2 - 1)\n",
        "    if df_den == 0:\n",
        "        return np.nan\n",
        "    df = df_num / df_den\n",
        "    p = 2 * t.sf(abs(t_stat), df)\n",
        "    return p\n",
        "\n",
        "\n",
        "\n",
        "def hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=10, min_expected_freq=5):\n",
        "\n",
        "\n",
        "    if not STATSMODELS_AVAILABLE:\n",
        "        warnings.warn(\"statsmodels not available - returning NaN for Hosmer-Lemeshow test\")\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 20 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
        "    try:\n",
        "        df['bin'] = pd.qcut(df['y_prob'], n_bins, labels=False, duplicates='drop')\n",
        "    except ValueError:\n",
        "        df['bin'] = np.floor(df['y_prob'] * n_bins).astype(int)\n",
        "        df.loc[df['bin'] == n_bins, 'bin'] = n_bins - 1\n",
        "\n",
        "\n",
        "    summary = df.groupby('bin').agg(\n",
        "        observed=('y_true', 'sum'),\n",
        "        expected=('y_prob', 'sum'),\n",
        "        n_total=('y_true', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    while True:\n",
        "        sparse_bins = summary[summary['expected'] < min_expected_freq]\n",
        "        if sparse_bins.empty or len(summary) <= 2:\n",
        "            break\n",
        "\n",
        "\n",
        "        merge_idx = sparse_bins.index[0]\n",
        "        if merge_idx == 0:\n",
        "            summary.loc[1, ['observed', 'expected', 'n_total']] += summary.loc[0, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(0).reset_index(drop=True)\n",
        "        else:\n",
        "            summary.loc[merge_idx - 1, ['observed', 'expected', 'n_total']] += summary.loc[merge_idx, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(merge_idx).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    g = len(summary)\n",
        "    summary['variance'] = summary['expected'] * (1 - summary['expected'] / summary['n_total'])\n",
        "    hl_statistic = ((summary['observed'] - summary['expected'])**2 / (summary['variance'] + 1e-8)).sum()\n",
        "\n",
        "\n",
        "    df_hl = g - 2\n",
        "    if df_hl <= 0:\n",
        "        return hl_statistic, np.nan\n",
        "\n",
        "    p_value = 1 - chi2.cdf(hl_statistic, df_hl)\n",
        "\n",
        "    return hl_statistic, p_value\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap_metrics(y_true, y_proba, y_pred):\n",
        "\n",
        "    METRIC_FN = {\n",
        "        'roc_auc_bs': roc_auc_score,\n",
        "        'pr_auc_bs': average_precision_score,\n",
        "        'recall_bs': recall_score,\n",
        "        'f1_bs': f1_score,\n",
        "        'accuracy_bs': accuracy_score,\n",
        "        'kappa_bs': cohen_kappa_score,\n",
        "        'mcc_bs': matthews_corrcoef,\n",
        "        'brier_bs': brier_score_loss,\n",
        "        'logloss_bs': log_loss\n",
        "    }\n",
        "\n",
        "    metrics = {}\n",
        "    try:\n",
        "        for name, fn in METRIC_FN.items():\n",
        "\n",
        "            if 'auc' in name or 'brier' in name or 'logloss' in name:\n",
        "                if len(np.unique(y_true)) > 1:\n",
        "                    metrics[name] = fn(y_true, y_proba)\n",
        "                else:\n",
        "                    metrics[name] = np.nan\n",
        "            else:\n",
        "                if len(np.unique(y_true)) > 1:\n",
        "                    metrics[name] = fn(y_true, y_pred)\n",
        "                else:\n",
        "                    metrics[name] = np.nan\n",
        "    except Exception as e:\n",
        "        print(f\"Error in bootstrap_metrics: {e}\")\n",
        "\n",
        "        for name in METRIC_FN.keys():\n",
        "            metrics[name] = np.nan\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "CALIBRATION_STATS_KEYS = [\n",
        "    'calibration_intercept', 'calibration_slope', 'z_statistic', 'z_p_value',\n",
        "    'hl_statistic', 'hl_p_value', 'calibration_intercept_ci_low',\n",
        "    'calibration_intercept_ci_high', 'calibration_slope_ci_low',\n",
        "    'calibration_slope_ci_high', 'ece_uniform', 'mce'\n",
        "]\n",
        "\n",
        "def collect_metrics(y_true, y_proba, y_pred):\n",
        "\n",
        "    res = {\n",
        "        'roc_auc': roc_auc_score(y_true, y_proba),\n",
        "        'pr_auc': average_precision_score(y_true, y_proba),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'kappa': cohen_kappa_score(y_true, y_pred),\n",
        "        'mcc': matthews_corrcoef(y_true, y_pred),\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'brier': brier_score_loss(y_true, y_proba),\n",
        "        'logloss': log_loss(y_true, y_proba),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "\n",
        "        n_bootstrap = CONFIG.get('n_bootstrap', 1000)\n",
        "        n_bins = CONFIG['evaluation']['n_bins']\n",
        "\n",
        "\n",
        "        res['ece_uniform'] = calculate_ece(y_true, y_proba, n_bins=n_bins, method='uniform')\n",
        "\n",
        "\n",
        "        res['mce_quantile'] = calculate_mce(y_true, y_proba, n_bins=n_bins, method='quantile')\n",
        "\n",
        "\n",
        "        intercept, slope, intercept_ci, slope_ci = calibration_slope_intercept_ci(\n",
        "            y_true, y_proba, n_bootstrap=n_bootstrap\n",
        "        )\n",
        "        res.update({\n",
        "            'calibration_intercept': intercept,\n",
        "            'calibration_slope': slope,\n",
        "            'calibration_intercept_ci_low': intercept_ci[0],\n",
        "            'calibration_intercept_ci_high': intercept_ci[1],\n",
        "            'calibration_slope_ci_low': slope_ci[0],\n",
        "            'calibration_slope_ci_high': slope_ci[1]\n",
        "        })\n",
        "\n",
        "        hl_stat, hl_p = hosmer_lemeshow_test_advanced(y_true, y_proba, n_bins=n_bins)\n",
        "        res['hl_statistic'] = hl_stat\n",
        "        res['hl_p_value'] = hl_p\n",
        "\n",
        "        z_stat, z_p = z_test_standard(y_true, y_proba)\n",
        "        res['z_statistic'] = z_stat\n",
        "        res['z_p_value'] = z_p\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Statistical calibration analysis failed: {e}. Filling with NaN.\")\n",
        "\n",
        "\n",
        "        for key in CALIBRATION_STATS_KEYS:\n",
        "            res[key] = np.nan\n",
        "\n",
        "\n",
        "    extended_metrics = classification_extended_metrics(y_true, y_pred)\n",
        "    res.update(extended_metrics)\n",
        "\n",
        "    return res\n",
        "\n",
        "def classification_extended_metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
        "        'NPV': npv, 'PPV': ppv, 'FPR': fpr, 'FNR': fnr\n",
        "    }\n",
        "\n",
        "def calibration_curve_fixed_bins(y_true, y_prob, n_bins=5):\n",
        "\n",
        "    edges = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
        "    idx = np.digitize(y_prob, edges) - 1\n",
        "    prob_true, prob_pred = np.zeros(n_bins), np.zeros(n_bins)\n",
        "    for b in range(n_bins):\n",
        "        m = idx == b\n",
        "        if m.any():\n",
        "            prob_true[b] = y_true[m].mean()\n",
        "            prob_pred[b] = y_prob[m].mean()\n",
        "        else:\n",
        "            prob_true[b] = np.nan\n",
        "            prob_pred[b] = 0.5 * (edges[b] + edges[b+1])\n",
        "    return prob_true, prob_pred\n",
        "\n",
        "\n",
        "\n",
        "def calculate_effect_sizes_and_tests(summaries, output_dir):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    models = [s['model'] for s in summaries]\n",
        "\n",
        "    numeric_cols = [k for k in summaries[0].keys() if k.endswith('_mean')]\n",
        "    metrics = [k[:-5] for k in numeric_cols]\n",
        "\n",
        "    stats_records = []\n",
        "    for s in summaries:\n",
        "        for metric in metrics:\n",
        "\n",
        "            mean_key = f\"{metric}_mean\"\n",
        "            std_key = f\"{metric}_std\"\n",
        "\n",
        "            if mean_key in s and std_key in s:\n",
        "                mean = s[mean_key]\n",
        "                std = s[std_key]\n",
        "                n_folds = s.get(\"n_folds\", CONFIG['evaluation']['outer_cv_folds'])\n",
        "                ci_lo, ci_hi = ci95(mean, std, n_folds)\n",
        "                stats_records.append({\n",
        "                    'Metric': metric, 'Model': s['model'], 'Mean': mean, 'Std': std,\n",
        "                    'N_Folds': n_folds, '95%CI_low': ci_lo, '95%CI_high': ci_hi\n",
        "                })\n",
        "            else:\n",
        "                logging.warning(f\"Missing mean or std for metric '{metric}' in model '{s['model']}'\")\n",
        "\n",
        "    stats_df = pd.DataFrame(stats_records)\n",
        "    stats_df.to_csv(os.path.join(output_dir, \"cv_metric_summary.csv\"), index=False)\n",
        "\n",
        "    pairwise_results = []\n",
        "    for metric in metrics:\n",
        "\n",
        "        metric_values = {s['model']: s.get(f\"{metric}_mean\", np.nan) for s in summaries}\n",
        "        metric_stds = {s['model']: s.get(f\"{metric}_std\", np.nan) for s in summaries}\n",
        "        metric_n_folds = {s['model']: s.get(\"n_folds\", CONFIG['evaluation']['outer_cv_folds']) for s in summaries}\n",
        "\n",
        "        for m1, m2 in combinations(models, 2):\n",
        "            mean1, std1, n1 = metric_values[m1], metric_stds[m1], metric_n_folds[m1]\n",
        "            mean2, std2, n2 = metric_values[m2], metric_stds[m2], metric_n_folds[m2]\n",
        "\n",
        "            d = cohens_d_from_summary(mean1, std1, mean2, std2)\n",
        "            pval = pairwise_pvalue(mean1, std1, n1, mean2, std2, n2)\n",
        "\n",
        "            pairwise_results.append({\n",
        "                \"Metric\": metric, \"Model_A\": m1, \"Model_B\": m2,\n",
        "                \"Diff\": mean1 - mean2 if not (np.isnan(mean1) or np.isnan(mean2)) else np.nan,\n",
        "                \"Cohens_d\": d,\n",
        "                \"Cohens_d_interpretation\": interpret_effect_size(d),\n",
        "                \"p_value\": pval\n",
        "            })\n",
        "\n",
        "    effect_df = pd.DataFrame(pairwise_results)\n",
        "    effect_df.to_csv(os.path.join(output_dir, \"cv_pairwise_effects_pvalues.csv\"), index=False)\n",
        "    print(f\"CV-based statistical analysis saved to {output_dir}/\")\n",
        "    return stats_df, effect_df\n",
        "\n",
        "def perform_statistical_tests(plot_data, threshold=0.5, auc_bootstrap_n=1000, verbose=True):\n",
        "\n",
        "    models = list(plot_data.keys())\n",
        "    results = {}\n",
        "    for i, model1 in enumerate(models):\n",
        "        for model2 in models[i+1:]:\n",
        "            try:\n",
        "                y1, p1 = plot_data[model1]['y'], plot_data[model1]['p']\n",
        "                y2, p2 = plot_data[model2]['y'], plot_data[model2]['p']\n",
        "                if not np.array_equal(y1, y2):\n",
        "                    if verbose: print(f\"Sample mismatch: {model1} vs {model2} skipped\")\n",
        "                    continue\n",
        "                auc_diff = roc_auc_score(y1, p1) - roc_auc_score(y1, p2)\n",
        "                auc_diff_pval = bootstrap_auc_difference_test(y1, p1, p2, n_bootstrap=auc_bootstrap_n)\n",
        "                pred1 = (p1 >= threshold).astype(int)\n",
        "                pred2 = (p2 >= threshold).astype(int)\n",
        "                mcnemar_pval = perform_mcnemar_test(y1, pred1, pred2)\n",
        "                results[f\"{model1}_vs_{model2}\"] = {\n",
        "                    'auc_difference': auc_diff,\n",
        "                    'auc_difference_pval': auc_diff_pval,\n",
        "                    'mcnemar_pval': mcnemar_pval\n",
        "                }\n",
        "            except KeyError as e:\n",
        "                if verbose: print(f\"KeyError for {model1} or {model2}: {e}\")\n",
        "                continue\n",
        "    return results\n",
        "\n",
        "def bootstrap_auc_difference_test(y_true, y_prob1, y_prob2, n_bootstrap=1000, random_state=42):\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    observed_diff = roc_auc_score(y_true, y_prob1) - roc_auc_score(y_true, y_prob2)\n",
        "    n = len(y_true)\n",
        "    bootstrap_diffs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.choice(n, n, replace=True)\n",
        "        y_boot, p1_boot, p2_boot = y_true[idx], y_prob1[idx], y_prob2[idx]\n",
        "        if len(np.unique(y_boot)) > 1:\n",
        "            diff = roc_auc_score(y_boot, p1_boot) - roc_auc_score(y_boot, p2_boot)\n",
        "            bootstrap_diffs.append(diff)\n",
        "    if not bootstrap_diffs: return np.nan\n",
        "    bootstrap_diffs = np.array(bootstrap_diffs)\n",
        "    p_value = 2 * min((bootstrap_diffs >= observed_diff).mean(), (bootstrap_diffs <= observed_diff).mean())\n",
        "    return p_value\n",
        "\n",
        "def perform_mcnemar_test(y_true, y_pred1, y_pred2):\n",
        "    if not MLXTEND_AVAILABLE:\n",
        "        logging.warning(\"mlxtend not available - using basic McNemar implementation\")\n",
        "        n01 = np.sum((y_pred1 == 0) & (y_pred2 == 1) & (y_true == y_pred2))\n",
        "        n10 = np.sum((y_pred1 == 1) & (y_pred2 == 0) & (y_true == y_pred1))\n",
        "        if n01 + n10 == 0:\n",
        "            return 1.0\n",
        "        statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
        "        from scipy.stats import chi2\n",
        "        return 1 - chi2.cdf(statistic, 1)\n",
        "\n",
        "    try:\n",
        "        table = mcnemar_table(y_target=y_true, y_model1=y_pred1, y_model2=y_pred2)\n",
        "        result = mcnemar(table, exact=False, correction=True)\n",
        "        return result.pvalue\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"McNemar test failed: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "def apply_multiple_comparisons_correction(test_results_df, p_value_columns=None, alpha=0.05, method='both'):\n",
        "\n",
        "    df_corrected = test_results_df.copy()\n",
        "    if p_value_columns is None:\n",
        "        p_value_columns = [col for col in df_corrected.columns\n",
        "                           if 'pval' in col.lower() or 'p_value' in col.lower()]\n",
        "    for col in p_value_columns:\n",
        "        p_values = df_corrected[col].dropna().values\n",
        "        valid_indices = df_corrected[col].notna()\n",
        "        if len(p_values) == 0:\n",
        "            continue\n",
        "        if method in ['bonferroni', 'both']:\n",
        "            alpha_bonferroni = alpha / len(p_values)\n",
        "            bonferroni_corrected = np.minimum(p_values * len(p_values), 1.0)\n",
        "            df_corrected.loc[valid_indices, f'{col}_bonferroni'] = bonferroni_corrected\n",
        "            df_corrected.loc[valid_indices, f'{col}_bonferroni_sig'] = bonferroni_corrected < alpha\n",
        "            df_corrected.loc[valid_indices, f'{col}_bonferroni_alpha'] = alpha_bonferroni\n",
        "        if method in ['fdr', 'both']:\n",
        "            _, pvals_fdr, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
        "            df_corrected.loc[valid_indices, f'{col}_fdr'] = pvals_fdr\n",
        "            df_corrected.loc[valid_indices, f'{col}_fdr_sig'] = pvals_fdr < alpha\n",
        "    return df_corrected\n",
        "\n",
        "def print_correction_summary(df_corrected, original_alpha=0.05):\n",
        "\n",
        "    print(\"\\n ðŸ“Š  Multiple Comparisons Correction Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    p_cols = [col for col in df_corrected.columns if 'pval' in col.lower() and not any(x in col for x in ['bonferroni', 'fdr'])]\n",
        "    for col in p_cols:\n",
        "        if col in df_corrected.columns:\n",
        "            original_sig = (df_corrected[col] < original_alpha).sum()\n",
        "            total_tests = df_corrected[col].notna().sum()\n",
        "            print(f\"\\n{col.upper()}:\")\n",
        "            print(f\"  Original significant results (p < {original_alpha}): {original_sig}/{total_tests}\")\n",
        "            if f'{col}_bonferroni_sig' in df_corrected.columns:\n",
        "                bonf_sig = df_corrected[f'{col}_bonferroni_sig'].sum()\n",
        "                bonf_alpha = df_corrected[f'{col}_bonferroni_alpha'].iloc[0] if f'{col}_bonferroni_alpha' in df_corrected.columns else 'N/A'\n",
        "                print(f\"  Bonferroni significant (Î± = {bonf_alpha:.4f}): {bonf_sig}/{total_tests}\")\n",
        "            if f'{col}_fdr_sig' in df_corrected.columns:\n",
        "                fdr_sig = df_corrected[f'{col}_fdr_sig'].sum()\n",
        "                print(f\"  FDR significant (Î± = {original_alpha}): {fdr_sig}/{total_tests}\")\n",
        "\n",
        "\n",
        "\n",
        "class HybridFeatureSelector:\n",
        "\n",
        "    def __init__(self, cfg: Dict[str, Any]):\n",
        "\n",
        "        self.n_features = cfg['feature_selection']['n_features']\n",
        "        self.shap_weight = cfg['feature_selection']['shap_weight']\n",
        "        self.tree_weight = cfg['feature_selection']['tree_weight']\n",
        "        self.max_samples_shap = cfg['feature_selection']['max_samples_shap']\n",
        "        self.random_state = cfg['random_state']\n",
        "        self.selected_features_ = None\n",
        "\n",
        "    def _calculate_shap_importances(self, X, model):\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            X_samp = X.sample(min(self.max_samples_shap, len(X)), random_state=CONFIG['random_state'])\n",
        "\n",
        "\n",
        "            X_samp = X_samp.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "            shap_vals = explainer.shap_values(X_samp)\n",
        "\n",
        "\n",
        "            if isinstance(shap_vals, list):\n",
        "                shap_arr = np.abs(shap_vals[1]) if len(shap_vals) > 1 else np.abs(shap_vals[0])\n",
        "            else:\n",
        "                shap_arr = np.abs(shap_vals)\n",
        "\n",
        "            return pd.Series(shap_arr.mean(axis=0), index=X_samp.columns)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"SHAP calculation failed: {e}. Using uniform importances.\")\n",
        "            logging.warning(traceback.format_exc())\n",
        "            return pd.Series(1.0, index=X.columns)\n",
        "\n",
        "    def fit(self, X, y, model):\n",
        "\n",
        "        if not hasattr(model, 'fit'):\n",
        "            raise ValueError(\"Model must be fitted before feature selection\")\n",
        "\n",
        "\n",
        "        tree_models_supporting_shap = (XGBClassifier, lgb.LGBMClassifier, CatBoostClassifier)\n",
        "        if isinstance(model, tree_models_supporting_shap) and hasattr(model, 'feature_importances_'):\n",
        "            shap_imp = self._calculate_shap_importances(X, model)\n",
        "        else:\n",
        "\n",
        "            shap_imp = pd.Series(1.0, index=X.columns)\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            tree_imp = pd.Series(model.feature_importances_, index=X.columns)\n",
        "        elif hasattr(model, 'coef_'):\n",
        "\n",
        "            coef = model.coef_\n",
        "            if coef.ndim > 1:\n",
        "                coef = coef[0]\n",
        "            tree_imp = pd.Series(np.abs(coef), index=X.columns)\n",
        "        else:\n",
        "\n",
        "            tree_imp = pd.Series(1.0, index=X.columns)\n",
        "\n",
        "\n",
        "        shap_imp, tree_imp = shap_imp.align(tree_imp, fill_value=0)\n",
        "\n",
        "\n",
        "        if shap_imp.sum() == 0 and tree_imp.sum() == 0:\n",
        "            combined = pd.Series(np.ones(len(X.columns)), index=X.columns)\n",
        "        else:\n",
        "\n",
        "            if shap_imp.sum() > 0:\n",
        "                shap_imp = shap_imp / shap_imp.sum()\n",
        "            if tree_imp.sum() > 0:\n",
        "                tree_imp = tree_imp / tree_imp.sum()\n",
        "\n",
        "\n",
        "            shap_rank = shap_imp.rank(ascending=False)\n",
        "            tree_rank = tree_imp.rank(ascending=False)\n",
        "            combined = self.shap_weight * shap_rank + self.tree_weight * tree_rank\n",
        "\n",
        "\n",
        "        self.selected_features_ = combined.nsmallest(self.n_features).index.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.selected_features_ is None:\n",
        "            raise ValueError(\"Selector not fitted yet. Call fit() first.\")\n",
        "        return X[self.selected_features_]\n",
        "\n",
        "    def fit_transform(self, X, y, model):\n",
        "        return self.fit(X, y, model).transform(X)\n",
        "\n",
        "\n",
        "\n",
        "class NestedCVPipeline:\n",
        "\n",
        "    def __init__(self, X: pd.DataFrame, y: pd.Series, cfg: Dict[str, Any]):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.cfg = cfg\n",
        "        self.models = {\n",
        "            'LogisticRegression': LogisticRegression,\n",
        "            'RandomForest': RandomForestClassifier,\n",
        "            'XGBoost': XGBClassifier,\n",
        "            'LightGBM': lgb.LGBMClassifier,\n",
        "            'CatBoost': CatBoostClassifier,\n",
        "            'SVM': SVC\n",
        "        }\n",
        "        self.log = setup_logger(Config.from_env())\n",
        "\n",
        "    def _get_model_params(self, model_name: str, trial: optuna.Trial) -> Dict[str, Any]:\n",
        "\n",
        "        params: Dict[str, Any] = {\"random_state\": self.cfg['random_state']}\n",
        "        is_boosting_model = model_name in ['XGBoost', 'LightGBM', 'CatBoost']\n",
        "\n",
        "        if is_boosting_model:\n",
        "            param_name = 'iterations' if model_name == 'CatBoost' else 'n_estimators'\n",
        "            params[param_name] = 2000\n",
        "            params['early_stopping_rounds'] = trial.suggest_int('early_stopping_rounds', 10, self.cfg['early_stopping']['rounds'])\n",
        "\n",
        "        if model_name == 'XGBoost':\n",
        "            params.update({\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
        "                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
        "                'n_jobs': 1,\n",
        "                'random_state': self.cfg['random_state'],\n",
        "                'enable_categorical': False,\n",
        "                'tree_method': 'hist',\n",
        "                'verbosity': 0\n",
        "            })\n",
        "        elif model_name == 'RandomForest':\n",
        "            params.update({\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "                'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
        "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                'n_jobs': 1, 'random_state': self.cfg['random_state']\n",
        "            })\n",
        "        elif model_name == 'LogisticRegression':\n",
        "            penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
        "            solver = trial.suggest_categorical(\n",
        "                'solver', ['liblinear', 'saga', 'lbfgs', 'newton-cg', 'sag']\n",
        "            )\n",
        "            if (\n",
        "                (penalty == 'l1' and solver not in ['liblinear', 'saga']) or\n",
        "                (penalty == 'l2' and solver not in ['liblinear', 'saga', 'lbfgs', 'newton-cg', 'sag']) or\n",
        "                (penalty == 'elasticnet' and solver != 'saga')\n",
        "            ):\n",
        "                raise optuna.TrialPruned()\n",
        "            params.update({\n",
        "                'C': trial.suggest_float('C', 0.001, 100.0, log=True),\n",
        "                'penalty': penalty, 'solver': solver,\n",
        "                'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
        "                'max_iter': 2000, 'random_state': CONFIG['random_state']\n",
        "            })\n",
        "            if penalty == 'elasticnet':\n",
        "                params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
        "        elif model_name == 'CatBoost':\n",
        "            bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli'])\n",
        "            params.update({\n",
        "                'depth': trial.suggest_int('depth', 3, 10),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
        "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 1000, log=True),\n",
        "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
        "                'bootstrap_type': bootstrap_type,\n",
        "                'auto_class_weights': trial.suggest_categorical('auto_class_weights', [None, 'Balanced']),\n",
        "                'border_count': trial.suggest_int('border_count', 32, 128),\n",
        "                'random_state': self.cfg['random_state'], 'logging_level': 'Silent'\n",
        "            })\n",
        "            if bootstrap_type == 'Bernoulli':\n",
        "                params['subsample'] = trial.suggest_float('subsample', 0.6, 1.0)\n",
        "            elif bootstrap_type == 'Bayesian':\n",
        "                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 1.0)\n",
        "\n",
        "        elif model_name == 'SVM':\n",
        "            kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
        "            params.update({\n",
        "                'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n",
        "                'kernel': kernel, 'probability': True,\n",
        "                'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
        "                'random_state': self.cfg['random_state']\n",
        "            })\n",
        "            if kernel == 'rbf':\n",
        "                params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "        elif model_name == 'LightGBM':\n",
        "            params.update({\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
        "                'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
        "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
        "                'n_jobs': 1, 'random_state': self.cfg['random_state'], 'verbosity': -1\n",
        "            })\n",
        "        return params\n",
        "\n",
        "    def _create_model(self, model_name, params):\n",
        "        return self.models[model_name](**params)\n",
        "\n",
        "\n",
        "\n",
        "    def _preprocess_to_df(self, pre: ColumnTransformer, X_df_raw: pd.DataFrame, feat_names: List[str]) -> pd.DataFrame:\n",
        "\n",
        "        X_tx = pre.transform(X_df_raw)\n",
        "\n",
        "        if hasattr(X_tx, 'toarray'):\n",
        "             X_tx = X_tx.toarray()\n",
        "\n",
        "\n",
        "        current_cols_count = X_tx.shape[1]\n",
        "\n",
        "\n",
        "        if current_cols_count != len(feat_names):\n",
        "             temp_cols = [f'f{i}' for i in range(current_cols_count)]\n",
        "             temp_df = pd.DataFrame(X_tx, columns=temp_cols, index=X_df_raw.index)\n",
        "\n",
        "\n",
        "             try:\n",
        "                 temp_df = pd.DataFrame(X_tx, columns=feat_names, index=X_df_raw.index)\n",
        "             except ValueError:\n",
        "\n",
        "                 placeholder_cols = [f'_col_{i}' for i in range(current_cols_count)]\n",
        "                 temp_df = pd.DataFrame(X_tx, columns=placeholder_cols, index=X_df_raw.index)\n",
        "\n",
        "                 aligned_df = temp_df.reindex(columns=feat_names, fill_value=0.0)\n",
        "                 return aligned_df\n",
        "\n",
        "             return temp_df\n",
        "        else:\n",
        "\n",
        "             return pd.DataFrame(X_tx, columns=feat_names, index=X_df_raw.index)\n",
        "\n",
        "\n",
        "\n",
        "    def _preprocess_for_xgboost(self, X_train: pd.DataFrame, X_test: pd.DataFrame, pre: ColumnTransformer, feat_names: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "\n",
        "        X_train_df = self._preprocess_to_df(pre, X_train, feat_names)\n",
        "        X_test_df = self._preprocess_to_df(pre, X_test, feat_names)\n",
        "\n",
        "\n",
        "        X_train_processed = X_train_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "        X_test_processed = X_test_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "        return X_train_processed, X_test_processed\n",
        "\n",
        "    def _optimize_inner(self, X_train: pd.DataFrame, y_train: pd.Series, model_name: str, pre: ColumnTransformer, feat_names: List[str]) -> Dict[str, Any]:\n",
        "\n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            params = self._get_model_params(model_name, trial)\n",
        "            is_boosting = model_name in ['XGBoost', 'LightGBM', 'CatBoost']\n",
        "            early_stopping_rounds = params.pop('early_stopping_rounds', 30) if is_boosting else None\n",
        "\n",
        "\n",
        "            eval_metric = None\n",
        "            if model_name == 'XGBoost':\n",
        "                eval_metric = 'logloss'\n",
        "            elif model_name == 'LightGBM':\n",
        "                eval_metric = 'binary_logloss'\n",
        "            elif model_name == 'CatBoost':\n",
        "                eval_metric = 'Logloss'\n",
        "\n",
        "            if eval_metric and is_boosting:\n",
        "                params['eval_metric'] = eval_metric\n",
        "\n",
        "\n",
        "\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
        "                                         random_state=self.cfg['random_state'])\n",
        "            for tr_idx, val_idx in sss.split(X_train, y_train):\n",
        "                X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
        "                y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "\n",
        "            if len(np.unique(y_tr)) < 2 or len(np.unique(y_val)) < 2:\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "            X_tr_df = self._preprocess_to_df(pre, X_tr, feat_names)\n",
        "            X_val_df = self._preprocess_to_df(pre, X_val, feat_names)\n",
        "\n",
        "\n",
        "            if model_name == 'XGBoost':\n",
        "                X_tr_df = X_tr_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "                X_val_df = X_val_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "\n",
        "            base = self._create_model(model_name, params)\n",
        "\n",
        "            fit_args = {}\n",
        "            if is_boosting:\n",
        "                fit_args['eval_set'] = [(X_val_df, y_val)]\n",
        "                fit_args['early_stopping_rounds'] = early_stopping_rounds\n",
        "                fit_args['verbose'] = False\n",
        "\n",
        "            if model_name == 'LightGBM':\n",
        "                try:\n",
        "                    import lightgbm as lgb\n",
        "                    callbacks = [lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False), lgb.log_evaluation(0)]\n",
        "                    fit_args['callbacks'] = callbacks\n",
        "                    fit_args['eval_metric'] = params.get('eval_metric', 'logloss')\n",
        "                    base.fit(X_tr_df, y_tr, **fit_args)\n",
        "                except Exception:\n",
        "                    base.fit(X_tr_df, y_tr)\n",
        "            elif model_name == 'XGBoost':\n",
        "               try:\n",
        "                   base.fit(\n",
        "                       X_tr_df, y_tr,\n",
        "                       eval_set=fit_args['eval_set'],\n",
        "                       early_stopping_rounds=fit_args['early_stopping_rounds'],\n",
        "                       verbose=False\n",
        "                   )\n",
        "               except Exception:\n",
        "                   base.fit(X_tr_df, y_tr)\n",
        "            elif model_name == 'CatBoost':\n",
        "                try:\n",
        "\n",
        "                    base.fit(X_tr_df, y_tr, eval_set=fit_args['eval_set'], early_stopping_rounds=fit_args['early_stopping_rounds'], verbose=fit_args['verbose'])\n",
        "                except Exception:\n",
        "                    base.fit(X_tr_df, y_tr)\n",
        "            else:\n",
        "                base.fit(X_tr_df, y_tr)\n",
        "\n",
        "\n",
        "            selector = HybridFeatureSelector(self.cfg)\n",
        "            selector.fit(X_tr_df, y_tr, base)\n",
        "            X_tr_sel = selector.transform(X_tr_df)\n",
        "            X_val_sel = selector.transform(X_val_df)\n",
        "\n",
        "\n",
        "            final_model = self._create_model(model_name, params)\n",
        "\n",
        "            fit_args = {}\n",
        "\n",
        "            is_boosting = model_name in ['XGBoost', 'LightGBM', 'CatBoost']\n",
        "            if is_boosting:\n",
        "                early_stopping_rounds = params.get('early_stopping_rounds', 30)\n",
        "                fit_args['early_stopping_rounds'] = early_stopping_rounds\n",
        "                fit_args['verbose'] = False\n",
        "\n",
        "            if model_name == 'LightGBM' or model_name == 'XGBoost' or model_name == 'CatBoost':\n",
        "                fit_args['eval_set'] = [(X_val_sel, y_val)]\n",
        "\n",
        "\n",
        "            if model_name == 'LightGBM':\n",
        "                final_model.fit(X_tr_sel, y_tr, callbacks=fit_args.get('callbacks'), eval_set=fit_args['eval_set'], eval_metric=fit_args.get('eval_metric'))\n",
        "            elif model_name == 'XGBoost':\n",
        "                final_model.fit(X_tr_sel, y_tr, eval_set=fit_args['eval_set'], early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
        "            elif model_name == 'CatBoost':\n",
        "                final_model.fit(X_tr_sel, y_tr, eval_set=fit_args['eval_set'], early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
        "            else:\n",
        "                final_model.fit(X_tr_sel, y_tr)\n",
        "\n",
        "\n",
        "\n",
        "            if hasattr(final_model, 'predict_proba'):\n",
        "                y_prob = final_model.predict_proba(X_val_sel)[:, 1]\n",
        "            else:\n",
        "                y_prob = _decision_to_proba(final_model, X_val_sel)\n",
        "\n",
        "\n",
        "            try:\n",
        "                logloss = log_loss(y_val, y_prob)\n",
        "                return -logloss\n",
        "            except ValueError:\n",
        "                return -999.0\n",
        "\n",
        "\n",
        "        study = optuna.create_study(\n",
        "            direction='maximize',\n",
        "            study_name=f'opt_{model_name}',\n",
        "            sampler=TPESampler(seed=self.cfg['random_state'])\n",
        "        )\n",
        "\n",
        "        study.optimize(\n",
        "            objective,\n",
        "            n_trials=self.cfg['optimization']['n_trials'],\n",
        "            n_jobs=1,\n",
        "            timeout=None\n",
        "        )\n",
        "\n",
        "\n",
        "        best_params = study.best_params\n",
        "        is_boosting = model_name in ['XGBoost', 'LightGBM', 'CatBoost']\n",
        "\n",
        "        if is_boosting:\n",
        "            param_name = 'iterations' if model_name == 'CatBoost' else 'n_estimators'\n",
        "            best_params[param_name] = 2000\n",
        "\n",
        "\n",
        "            best_params.pop('early_stopping_rounds', None)\n",
        "            best_params.pop('eval_metric', None)\n",
        "            best_params['random_state'] = self.cfg['random_state']\n",
        "\n",
        "\n",
        "        self.log.info(f\"       Validating best parameters with {self.cfg['optimization']['inner_cv_folds']}-fold CV...\")\n",
        "        cv_scores = self._validate_params_with_cv(X_train, y_train, model_name, best_params.copy(), pre, feat_names)\n",
        "        self.log.info(f\"       Inner CV AUC performance: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}\")\n",
        "\n",
        "        return best_params\n",
        "\n",
        "    def _validate_params_with_cv(self, X_train: pd.DataFrame, y_train: pd.Series, model_name: str, params: Dict[str, Any], pre: ColumnTransformer, feat_names: List[str]) -> List[float]:\n",
        "\n",
        "        inner_cv = StratifiedKFold(\n",
        "            n_splits=self.cfg['optimization']['inner_cv_folds'],\n",
        "            shuffle=True,\n",
        "            random_state=self.cfg['random_state']\n",
        "        )\n",
        "        cv_scores = []\n",
        "        cv_params = params.copy()\n",
        "        cv_params.pop('early_stopping_rounds', None)\n",
        "\n",
        "        for inner_fold, (tr_idx, val_idx) in enumerate(inner_cv.split(X_train, y_train)):\n",
        "            X_tr_fold, X_val_fold = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
        "            y_tr_fold, y_val_fold = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "\n",
        "            X_tr_df = self._preprocess_to_df(pre, X_tr_fold, feat_names)\n",
        "            X_val_df = self._preprocess_to_df(pre, X_val_fold, feat_names)\n",
        "\n",
        "            if model_name == 'XGBoost':\n",
        "                X_tr_df = X_tr_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "                X_val_df = X_val_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "\n",
        "            base_for_selector = self._create_model(model_name, cv_params)\n",
        "            base_for_selector.fit(X_tr_df, y_tr_fold)\n",
        "\n",
        "            selector = HybridFeatureSelector(self.cfg)\n",
        "            selector.fit(X_tr_df, y_tr_fold, base_for_selector)\n",
        "\n",
        "\n",
        "            X_tr_sel = selector.transform(X_tr_df)\n",
        "            X_val_sel = selector.transform(X_val_df)\n",
        "\n",
        "\n",
        "\n",
        "            final_model = self._create_model(model_name, cv_params)\n",
        "            final_model.fit(X_tr_sel, y_tr_fold)\n",
        "\n",
        "            if hasattr(final_model, 'predict_proba'):\n",
        "                y_prob = final_model.predict_proba(X_val_sel)[:, 1]\n",
        "            else:\n",
        "                y_prob = _decision_to_proba(final_model, X_val_sel)\n",
        "\n",
        "            cv_scores.append(roc_auc_score(y_val_fold, y_prob))\n",
        "\n",
        "        return cv_scores\n",
        "\n",
        "    def _train_calibrate(self, X_tr: pd.DataFrame, y_tr: pd.Series, model_name: str, params: Dict[str, Any], pre: ColumnTransformer, feat_names: List[str]):\n",
        "\n",
        "        if model_name == 'XGBoost':\n",
        "            X_tr_df, _ = self._preprocess_for_xgboost(X_tr, X_tr, pre, feat_names)\n",
        "        else:\n",
        "            X_tr_df = self._preprocess_to_df(pre, X_tr, feat_names)\n",
        "\n",
        "        params.pop('early_stopping_rounds', None)\n",
        "        params.pop('eval_metric', None)\n",
        "\n",
        "        base_for_selector = self._create_model(model_name, params)\n",
        "        base_for_selector.fit(X_tr_df, y_tr)\n",
        "\n",
        "        selector = HybridFeatureSelector(self.cfg)\n",
        "        selector.fit(X_tr_df, y_tr, base_for_selector)\n",
        "        X_tr_sel = selector.transform(X_tr_df)\n",
        "\n",
        "        base_model = self._create_model(model_name, params)\n",
        "\n",
        "        if model_name == 'SVM':\n",
        "\n",
        "            base_model.set_params(probability=True)\n",
        "\n",
        "        calibrated = CalibratedClassifierCV(\n",
        "            base_model,\n",
        "            method=self.cfg['evaluation']['calibration_method'],\n",
        "            cv=self.cfg['evaluation']['calibration_cv_folds']\n",
        "        )\n",
        "        calibrated.fit(X_tr_sel, y_tr)\n",
        "        return calibrated, selector\n",
        "\n",
        "\n",
        "    def run_pipeline(self):\n",
        "\n",
        "        self.log.info(\"Starting nested cross-validation pipeline...\")\n",
        "        outer = StratifiedKFold(self.cfg['evaluation']['outer_cv_folds'], shuffle=True, random_state=self.cfg['random_state'])\n",
        "        summaries, plot_data = [], {}\n",
        "\n",
        "        for model_name in self.models:\n",
        "            self.log.info(f\"=== Processing {model_name} ===\")\n",
        "            fold_metrics, all_y, all_p = [], [], []\n",
        "            model_params = {}\n",
        "\n",
        "            for fold_id, (tr_idx, te_idx) in enumerate(outer.split(self.X, self.y), 1):\n",
        "\n",
        "                try:\n",
        "                    X_tr, X_te = self.X.iloc[tr_idx], self.X.iloc[te_idx]\n",
        "                    y_tr, y_te = self.y.iloc[tr_idx], self.y.iloc[te_idx]\n",
        "\n",
        "                    if len(np.unique(y_tr)) < 2 or len(np.unique(y_te)) < 2:\n",
        "                        self.log.warning(f\"Fold {fold_id}: single-class â€“ skipped\")\n",
        "                        continue\n",
        "                    pre = create_preprocessor(X_tr).fit(X_tr)\n",
        "                    feat_names = get_feature_names(pre, X_tr)\n",
        "\n",
        "\n",
        "                    best_params = self._optimize_inner(X_tr, y_tr, model_name, pre, feat_names)\n",
        "\n",
        "\n",
        "                    calibrated, selector = self._train_calibrate(X_tr, y_tr, model_name, best_params.copy(), pre, feat_names)\n",
        "\n",
        "\n",
        "                    if model_name == 'XGBoost':\n",
        "                        X_te_df, _ = self._preprocess_for_xgboost(X_te, X_te, pre, feat_names)\n",
        "                    else:\n",
        "                        X_te_df = self._preprocess_to_df(pre, X_te, feat_names)\n",
        "                    X_te_sel = selector.transform(X_te_df)\n",
        "\n",
        "                    y_proba = calibrated.predict_proba(X_te_sel)[:, 1]\n",
        "                    y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "                    m = collect_metrics(y_te.to_numpy(), y_proba, y_pred)\n",
        "                    fold_metrics.append(m)\n",
        "\n",
        "\n",
        "                    artifact_path = self.cfg['output_dirs']['models'] / f\"{model_name}_fold{fold_id}_artifact.pkl\"\n",
        "                    joblib.dump({'calibrated_model': calibrated, 'preprocessor': pre, 'selector': selector, 'best_params': best_params}, artifact_path)\n",
        "\n",
        "                    all_y.append(y_te.to_numpy())\n",
        "                    all_p.append(y_proba)\n",
        "                    gc.collect()\n",
        "\n",
        "                except Exception:\n",
        "                    self.log.error(f\"Catastrophic error in {model_name} fold {fold_id}:\\n{traceback.format_exc()}\")\n",
        "                    continue\n",
        "\n",
        "            if fold_metrics:\n",
        "                df_fold = pd.DataFrame(fold_metrics)\n",
        "                numeric_cols = df_fold.select_dtypes(include=[np.number]).columns\n",
        "                summary = {f\"{c}_mean\": df_fold[c].mean() for c in numeric_cols}\n",
        "                summary.update({f\"{c}_std\": df_fold[c].std() for c in numeric_cols})\n",
        "                summary['model'] = model_name\n",
        "                summary['n_folds'] = len(fold_metrics)\n",
        "                summaries.append(summary)\n",
        "\n",
        "                plot_data[model_name] = {'y': np.concatenate(all_y), 'p': np.concatenate(all_p)}\n",
        "\n",
        "\n",
        "                try:\n",
        "                    pre_full = create_preprocessor(self.X).fit(self.X)\n",
        "                    feat_names_full = get_feature_names(pre_full, self.X)\n",
        "                    best_full = self._optimize_inner(self.X, self.y, model_name, pre_full, feat_names_full)\n",
        "                    model_params['final_model'] = best_full\n",
        "\n",
        "                    param_path = self.cfg['output_dirs']['models'].parent / \"best_params\" / f'{model_name}_params.json'\n",
        "                    with open(param_path, 'w') as f:\n",
        "                        json.dump(model_params, f, indent=4)\n",
        "\n",
        "                    calibrated_full, selector_full = self._train_calibrate(self.X, self.y, model_name, best_full.copy(), pre_full, feat_names_full)\n",
        "\n",
        "                    final_path = self.cfg['output_dirs']['models'] / f\"{model_name}_final_calibrated_model.pkl\"\n",
        "                    joblib.dump({'calibrated_model': calibrated_full, 'preprocessor': pre_full, 'selector': selector_full,\n",
        "                                 'metadata': {'selected_features': selector_full.selected_features_, 'model_name': model_name, 'best_params': best_full}}, final_path)\n",
        "\n",
        "                    self.log.info(f\"Final model {model_name} complete and saved.\")\n",
        "                except Exception:\n",
        "                    self.log.error(f\"Error training final {model_name}:\\n{traceback.format_exc()}\")\n",
        "            else:\n",
        "                self.log.warning(f\"{model_name} failed completely - skipping final analysis.\")\n",
        "\n",
        "\n",
        "        if summaries:\n",
        "            self.log.info(\"Performing statistical analysis...\")\n",
        "\n",
        "\n",
        "            metrics_df = pd.DataFrame(summaries)\n",
        "            metrics_df.to_csv(self.cfg['output_dirs']['metrics'] / 'final_metrics.csv', index=False)\n",
        "\n",
        "\n",
        "            global CONFIG\n",
        "            CONFIG = self.cfg\n",
        "\n",
        "\n",
        "            _, effect_df = calculate_effect_sizes_and_tests(summaries, output_dir=CONFIG['output_dirs']['metrics'])\n",
        "            effect_df['Comparison'] = effect_df['Model_A'] + '_vs_' + effect_df['Model_B']\n",
        "\n",
        "            statistical_tests_dict = perform_statistical_tests(plot_data)\n",
        "            test_records = [{'Comparison': comp, **tests} for comp, tests in statistical_tests_dict.items()]\n",
        "            test_df = pd.DataFrame(test_records)\n",
        "            test_df.to_csv(os.path.join(CONFIG['output_dirs']['metrics'], 'prediction_statistical_tests.csv'), index=False)\n",
        "\n",
        "            print(\"\\n ðŸ”§  Applying multiple comparisons corrections...\")\n",
        "            test_df_corrected = test_df.copy()\n",
        "            if not effect_df.empty:\n",
        "                effect_df_corrected = apply_multiple_comparisons_correction(effect_df, p_value_columns=['p_value'], alpha=0.05, method='both')\n",
        "                effect_df_corrected.to_csv(os.path.join(CONFIG['output_dirs']['metrics'], 'cv_pairwise_effects_corrected.csv'), index=False)\n",
        "                print_correction_summary(effect_df_corrected)\n",
        "\n",
        "            if not test_df.empty:\n",
        "                test_df_corrected = apply_multiple_comparisons_correction(test_df, p_value_columns=['auc_difference_pval', 'mcnemar_pval'], alpha=0.05, method='both')\n",
        "                test_df_corrected.to_csv(os.path.join(CONFIG['output_dirs']['metrics'], 'prediction_statistical_tests_corrected.csv'), index=False)\n",
        "                print_correction_summary(test_df_corrected)\n",
        "\n",
        "\n",
        "            print(\"\\n ðŸŽ¨ Generating plots...\")\n",
        "            self._make_plots(summaries, plot_data, effect_df, test_df_corrected)\n",
        "            print(f\"All plots saved to {CONFIG['output_dirs']['plots']}/\")\n",
        "\n",
        "\n",
        "            print(\"\\n ðŸ“ˆ  Statistical Summary of AUC Differences (Bootstrap Test):\")\n",
        "            if not test_df_corrected.empty:\n",
        "                pval_col = 'auc_difference_pval_fdr' if 'auc_difference_pval_fdr' in test_df_corrected.columns else 'auc_difference_pval'\n",
        "                sig_tests = test_df_corrected[test_df_corrected[pval_col] < 0.05]\n",
        "\n",
        "                if not sig_tests.empty:\n",
        "                    print(f\"Significant differences (FDR p < 0.05):\")\n",
        "                    for _, row in sig_tests.iterrows():\n",
        "                        print(f\"   {row['Comparison']}: Î”AUC={row['auc_difference']:.3f}, p-fdr={row[pval_col]:.3f}\")\n",
        "                else:\n",
        "                    print(\"No significant AUC differences found between models (FDR p < 0.05).\")\n",
        "            else:\n",
        "                print(\"No test results available to summarize.\")\n",
        "        else:\n",
        "            print(\"\\n ðŸ“Š  No model summaries available for statistical analysis and plotting.\")\n",
        "\n",
        "        return summaries, plot_data\n",
        "\n",
        "\n",
        "    def _make_plots(self, summaries, plot_data, effect_df, test_df):\n",
        "\n",
        "        try:\n",
        "            matplotlib.rcParams.update({\n",
        "                'font.family': 'DejaVu Sans', 'font.size': 6, 'axes.titlesize': 6,\n",
        "                'axes.labelsize': 6, 'xtick.labelsize': 6, 'ytick.labelsize': 6,\n",
        "                'legend.fontsize': 6, 'figure.dpi': CONFIG['plotting']['dpi']\n",
        "            })\n",
        "            colors = sns.color_palette(\"colorblind\")\n",
        "\n",
        "\n",
        "            plt.figure(figsize=(3.5, 3.5))\n",
        "            for i, (m, d) in enumerate(plot_data.items()):\n",
        "                pt, pp = calibration_curve_fixed_bins(d['y'], d['p'], CONFIG['evaluation']['n_bins'])\n",
        "                plt.plot(pp, pt, 'o-', label=m, color=colors[i % len(colors)],\n",
        "                         linewidth=0.7, markersize=2, markeredgewidth=0.7)\n",
        "            plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated', linewidth=0.7)\n",
        "            plt.xlabel('Mean Predicted Probability'); plt.ylabel('Fraction of Positives')\n",
        "            plt.title(\"Calibration Curves (Reliability Diagram)\"); plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.5); plt.tight_layout()\n",
        "            plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'calibration_all_models.png'))\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "            dfm = pd.DataFrame(summaries)\n",
        "            metrics = ['roc_auc_mean', 'pr_auc_mean', 'f1_mean', 'accuracy_mean', 'kappa_mean', 'mcc_mean']\n",
        "\n",
        "\n",
        "            metrics = [m for m in metrics if m in dfm.columns]\n",
        "            if len(metrics) > 0:\n",
        "                dfr = dfm[['model'] + metrics].copy()\n",
        "                for c in metrics:\n",
        "                    dfr[c] = (dfr[c] - dfr[c].min()) / (dfr[c].max() - dfr[c].min() + 1e-9)\n",
        "\n",
        "                angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist() + [0]\n",
        "                fig, ax = plt.subplots(subplot_kw={'polar': True}, figsize=(3.5, 3.5))\n",
        "                matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "                for i, row in dfr.iterrows():\n",
        "                    vals = row[metrics].tolist() + [row[metrics[0]]]\n",
        "                    ax.plot(angles, vals, label=row['model'], color=colors[i % len(colors)], linewidth=0.8)\n",
        "                    ax.fill(angles, vals, alpha=0.1, color=colors[i % len(colors)])\n",
        "\n",
        "                metric_labels = [m.replace('_mean', '').replace('_', ' ').upper() for m in metrics]\n",
        "                ax.set_thetagrids(np.degrees(angles[:-1]), metric_labels, fontsize=6)\n",
        "                ax.set_ylim(0, 1)\n",
        "                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=5)\n",
        "                ax.grid(True, alpha=0.3, linewidth=0.5)\n",
        "                ax.legend(\n",
        "                    loc='upper center',\n",
        "                    bbox_to_anchor=(0.5, -0.15),\n",
        "                    fontsize=6,\n",
        "                    frameon=False,\n",
        "                    borderaxespad=0,\n",
        "                    ncol=3\n",
        "                )\n",
        "                plt.title('Model Performance Radar', fontsize=7, pad=20)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'radar_chart.png'), dpi=450, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
        "                plt.close()\n",
        "            else:\n",
        "                logging.warning(\"Skipping radar chart: Not enough metrics available.\")\n",
        "\n",
        "\n",
        "\n",
        "            dfm = pd.DataFrame(summaries)\n",
        "            if 'model' not in dfm.columns or 'roc_auc_mean' not in dfm.columns or 'pr_auc_mean' not in dfm.columns:\n",
        "\n",
        "                logging.warning(\"Cannot align plot legends: Model summary data is missing required columns (model, roc_auc_mean, pr_auc_mean).\")\n",
        "                return\n",
        "\n",
        "\n",
        "            mean_auc_lookup = dfm.set_index('model')['roc_auc_mean'].to_dict()\n",
        "            mean_ap_lookup = dfm.set_index('model')['pr_auc_mean'].to_dict()\n",
        "\n",
        "\n",
        "            plt.figure(figsize=(3.5, 3.5))\n",
        "            for i, (m, d) in enumerate(plot_data.items()):\n",
        "\n",
        "                fpr, tpr, _ = roc_curve(d['y'], d['p'])\n",
        "\n",
        "\n",
        "                mean_auc = mean_auc_lookup.get(m, np.nan)\n",
        "\n",
        "\n",
        "                label_text = f\"{m} (AUC={mean_auc:.3f})\" if not np.isnan(mean_auc) else f\"{m} (AUC=N/A)\"\n",
        "\n",
        "                plt.plot(fpr, tpr, label=label_text, color=colors[i % len(colors)], linewidth=0.7)\n",
        "\n",
        "            plt.plot([0, 1], [0, 1], 'k--', label='Chance', linewidth=0.7)\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('ROC Curves')\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'roc_curves.png'))\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "            plt.figure(figsize=(3.5, 3.5))\n",
        "            for i, (m, d) in enumerate(plot_data.items()):\n",
        "\n",
        "                precision, recall, _ = precision_recall_curve(d['y'], d['p'])\n",
        "\n",
        "\n",
        "                mean_ap = mean_ap_lookup.get(m, np.nan)\n",
        "\n",
        "\n",
        "                label_text = f\"{m} (AP={mean_ap:.3f})\" if not np.isnan(mean_ap) else f\"{m} (AP=N/A)\"\n",
        "\n",
        "                plt.plot(recall, precision, label=label_text, color=colors[i % len(colors)], linewidth=0.7)\n",
        "\n",
        "            plt.xlabel('Recall')\n",
        "            plt.ylabel('Precision')\n",
        "            plt.title('Precision-Recall Curves')\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'pr_curves.png'))\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "            if not effect_df.empty:\n",
        "                heatmap_df = effect_df.pivot(index='Metric', columns='Comparison', values='Cohens_d')\n",
        "                plt.figure(figsize=(max(6, len(heatmap_df.columns)*0.5), max(4, len(heatmap_df.index)*0.4)))\n",
        "                sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap='RdBu_r', vmin=-1.5, vmax=1.5,\n",
        "                            cbar_kws={'label': \"Cohen's d Effect Size\"},\n",
        "                            annot_kws={'fontsize': 6, 'fontname': 'DejaVu Sans'},\n",
        "                            linewidths=0.5, linecolor='lightgrey', square=False)\n",
        "                plt.title(\"Effect Sizes (Cohen's d) Between Models Across CV Folds\", fontsize=6, pad=8)\n",
        "                plt.ylabel('', fontsize=6); plt.xlabel('', fontsize=6)\n",
        "                plt.xticks(rotation=45, ha='right', fontsize=6, fontname='DejaVu Sans')\n",
        "                plt.yticks(rotation=0, fontsize=6, fontname='DejaVu Sans')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'effect_sizes_heatmap.png'))\n",
        "                plt.close()\n",
        "            else:\n",
        "                logging.warning(\"Skipping effect size heatmap: effect_df is empty.\")\n",
        "\n",
        "\n",
        "            if not test_df.empty:\n",
        "                fig, ax = plt.subplots(figsize=(max(6, len(test_df)*0.5), 4.0))\n",
        "\n",
        "\n",
        "                pval_col = 'auc_difference_pval_fdr' if 'auc_difference_pval_fdr' in test_df.columns else 'auc_difference_pval'\n",
        "                p_values = test_df[pval_col]\n",
        "\n",
        "                bar_colors = ['darkred' if p < 0.001 else 'red' if p < 0.01 else 'orange' if p < 0.05 else 'lightblue' for p in p_values]\n",
        "                bars = ax.bar(test_df['Comparison'], test_df['auc_difference'], color=bar_colors, edgecolor='black', linewidth=0.7)\n",
        "                ax.axhline(y=0, color='black', linestyle='-', alpha=0.7, linewidth=0.7)\n",
        "\n",
        "                for i, p_val in enumerate(p_values):\n",
        "                    height = bars[i].get_height()\n",
        "                    significance = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
        "                    if significance:\n",
        "                        ax.text(bars[i].get_x() + bars[i].get_width()/2., height + (0.01 if height >= 0 else -0.01),\n",
        "                                significance, ha='center', va='bottom' if height >= 0 else 'top', fontsize=6, fontname='DejaVu Sans')\n",
        "\n",
        "                ax.set_ylabel('AUC Difference\\n(Model A - Model B)', fontsize=6, fontname='DejaVu Sans')\n",
        "                ax.set_xlabel('Model Comparisons', fontsize=6, fontname='DejaVu Sans')\n",
        "                ax.set_title(f'Statistical Significance of AUC Differences (Bootstrap Test, FDR corrected)', fontsize=6)\n",
        "                plt.xticks(rotation=45, ha='left', fontsize=6, fontname='DejaVu Sans')\n",
        "                legend_elements = [\n",
        "                    plt.Rectangle((0,0),1,1, color='darkred', label='p < 0.001 (***)'),\n",
        "                    plt.Rectangle((0,0),1,1, color='red', label='p < 0.01 (**)'),\n",
        "                    plt.Rectangle((0,0),1,1, color='orange', label='p < 0.05 (*)'),\n",
        "                    plt.Rectangle((0,0),1,1, color='lightblue', label='NS (p â‰¥ 0.05)')]\n",
        "                ax.legend(handles=legend_elements, loc='upper right', title='Significance Level', title_fontsize=6, frameon=False)\n",
        "                plt.grid(True, alpha=0.2, axis='y', linewidth=0.5)\n",
        "                ax.set_axisbelow(True)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(CONFIG['output_dirs']['plots'], 'statistical_significance.png'))\n",
        "                plt.close()\n",
        "            else:\n",
        "                logging.warning(\"Skipping statistical significance bar plot: test_df is empty.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during plotting in _make_plots: {e}\\n{traceback.format_exc()}\")\n",
        "            print(f\"       âŒ   Error during plotting: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "\n",
        "        cfg_obj = Config.from_env()\n",
        "        CONFIG = cfg_obj.to_dict()\n",
        "        log = setup_logger(cfg_obj)\n",
        "\n",
        "        log.info(\"Loading training data...\")\n",
        "\n",
        "        train_csv_path = CONFIG['data']['train_csv']\n",
        "        target_col = CONFIG['data']['target']\n",
        "\n",
        "        if not Path(train_csv_path).exists():\n",
        "            log.critical(f\"Training data file '{train_csv_path}' not found.\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        df_train = pd.read_csv(train_csv_path)\n",
        "        if target_col not in df_train.columns:\n",
        "            log.critical(f\"Target column '{target_col}' not found in training data.\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        X_train = df_train.drop(columns=[target_col])\n",
        "        y_train = df_train[target_col].astype(int)\n",
        "\n",
        "        log.info(f\"Data shape: {X_train.shape} | Class balance:\\n{y_train.value_counts(normalize=True)}\")\n",
        "\n",
        "        pipeline = NestedCVPipeline(X_train, y_train, CONFIG)\n",
        "        summaries, plot_data = pipeline.run_pipeline()\n",
        "\n",
        "        if summaries and plot_data:\n",
        "            log.info(\"=== Pipeline Complete ===\")\n",
        "        else:\n",
        "            log.warning(\"Pipeline finished but no models completed successfully.\")\n",
        "\n",
        "    except Exception:\n",
        "        log.critical(f\"A critical error occurred:\\n{traceback.format_exc()}\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "id": "R-h1_TFcjTot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model Evaluation (Validation Set)**     \n",
        "Evaluate models on the validation set to compare performance before threshold optimization."
      ],
      "metadata": {
        "id": "T0XrmiQEjXv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import warnings\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels not available - some statistical functions will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    MULTITEST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MULTITEST_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.multitest not available - multiple comparison corrections will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.contingency_tables import mcnemar\n",
        "    MCNEMAR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MCNEMAR_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.contingency_tables not available - McNemar test will be limited\")\n",
        "\n",
        "try:\n",
        "    from mlxtend.evaluate import mcnemar_table\n",
        "    MLXTEND_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLXTEND_AVAILABLE = False\n",
        "    warnings.warn(\"mlxtend not available - McNemar table functionality will be limited\")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, brier_score_loss,\n",
        "    matthews_corrcoef, cohen_kappa_score, roc_curve,\n",
        "    precision_recall_curve, balanced_accuracy_score, log_loss, confusion_matrix\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from scipy.special import logit, expit\n",
        "from scipy.stats import norm, chi2, t\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({\n",
        "    'figure.facecolor': 'white',\n",
        "    'savefig.facecolor': 'white',\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 6,\n",
        "    'axes.titlesize': 6,\n",
        "    'axes.labelsize': 6,\n",
        "    'xtick.labelsize': 6,\n",
        "    'ytick.labelsize': 6,\n",
        "    'legend.fontsize': 6,\n",
        "    'figure.dpi': 450,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_names_from_column_transformer(ct):\n",
        "\n",
        "    names = []\n",
        "    for nm, pipe, cols in getattr(ct, 'transformers_', []):\n",
        "        if nm == 'remainder' and pipe == 'drop':\n",
        "            continue\n",
        "        if hasattr(pipe, 'named_steps') and 'onehot' in pipe.named_steps:\n",
        "            cats = pipe.named_steps['onehot'].categories_\n",
        "            for c, cl in zip(cols, cats):\n",
        "                names += [f\"{c}_{v}\" for v in cl]\n",
        "        else:\n",
        "            names += list(cols)\n",
        "    return names\n",
        "\n",
        "def enforce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    out = df.copy()\n",
        "    out = out.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    out.dropna(axis=0, how=\"all\", inplace=True)\n",
        "    out.dropna(axis=1, how=\"all\", inplace=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def apply_multiple_comparisons_correction(df, pval_cols, alpha=0.05, method='both'):\n",
        "\n",
        "    if not MULTITEST_AVAILABLE:\n",
        "        warnings.warn(\"multipletests not available - returning original dataframe\")\n",
        "        return df\n",
        "\n",
        "    df_corrected = df.copy()\n",
        "\n",
        "    for col in pval_cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        pvals = df[col].dropna().values\n",
        "        valid_idx = df[col].notna()\n",
        "\n",
        "        if len(pvals) == 0:\n",
        "            continue\n",
        "\n",
        "\n",
        "        if method in ['bonferroni', 'both']:\n",
        "            bonf_pvals = np.minimum(pvals * len(pvals), 1.0)\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni'] = bonf_pvals\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni_sig'] = bonf_pvals < alpha\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni_alpha'] = alpha / len(pvals)\n",
        "\n",
        "\n",
        "        if method in ['fdr', 'both']:\n",
        "            _, fdr_pvals, _, _ = multipletests(pvals, alpha=alpha, method='fdr_bh')\n",
        "            df_corrected.loc[valid_idx, f'{col}_fdr'] = fdr_pvals\n",
        "            df_corrected.loc[valid_idx, f'{col}_fdr_sig'] = fdr_pvals < alpha\n",
        "\n",
        "    return df_corrected\n",
        "\n",
        "\n",
        "def print_correction_summary(df_corrected, original_alpha=0.05):\n",
        "\n",
        "    print(\"\\nðŸ“Š Multiple Comparisons Correction Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "    pval_cols = [col for col in df_corrected.columns\n",
        "                 if 'p_value' in col.lower() or 'pval' in col.lower()\n",
        "                 and not any(x in col for x in ['bonferroni', 'fdr'])]\n",
        "\n",
        "    for col in pval_cols:\n",
        "        if col in df_corrected.columns:\n",
        "            original_sig = (df_corrected[col] < original_alpha).sum()\n",
        "            total_tests = df_corrected[col].notna().sum()\n",
        "\n",
        "            print(f\"\\n{col.upper()}:\")\n",
        "            print(f\"  Original significant results (p < {original_alpha}): {original_sig}/{total_tests}\")\n",
        "\n",
        "            if f'{col}_bonferroni_sig' in df_corrected.columns:\n",
        "                bonf_sig = df_corrected[f'{col}_bonferroni_sig'].sum()\n",
        "                bonf_alpha = df_corrected[f'{col}_bonferroni_alpha'].iloc[0] if not df_corrected.empty else 'N/A'\n",
        "                print(f\"  Bonferroni significant (Î± = {bonf_alpha:.4f}): {bonf_sig}/{total_tests}\")\n",
        "\n",
        "            if f'{col}_fdr_sig' in df_corrected.columns:\n",
        "                fdr_sig = df_corrected[f'{col}_fdr_sig'].sum()\n",
        "                print(f\"  FDR significant (Î± = {original_alpha}): {fdr_sig}/{total_tests}\")\n",
        "\n",
        "\n",
        "\n",
        "def cohens_d(group1, group2):\n",
        "    group1, group2 = np.asarray(group1), np.asarray(group2)\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    if n1 < 2 or n2 < 2:\n",
        "        return np.nan\n",
        "    m1, m2 = np.mean(group1), np.mean(group2)\n",
        "    s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
        "    pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
        "    return np.nan if pooled_std == 0 else (m1 - m2) / pooled_std\n",
        "\n",
        "def cliffs_delta(group1, group2):\n",
        "    dominance = 0\n",
        "    for x in group1:\n",
        "        for y in group2:\n",
        "            if x > y:\n",
        "                dominance += 1\n",
        "            elif x < y:\n",
        "                dominance -= 1\n",
        "    return dominance / (len(group1) * len(group2))\n",
        "\n",
        "\n",
        "def interpret_cohens_d(d):\n",
        "    d = abs(d)\n",
        "    if np.isnan(d): return \"Unknown\"\n",
        "    return (\"Negligible\" if d < 0.2 else\n",
        "            \"Small\" if d < 0.5 else\n",
        "            \"Medium\" if d < 0.8 else \"Large\")\n",
        "\n",
        "def interpret_cliffs_delta(delta):\n",
        "    d = abs(delta)\n",
        "    if np.isnan(d): return \"Unknown\"\n",
        "    return (\"Negligible\" if d < 0.147 else\n",
        "            \"Small\" if d < 0.33 else\n",
        "            \"Medium\" if d < 0.474 else \"Large\")\n",
        "\n",
        "def bootstrap_effect_size_ci(group1, group2, effect_fn, n_bootstrap=1000, alpha=0.05):\n",
        "    group1, group2 = np.asarray(group1), np.asarray(group2)\n",
        "    rng = np.random.default_rng(42)\n",
        "    effects = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx1 = rng.choice(len(group1), len(group1), replace=True)\n",
        "        idx2 = rng.choice(len(group2), len(group2), replace=True)\n",
        "\n",
        "        eff = effect_fn(group1[idx1], group2[idx2])\n",
        "        if not np.isnan(eff):\n",
        "            effects.append(eff)\n",
        "    if not effects:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    effects = np.array(effects)\n",
        "    return effects.mean(), np.percentile(effects, 100 * alpha / 2), np.percentile(effects, 100 * (1 - alpha / 2))\n",
        "\n",
        "\n",
        "\n",
        "def calibration_slope_intercept(y_true, y_prob, method='logistic'):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob) | np.isnan(y_true))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_prob = np.clip(y_prob, 1e-6, 1 - 1e-6)\n",
        "    X_base = logit(y_prob) if method == 'logistic' else y_prob\n",
        "    X = sm.add_constant(X_base.reshape(-1, 1))\n",
        "\n",
        "    try:\n",
        "        mod = sm.GLM(y_true, X, family=sm.families.Binomial())\n",
        "        res = mod.fit(disp=0)\n",
        "        intercept, slope = float(res.params[0]), float(res.params[1])\n",
        "        return intercept, slope\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def calibration_slope_intercept_ci(y_true, y_prob, n_bootstrap=1000, alpha=0.05, seed=42):\n",
        "\n",
        "\n",
        "    intercept_point, slope_point = calibration_slope_intercept(y_true, y_prob)\n",
        "\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    slopes, intercepts = [], []\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        try:\n",
        "            indices = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "            if len(np.unique(y_true[indices])) < 2:\n",
        "                continue\n",
        "\n",
        "            int_b, slp_b = calibration_slope_intercept(y_true[indices], y_prob[indices])\n",
        "            if not (np.isnan(slp_b) or np.isnan(int_b)):\n",
        "                slopes.append(slp_b)\n",
        "                intercepts.append(int_b)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if slopes and intercepts:\n",
        "        slope_ci = (np.percentile(slopes, 100 * alpha/2), np.percentile(slopes, 100 * (1 - alpha/2)))\n",
        "        intercept_ci = (np.percentile(intercepts, 100 * alpha/2), np.percentile(intercepts, 100 * (1 - alpha/2)))\n",
        "    else:\n",
        "\n",
        "        slope_ci = (np.nan, np.nan)\n",
        "        intercept_ci = (np.nan, np.nan)\n",
        "\n",
        "    return intercept_point, slope_point, intercept_ci, slope_ci\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_ece(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "    if len(y_true) == 0: return np.nan\n",
        "    y_prob = np.clip(y_prob, 0.0, 1.0)\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        bin_edges = np.unique(bin_edges) # Fix zero-width bins\n",
        "    else:\n",
        "        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "\n",
        "    actual_n_bins = len(bin_edges) - 1\n",
        "    if actual_n_bins == 0: return 0.0\n",
        "\n",
        "    ece = 0.0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for i in range(actual_n_bins):\n",
        "        # Fix double counting: use [a, b) intervals\n",
        "        if i == actual_n_bins - 1:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            ece += (n_in_bin / total_samples) * np.abs(avg_pred - avg_true)\n",
        "    return float(ece)\n",
        "\n",
        "def calculate_mce(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "    if len(y_true) == 0: return np.nan\n",
        "    y_prob = np.clip(y_prob, 0.0, 1.0)\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        bin_edges = np.unique(bin_edges)\n",
        "    else:\n",
        "        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "\n",
        "    actual_n_bins = len(bin_edges) - 1\n",
        "    if actual_n_bins == 0: return 0.0\n",
        "\n",
        "    max_error = 0.0\n",
        "    for i in range(actual_n_bins):\n",
        "        if i == actual_n_bins - 1:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            error = abs(avg_pred - avg_true)\n",
        "            if error > max_error: max_error = error\n",
        "    return float(max_error)\n",
        "\n",
        "def hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=10, min_expected_freq=5):\n",
        "\n",
        "\n",
        "    if not STATSMODELS_AVAILABLE:\n",
        "        warnings.warn(\"statsmodels not available - returning NaN for Hosmer-Lemeshow test\")\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 20 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
        "    try:\n",
        "        df['bin'] = pd.qcut(df['y_prob'], n_bins, labels=False, duplicates='drop')\n",
        "    except ValueError:\n",
        "        df['bin'] = np.floor(df['y_prob'] * n_bins).astype(int)\n",
        "        df.loc[df['bin'] == n_bins, 'bin'] = n_bins - 1\n",
        "\n",
        "\n",
        "    summary = df.groupby('bin').agg(\n",
        "        observed=('y_true', 'sum'),\n",
        "        expected=('y_prob', 'sum'),\n",
        "        n_total=('y_true', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    while True:\n",
        "        sparse_bins = summary[summary['expected'] < min_expected_freq]\n",
        "        if sparse_bins.empty or len(summary) <= 2:\n",
        "            break\n",
        "\n",
        "\n",
        "        merge_idx = sparse_bins.index[0]\n",
        "        if merge_idx == 0:\n",
        "            summary.loc[1, ['observed', 'expected', 'n_total']] += summary.loc[0, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(0).reset_index(drop=True)\n",
        "        else:\n",
        "            summary.loc[merge_idx - 1, ['observed', 'expected', 'n_total']] += summary.loc[merge_idx, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(merge_idx).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    g = len(summary)\n",
        "    summary['variance'] = summary['expected'] * (1 - summary['expected'] / summary['n_total'])\n",
        "    hl_statistic = ((summary['observed'] - summary['expected'])**2 / (summary['variance'] + 1e-8)).sum()\n",
        "\n",
        "\n",
        "    df_hl = g - 2\n",
        "    if df_hl <= 0:\n",
        "        return hl_statistic, np.nan\n",
        "\n",
        "    p_value = 1 - chi2.cdf(hl_statistic, df_hl)\n",
        "\n",
        "    return hl_statistic, p_value\n",
        "\n",
        "def calibration_curve_fixed_bins(y_true, y_prob, n_bins=10):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return np.full(n_bins, np.nan), np.full(n_bins, np.nan)\n",
        "\n",
        "\n",
        "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "    bin_edges[0] = 0.0\n",
        "    bin_edges[-1] = 1.0\n",
        "\n",
        "    prob_true = np.full(n_bins, np.nan)\n",
        "    prob_pred = np.full(n_bins, np.nan)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        n_in_bin = np.sum(mask)\n",
        "\n",
        "        if n_in_bin > 0:\n",
        "            prob_true[i] = np.mean(y_true[mask])\n",
        "            prob_pred[i] = np.mean(y_prob[mask])\n",
        "        else:\n",
        "\n",
        "            prob_true[i] = np.nan\n",
        "            prob_pred[i] = (bin_edges[i] + bin_edges[i+1]) / 2\n",
        "\n",
        "    return prob_true, prob_pred\n",
        "\n",
        "\n",
        "\n",
        "def calculate_calibration_summary_robust(y_true, y_prob, n_bins=10,\n",
        "                                       n_bootstrap=1000, alpha=0.05):\n",
        "\n",
        "    summary = {}\n",
        "\n",
        "\n",
        "    summary['Brier'] = brier_score_loss(y_true, y_prob)\n",
        "    summary['LogLoss'] = log_loss(y_true, y_prob)\n",
        "\n",
        "\n",
        "    summary['ECE_Quantile'] = calculate_ece(y_true, y_prob, n_bins, 'quantile')\n",
        "\n",
        "    summary['MCE'] = calculate_mce(y_true, y_prob, n_bins, 'quantile')\n",
        "\n",
        "\n",
        "\n",
        "    intercept, slope, intercept_ci, slope_ci = calibration_slope_intercept_ci(\n",
        "        y_true, y_prob, n_bootstrap=n_bootstrap, alpha=alpha\n",
        "    )\n",
        "\n",
        "    summary['Cal_Intercept'] = intercept\n",
        "    summary['Cal_Slope'] = slope\n",
        "    summary['Cal_Slope_CI_Low'] = slope_ci[0]\n",
        "    summary['Cal_Slope_CI_High'] = slope_ci[1]\n",
        "    summary['Cal_Intercept_CI_Low'] = intercept_ci[0]\n",
        "    summary['Cal_Intercept_CI_High'] = intercept_ci[1]\n",
        "\n",
        "\n",
        "    z_statistic, z_p_value = z_test_standard(y_true, y_prob)\n",
        "    summary['Spiegelhalter_Z'] = z_statistic\n",
        "    summary['Spiegelhalter_P_Value'] = z_p_value\n",
        "\n",
        "\n",
        "    hl_statistic, hl_p_value = hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins)\n",
        "    summary['HL_Statistic'] = hl_statistic\n",
        "    summary['HL_P_Value'] = hl_p_value\n",
        "\n",
        "\n",
        "    summary['Well_Calibrated'] = (\n",
        "        not np.isnan(slope) and not np.isnan(intercept) and\n",
        "        abs(slope - 1) < 0.2 and abs(intercept) < 0.15 and\n",
        "        (not np.isnan(hl_p_value) and hl_p_value > 0.05) and\n",
        "        (not np.isnan(z_p_value) and z_p_value > 0.05) and\n",
        "        summary['ECE_Quantile'] < 0.10\n",
        "    )\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "def z_test_standard(y_true, y_prob):\n",
        "\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = np.sum(y_true - y_prob)\n",
        "\n",
        "\n",
        "    denominator = np.sqrt(np.sum(y_prob * (1 - y_prob)))\n",
        "\n",
        "    if denominator < 1e-8:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    z_stat = numerator / denominator\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return z_stat, p_value\n",
        "\n",
        "def bootstrap_ci(y_true, y_pred, y_prob, metric_func, n_bootstrap=1000, ci_width=0.95, **kwargs):\n",
        "    boot = []\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    n = len(y_true)\n",
        "    if n < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    pos_indices = np.where(y_true == 1)[0]\n",
        "    neg_indices = np.where(y_true == 0)[0]\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "\n",
        "        pos_sample = rng.choice(pos_indices, len(pos_indices), replace=True)\n",
        "        neg_sample = rng.choice(neg_indices, len(neg_indices), replace=True)\n",
        "        idx = np.concatenate([pos_sample, neg_sample])\n",
        "\n",
        "\n",
        "        try:\n",
        "            if metric_func.__name__ in ['roc_auc_score', 'average_precision_score', 'brier_score_loss', 'log_loss', 'calculate_ece', 'calculate_mce']:\n",
        "                # PASS KWARGS HERE\n",
        "                val = metric_func(y_true[idx], y_prob[idx], **kwargs)\n",
        "            else:\n",
        "                val = metric_func(y_true[idx], y_pred[idx])\n",
        "\n",
        "            if not np.isnan(val):\n",
        "                boot.append(val)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if len(boot) < 100:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    boot = np.array(boot)\n",
        "    lower = np.percentile(boot, 100 * (1 - ci_width) / 2)\n",
        "    upper = np.percentile(boot, 100 * (1 + ci_width) / 2)\n",
        "\n",
        "    return lower, upper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_model_artifacts(cfg: dict) -> dict:\n",
        "\n",
        "    model_dir = Path(cfg['output_dirs']['models'])\n",
        "\n",
        "    log = logging.getLogger(\"nested_cv\")\n",
        "\n",
        "    log.info(f\"ðŸ” Loading final models from: {model_dir.resolve()}\")\n",
        "\n",
        "    artifacts = {}\n",
        "\n",
        "    pattern = str(model_dir / \"*_final_calibrated_model.pkl\")\n",
        "    files = sorted(glob.glob(pattern))\n",
        "\n",
        "    if not files:\n",
        "        log.warning(f\"âš ï¸ No models found in {model_dir} matching pattern '{pattern}'.\")\n",
        "        return artifacts\n",
        "\n",
        "    for fpath_str in files:\n",
        "        fpath = Path(fpath_str)\n",
        "\n",
        "        try:\n",
        "            art = joblib.load(fpath)\n",
        "        except Exception as e:\n",
        "            log.error(f\"âš ï¸ Could not load artifact {fpath.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        required_keys = ['preprocessor', 'selector', 'calibrated_model', 'metadata']\n",
        "        if not all(k in art for k in required_keys):\n",
        "            log.warning(f\"âš ï¸ Missing required keys in {fpath.name}, skipping file.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        basename = fpath.name\n",
        "\n",
        "        raw_name = basename.replace(\"_final_calibrated_model.pkl\", \"\")\n",
        "\n",
        "        name = raw_name.replace(\"_\", \" \")\n",
        "\n",
        "        metadata = art.get('metadata', {})\n",
        "\n",
        "\n",
        "        artifacts[name] = {\n",
        "            'artifact': art,\n",
        "            'preprocessor': art.get('preprocessor'),\n",
        "            'selector': art.get('selector'),\n",
        "            'calibrated_model': art.get('calibrated_model', None),\n",
        "            'base_model': art.get('base_model', None),\n",
        "            'metadata': metadata,\n",
        "            'selected_features': metadata.get('selected_features', None),\n",
        "            'all_features': metadata.get('all_features', None)\n",
        "        }\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "\n",
        "\n",
        "def validate_pre_split_model(X_raw, y_true, entry, threshold=0.5):\n",
        "\n",
        "    X_val_clean = X_raw.copy().reset_index(drop=True)\n",
        "    y_val_clean = y_true.copy().reset_index(drop=True)\n",
        "    pre = entry['preprocessor']\n",
        "    calibrated_model = entry['calibrated_model'] or entry['base_model']\n",
        "    X_pre = pre.transform(X_val_clean)\n",
        "    all_features = get_feature_names_from_column_transformer(pre)\n",
        "    X_pre_df = pd.DataFrame(X_pre, columns=all_features)\n",
        "    selected_features = entry['metadata']['selected_features']\n",
        "    X_final = X_pre_df[selected_features]\n",
        "    proba = calibrated_model.predict_proba(X_final)[:, 1]\n",
        "    pred = (proba >= threshold).astype(int)\n",
        "    return pred, proba, y_val_clean\n",
        "\n",
        "\n",
        "def evaluate_single_model(entry, X_raw, y_true, threshold=0.5, bootstrap_n=1000):\n",
        "\n",
        "    pred, proba, y_val_clean = validate_pre_split_model(X_raw, y_true, entry, threshold)\n",
        "\n",
        "    metrics = {}\n",
        "    y_true_np = y_val_clean.values\n",
        "\n",
        "\n",
        "    metrics['Accuracy'] = accuracy_score(y_true_np, pred)\n",
        "    metrics['Balanced_Accuracy'] = balanced_accuracy_score(y_true_np, pred)\n",
        "    metrics['Precision'] = precision_score(y_true_np, pred, zero_division=0)\n",
        "    metrics['Recall'] = recall_score(y_true_np, pred, zero_division=0)\n",
        "    metrics['F1'] = f1_score(y_true_np, pred, zero_division=0)\n",
        "\n",
        "    if len(np.unique(y_true_np)) > 1:\n",
        "        metrics['AUC_ROC'] = roc_auc_score(y_true_np, proba)\n",
        "        metrics['AUPRC'] = average_precision_score(y_true_np, proba)\n",
        "    else:\n",
        "        metrics['AUC_ROC'] = np.nan\n",
        "        metrics['AUPRC'] = np.nan\n",
        "\n",
        "    metrics['MCC'] = matthews_corrcoef(y_true_np, pred)\n",
        "    metrics['Kappa'] = cohen_kappa_score(y_true_np, pred)\n",
        "    metrics['Brier'] = brier_score_loss(y_true_np, proba)\n",
        "    metrics['Log_Loss'] = log_loss(y_true_np, proba)\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true_np, pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    metrics.update({\n",
        "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
        "        'NPV': tn / (tn + fn) if (tn + fn) > 0 else 0,\n",
        "        'PPV': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
        "        'FPR': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
        "        'FNR': fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    })\n",
        "\n",
        "\n",
        "    metrics['ECE'] = calculate_ece(y_true_np, proba, method='uniform')\n",
        "    metrics['MCE'] = calculate_mce(y_true_np, proba, method='uniform')\n",
        "\n",
        "\n",
        "    intercept, slope, intercept_ci, slope_ci = calibration_slope_intercept_ci(\n",
        "        y_true_np, proba, n_bootstrap=bootstrap_n\n",
        "    )\n",
        "    metrics['calibration_intercept'] = intercept\n",
        "    metrics['calibration_slope'] = slope\n",
        "    metrics['calibration_intercept_CI_low'] = intercept_ci[0]\n",
        "    metrics['calibration_intercept_CI_high'] = intercept_ci[1]\n",
        "    metrics['calibration_slope_CI_low'] = slope_ci[0]\n",
        "    metrics['calibration_slope_CI_high'] = slope_ci[1]\n",
        "\n",
        "\n",
        "    z, p_z = z_test_standard(y_true_np, proba)\n",
        "    metrics['z_statistic'] = z\n",
        "    metrics['z_p_value'] = p_z\n",
        "\n",
        "\n",
        "    hl_statistic, hl_p = hosmer_lemeshow_test_advanced(y_true_np, proba)\n",
        "    metrics['hl_statistic'] = hl_statistic\n",
        "    metrics['hl_p_value'] = hl_p\n",
        "\n",
        "\n",
        "    ci_metrics = {}\n",
        "    metric_definitions = [\n",
        "        ('Accuracy', accuracy_score),\n",
        "        ('Balanced_Accuracy', balanced_accuracy_score),\n",
        "        ('Precision', precision_score),\n",
        "        ('Recall', recall_score),\n",
        "        ('F1', f1_score),\n",
        "        ('AUC_ROC', roc_auc_score),\n",
        "        ('AUPRC', average_precision_score),\n",
        "        ('MCC', matthews_corrcoef),\n",
        "        ('Kappa', cohen_kappa_score),\n",
        "        ('Brier', brier_score_loss),\n",
        "        ('Log_Loss', log_loss),\n",
        "        ('ECE', calculate_ece),\n",
        "        ('MCE', calculate_mce),\n",
        "    ]\n",
        "\n",
        "    for name, func in metric_definitions:\n",
        "        kwargs = {}\n",
        "        # FORCE 'uniform' for ECE and MCE to match point estimate\n",
        "        if name in ['ECE', 'MCE']:\n",
        "            kwargs = {'method': 'uniform'}\n",
        "\n",
        "        lo, hi = bootstrap_ci(y_true_np, pred, proba, func,\n",
        "                              n_bootstrap=bootstrap_n, ci_width=0.95, **kwargs)\n",
        "\n",
        "        ci_metrics[f'{name}_CI_low'] = lo\n",
        "        ci_metrics[f'{name}_CI_high'] = hi\n",
        "\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true_np, proba)\n",
        "    precision, recall, _ = precision_recall_curve(y_true_np, proba)\n",
        "    prob_true, prob_pred = calibration_curve_fixed_bins(y_true_np, proba, n_bins=10)\n",
        "\n",
        "    selected_features = entry['metadata'].get('selected_features', [])\n",
        "\n",
        "    return {\n",
        "        'metrics': {**metrics, **ci_metrics},\n",
        "        'roc_curve': (fpr, tpr),\n",
        "        'pr_curve': (precision, recall),\n",
        "        'calibration_curve': (prob_true, prob_pred),\n",
        "        'avg_pred': pred,\n",
        "        'avg_proba': proba,\n",
        "        'y_true': y_val_clean,\n",
        "        'selected_features': selected_features,\n",
        "        'n_features_used': len(selected_features)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap_metric_diff(y, p1, p2, metric_fn, n_bootstraps=1000, seed=42):\n",
        "    y, p1, p2 = np.asarray(y), np.asarray(p1), np.asarray(p2)\n",
        "    if len(np.unique(y)) < 2: return np.nan, np.nan, np.nan\n",
        "    rng = np.random.default_rng(seed)\n",
        "    diffs = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        idx = rng.choice(len(y), len(y), replace=True)\n",
        "        try:\n",
        "            diffs.append(metric_fn(y[idx], p1[idx]) - metric_fn(y[idx], p2[idx]))\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    if not diffs: return np.nan, np.nan, np.nan\n",
        "    diffs = np.asarray(diffs)\n",
        "    p_val = 2 * min((diffs > 0).mean(), (diffs < 0).mean())\n",
        "    return p_val, diffs.mean(), diffs.std()\n",
        "\n",
        "def perform_mcnemar(y_true, y_pred1, y_pred2):\n",
        "    if not MLXTEND_AVAILABLE or not MCNEMAR_AVAILABLE:\n",
        "        warnings.warn(\"mlxtend or mcnemar not available - using basic implementation\")\n",
        "\n",
        "        n01 = np.sum((y_pred1 == 0) & (y_pred2 == 1) & (y_true == y_pred2))\n",
        "        n10 = np.sum((y_pred1 == 1) & (y_pred2 == 0) & (y_true == y_pred1))\n",
        "        if n01 + n10 == 0:\n",
        "            return 1.0\n",
        "        statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
        "        from scipy.stats import chi2\n",
        "        return 1 - chi2.cdf(statistic, 1)\n",
        "\n",
        "    try:\n",
        "        from mlxtend.evaluate import mcnemar_table\n",
        "        y_true, y_pred1, y_pred2 = np.asarray(y_true), np.asarray(y_pred1), np.asarray(y_pred2)\n",
        "        table = mcnemar_table(y_target=y_true, y_model1=y_pred1, y_model2=y_pred2)\n",
        "        return mcnemar(table, exact=False, correction=True).pvalue, table\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"McNemar test failed: {e}\")\n",
        "        return np.nan, None\n",
        "\n",
        "def mcnemar_effect_size(table):\n",
        "    if table is None:\n",
        "        return np.nan\n",
        "    b, c = table[0,1], table[1,0]\n",
        "    if c == 0: return np.inf if b > 0 else np.nan\n",
        "    return b/c\n",
        "\n",
        "def interpret_odds_ratio(odds_ratio):\n",
        "    if np.isnan(odds_ratio): return \"Unknown\"\n",
        "    if np.isinf(odds_ratio): return \"Complete dominance\"\n",
        "    return (\"Large\" if odds_ratio >= 3 or odds_ratio <= 1/3 else\n",
        "            \"Medium\" if odds_ratio >= 1.5 or odds_ratio <= 1/1.5 else \"Small\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_pairwise_effect_sizes(results, n_bootstrap=1000):\n",
        "\n",
        "    model_names = list(results.keys())\n",
        "    effect_data = []\n",
        "    for m1, m2 in combinations(model_names, 2):\n",
        "        r1, r2 = results[m1], results[m2]\n",
        "        y = r1['y_true']\n",
        "        p1, p2 = r1['avg_proba'], r2['avg_proba']\n",
        "        pred1, pred2 = r1['avg_pred'], r2['avg_pred']\n",
        "\n",
        "\n",
        "        try:\n",
        "            pval_roc, diff_roc, std_roc = bootstrap_metric_diff(y, p1, p2, roc_auc_score, n_bootstraps=n_bootstrap)\n",
        "        except Exception:\n",
        "            pval_roc = diff_roc = std_roc = np.nan\n",
        "\n",
        "\n",
        "        rng = np.random.default_rng(42)\n",
        "        auprc_samples1, auprc_samples2 = [], []\n",
        "        for _ in range(450):\n",
        "            idx = rng.choice(len(y), len(y), replace=True)\n",
        "            try:\n",
        "                auprc_samples1.append(average_precision_score(y[idx], p1[idx]))\n",
        "                auprc_samples2.append(average_precision_score(y[idx], p2[idx]))\n",
        "            except Exception: continue\n",
        "        if auprc_samples1 and auprc_samples2:\n",
        "            coh_d = cohens_d(np.array(auprc_samples1), np.array(auprc_samples2))\n",
        "            cl_delta = cliffs_delta(np.array(auprc_samples1), np.array(auprc_samples2))\n",
        "            cd_mean, cd_low, cd_high = bootstrap_effect_size_ci(np.array(auprc_samples1), np.array(auprc_samples2), cohens_d)\n",
        "        else:\n",
        "            coh_d=cl_delta=cd_mean=cd_low=cd_high=np.nan\n",
        "\n",
        "\n",
        "        try:\n",
        "            pval_mcn, table = perform_mcnemar(y, pred1, pred2)\n",
        "            oratio = mcnemar_effect_size(table)\n",
        "            interp_or = interpret_odds_ratio(oratio)\n",
        "        except Exception:\n",
        "            pval_mcn = oratio = interp_or = np.nan\n",
        "\n",
        "        effect_data.append({\n",
        "            'Model_A': m1, 'Model_B': m2,\n",
        "            'AUROC_diff_mean': diff_roc, 'AUROC_diff_std': std_roc, 'AUROC_p_value': pval_roc,\n",
        "            'AUPRC_Cohens_d': coh_d,\n",
        "            'AUPRC_Cohens_d_CI_low': cd_low, 'AUPRC_Cohens_d_CI_high': cd_high,\n",
        "            'AUPRC_Cohens_d_interpretation': interpret_cohens_d(coh_d),\n",
        "            'AUPRC_Cliffs_delta': cl_delta,\n",
        "            'AUPRC_Cliffs_delta_interpretation': interpret_cliffs_delta(cl_delta),\n",
        "            'McNemar_p_value': pval_mcn,\n",
        "            'McNemar_Odds_ratio': oratio,\n",
        "            'McNemar_effect_interpretation': interp_or,\n",
        "        })\n",
        "    return pd.DataFrame(effect_data)\n",
        "\n",
        "def run_mcnemar_tests(preds_dict, y_true):\n",
        "\n",
        "    names = list(preds_dict.keys())\n",
        "    records = []\n",
        "    for m1, m2 in combinations(names, 2):\n",
        "        try:\n",
        "            pval, table = perform_mcnemar(y_true, preds_dict[m1], preds_dict[m2])\n",
        "            oratio = mcnemar_effect_size(table)\n",
        "            interp = interpret_odds_ratio(oratio)\n",
        "        except Exception:\n",
        "            pval = oratio = interp = np.nan\n",
        "        records.append({\n",
        "            \"Model_A\": m1, \"Model_B\": m2,\n",
        "            \"McNemar_p_value\": pval,\n",
        "            \"Odds_ratio\": oratio,\n",
        "            \"Effect_size_interpretation\": interp\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 6,\n",
        "    'axes.titlesize': 6,\n",
        "    'axes.labelsize': 6,\n",
        "    'xtick.labelsize': 6,\n",
        "    'ytick.labelsize': 6,\n",
        "    'legend.fontsize': 6,\n",
        "    'figure.dpi': 450,\n",
        "})\n",
        "\n",
        "def plot_roc_curves(results, outpath=None):\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    for model, res in results.items():\n",
        "        fpr = np.array(res['roc_curve'][0], dtype=float)\n",
        "        tpr = np.array(res['roc_curve'][1], dtype=float)\n",
        "        auc = res['metrics'].get('AUC_ROC', np.nan)\n",
        "        plt.plot(fpr, tpr, linewidth=0.7, label=f\"{model} (AUC = {auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=0.7)\n",
        "    plt.title(\"ROC Curves\")\n",
        "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend(loc='lower right', frameon=False)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    if outpath:\n",
        "        plt.savefig(outpath)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def plot_pr_curves(results, outpath=None):\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    for model, res in results.items():\n",
        "        prec = np.array(res['pr_curve'][0], dtype=float)\n",
        "        rec = np.array(res['pr_curve'][1], dtype=float)\n",
        "        ap = res['metrics'].get('AUPRC', np.nan)\n",
        "        plt.plot(rec, prec, linewidth=0.7, label=f\"{model} (AUPRC = {ap:.3f})\")\n",
        "    plt.title(\"Precision-Recall Curves\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "    plt.legend(loc='lower left', frameon=False)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    if outpath:\n",
        "        plt.savefig(outpath)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def plot_calibration_curves(results, outpath=None):\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    for model, res in results.items():\n",
        "        prob_true = np.array(res['calibration_curve'][0], dtype=float)\n",
        "        prob_pred = np.array(res['calibration_curve'][1], dtype=float)\n",
        "        plt.plot(prob_pred, prob_true, 'o-', markersize=2, linewidth=0.7, label=model)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label=\"Perfect\", linewidth=0.7)\n",
        "    plt.title(\"Calibration Curves\")\n",
        "    plt.xlabel(\"Mean Predicted Probability\")\n",
        "    plt.ylabel(\"Observed Fraction Positive\")\n",
        "    plt.legend(loc='upper left', frameon=False)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    if outpath:\n",
        "        plt.savefig(outpath)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def plot_heatmap(df: pd.DataFrame, outpath=None, title=\"Performance Heatmap\"):\n",
        "    df = enforce_numeric(df)\n",
        "    if df.empty:\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(max(6, df.shape[1] * 0.5), max(4, df.shape[0] * 0.4)))\n",
        "    sns.heatmap(\n",
        "        df, annot=True, fmt=\".3f\", cmap=\"viridis\", linewidths=0.5, linecolor='lightgrey',\n",
        "        annot_kws={'fontsize': 6, 'fontname': 'DejaVu Sans'}\n",
        "    )\n",
        "    plt.title(title, fontsize=6, pad=8)\n",
        "    plt.tight_layout()\n",
        "    if outpath:\n",
        "        plt.savefig(outpath, dpi=450)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def plot_radar_chart(df: pd.DataFrame, outpath=None, title=\"Radar Chart\"):\n",
        "    df = enforce_numeric(df)\n",
        "    if df.empty:\n",
        "        return\n",
        "    desired = [\"roc_auc\", \"pr_auc\", \"f1_\", \"accuracy\", \"kappa\", \"mcc\"]\n",
        "    present = [c for c in desired if c in df.columns]\n",
        "    if not present:\n",
        "        return\n",
        "    df = df[present].astype(float)\n",
        "    df_norm = (df - df.min()) / (df.max() - df.min() + 1e-9)\n",
        "    if df_norm.empty:\n",
        "        return\n",
        "    n_axes = len(df_norm.columns)\n",
        "    angles = np.linspace(0, 2 * np.pi, n_axes, endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(3.5, 3.5), subplot_kw=dict(polar=True))\n",
        "    colors = sns.color_palette(\"colorblind\", len(df_norm))\n",
        "    for idx, (row_name, row) in enumerate(df_norm.iterrows()):\n",
        "        vals = np.array(row.tolist() + [row.iloc[0]], dtype=float)\n",
        "        ax.plot(angles, vals, label=row_name, color=colors[idx], linewidth=0.7)\n",
        "        ax.fill(angles, vals, alpha=0.25, color=colors[idx])\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), df_norm.columns)\n",
        "    plt.title(title, pad=8)\n",
        "    plt.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.18), ncol=3, frameon=False, fontsize=6)\n",
        "    plt.tight_layout()\n",
        "    if outpath:\n",
        "        plt.savefig(outpath, dpi=450)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def create_effect_size_summary(effect_df, out_dir):\n",
        "    summary_lines = [\"# Effect Size Analysis Summary\\n\"]\n",
        "    if \"AUPRC_Cohens_d_interpretation\" in effect_df.columns:\n",
        "        summary_lines.append(\"## AUPRC Cohen's d:\")\n",
        "        vc = effect_df['AUPRC_Cohens_d_interpretation'].value_counts(dropna=False)\n",
        "        summary_lines.extend([f\"- {str(k)}: {str(v)} comps\" for k, v in vc.items()])\n",
        "    if \"McNemar_effect_interpretation\" in effect_df.columns:\n",
        "        summary_lines.append(\"\\n## McNemar Odds Ratio Interpretation:\")\n",
        "        vc = effect_df['McNemar_effect_interpretation'].value_counts(dropna=False)\n",
        "        summary_lines.extend([f\"- {str(k)}: {str(v)} comps\" for k, v in vc.items()])\n",
        "    with open(Path(out_dir,\"effect_size_summary.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(summary_lines))\n",
        "\n",
        "def run_evaluation(val_df, cfg: dict, out_dir=\"evaluation_1\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    print(\"=== MODEL EVALUATION PIPELINE ===\")\n",
        "    print(\"ðŸ” Focus: Calculating MCE (Maximum Calibration Error)\")\n",
        "\n",
        "    effect_df = None\n",
        "    if 'MDR status' not in val_df.columns:\n",
        "        raise ValueError(\"Validation DataFrame must contain 'MDR status' column.\")\n",
        "    df = val_df.reset_index(drop=True)\n",
        "    X_val = df.drop(columns=['MDR status'])\n",
        "    y_val = df['MDR status']\n",
        "    print(f\"âœ… Validation data loaded from val_df: {X_val.shape}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        artifacts = load_model_artifacts(cfg=cfg)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Config-based loading failed, trying direct path...\")\n",
        "\n",
        "        model_dir = Path(\"nested_cv_output\") / \"logloss\" / \"models\"\n",
        "        if not model_dir.exists():\n",
        "\n",
        "            model_dir = Path(\"models\")\n",
        "        artifacts = load_model_artifacts({'output_dirs': {'models': model_dir}})\n",
        "\n",
        "    if len(artifacts) == 0:\n",
        "        raise RuntimeError(\"No valid model artifacts found.\")\n",
        "    print(f\"âœ… Loaded {len(artifacts)} models\")\n",
        "\n",
        "\n",
        "    results = {}\n",
        "    preds = {}\n",
        "    for model_name, entry in artifacts.items():\n",
        "        print(f\"\\nâ–¶ Evaluating {model_name}...\")\n",
        "        try:\n",
        "            res = evaluate_single_model(entry, X_val, y_val, threshold=0.5, bootstrap_n=1000)\n",
        "            results[model_name] = res\n",
        "            preds[model_name] = res['avg_pred']\n",
        "\n",
        "\n",
        "            mce_val = res['metrics'].get('MCE', np.nan)\n",
        "            ece_val = res['metrics'].get('ECE', np.nan)\n",
        "            print(f\"âœ… {model_name}: AUC={res['metrics'].get('AUC_ROC', 0):.3f}, \"\n",
        "                  f\"MCE={mce_val:.3f}, ECE={ece_val:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed evaluating {model_name}: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        raise RuntimeError(\"No model produced results.\")\n",
        "\n",
        "\n",
        "    summary_rows = []\n",
        "    for m, r in results.items():\n",
        "        row = {'Model': m}\n",
        "        row.update(r['metrics'])\n",
        "        summary_rows.append(row)\n",
        "    perf_df = pd.DataFrame(summary_rows).set_index('Model')\n",
        "    perf_df.to_csv(os.path.join(out_dir, \"model_performance_with_ci.csv\"))\n",
        "\n",
        "    print(\"\\n=== Performance Summary (key metrics) ===\")\n",
        "    key_metrics = ['AUC_ROC', 'AUPRC', 'Accuracy', 'F1', 'Recall', 'Precision', 'calibration_slope', 'calibration_intercept']\n",
        "    ava = [m for m in key_metrics if m in perf_df.columns]\n",
        "    print(perf_df[ava].round(3).to_string())\n",
        "\n",
        "\n",
        "    print(f\"\\nðŸ“Š Generating visualizations...\")\n",
        "    try:\n",
        "        plot_roc_curves(results, outpath=os.path.join(out_dir, \"roc_curves.png\"))\n",
        "        plot_pr_curves(results, outpath=os.path.join(out_dir, \"pr_curves.png\"))\n",
        "        plot_calibration_curves(results, outpath=os.path.join(out_dir, \"calibration_curves.png\"))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Some curve plots failed: {e}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        rename_map = {\n",
        "            \"AUC_ROC\": \"roc_auc\",\n",
        "            \"AUPRC\": \"pr_auc\",\n",
        "            \"F1\": \"f1\",\n",
        "            \"Accuracy\": \"accuracy\",\n",
        "            \"Kappa\": \"kappa\",\n",
        "            \"MCC\": \"mcc\",\n",
        "            \"Cal_Intercept\": \"calibration_intercept\",\n",
        "            \"Cal_Slope\": \"calibration_slope\",\n",
        "        }\n",
        "        visual_cols_old = [k for k in rename_map.keys() if k in perf_df.columns]\n",
        "        visual_df = perf_df[visual_cols_old].rename(columns=rename_map)\n",
        "        heat_cols = [c for c in [\"roc_auc\", \"pr_auc\", \"f1\", \"accuracy\", \"kappa\", \"mcc\"] if c in visual_df.columns]\n",
        "        if heat_cols:\n",
        "            plot_heatmap(\n",
        "                visual_df[heat_cols],\n",
        "                outpath=os.path.join(out_dir, \"performance_heatmap.png\"),\n",
        "                title=\"Model Performance Heatmap\"\n",
        "            )\n",
        "            plot_radar_chart(\n",
        "                visual_df[heat_cols],\n",
        "                outpath=os.path.join(out_dir, \"metrics_radar_chart.png\"),\n",
        "                title=\"Radar Chart\"\n",
        "            )\n",
        "        print(\"âœ… All plots generated successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Visual plots failed: {e}\")\n",
        "\n",
        "\n",
        "    mcn_df = None\n",
        "    try:\n",
        "        mcn_df = run_mcnemar_tests(preds, y_val.values)\n",
        "        if mcn_df is not None and not mcn_df.empty:\n",
        "            print(f\"\\n=== Statistical Significance Tests (McNemar) ===\")\n",
        "            print(mcn_df.round(4).to_string(index=False))\n",
        "            mcn_df.to_csv(os.path.join(out_dir, \"mcnemar_results.csv\"), index=False)\n",
        "\n",
        "\n",
        "            print(f\"\\nðŸ”§ Applying multiple comparisons corrections to McNemar tests...\")\n",
        "            mcn_corrected = apply_multiple_comparisons_correction(\n",
        "                mcn_df,\n",
        "                pval_cols=['McNemar_p_value'],\n",
        "                alpha=0.05,\n",
        "                method='both'\n",
        "            )\n",
        "            mcn_corrected.to_csv(os.path.join(out_dir, \"mcnemar_results_corrected.csv\"), index=False)\n",
        "            print_correction_summary(mcn_corrected)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Statistical tests failed: {e}\")\n",
        "\n",
        "    effect_df = None\n",
        "    try:\n",
        "        print(f\"\\nðŸ” Computing pairwise effect sizes...\")\n",
        "        effect_df = compute_pairwise_effect_sizes(results, n_bootstrap=1000)\n",
        "        if not effect_df.empty:\n",
        "            effect_csv_fp = os.path.join(out_dir, \"pairwise_effect_sizes.csv\")\n",
        "            effect_df.to_csv(effect_csv_fp, index=False)\n",
        "            print(f\"âœ… Pairwise effect sizes saved to: {effect_csv_fp}\")\n",
        "\n",
        "\n",
        "            print(f\"\\nðŸ”§ Applying multiple comparisons corrections to effect size tests...\")\n",
        "            effect_corrected = apply_multiple_comparisons_correction(\n",
        "                effect_df,\n",
        "                pval_cols=['AUROC_p_value', 'McNemar_p_value'],\n",
        "                alpha=0.05,\n",
        "                method='both'\n",
        "            )\n",
        "            effect_corrected.to_csv(os.path.join(out_dir, \"pairwise_effect_sizes_corrected.csv\"), index=False)\n",
        "\n",
        "\n",
        "        best_model = perf_df['Accuracy'].idxmax() if 'Accuracy' in perf_df.columns else perf_df.index[0]\n",
        "        slope = perf_df.loc[best_model, 'calibration_slope'] if 'calibration_slope' in perf_df.columns else np.nan\n",
        "        intercept = perf_df.loc[best_model, 'calibration_intercept'] if 'calibration_intercept' in perf_df.columns else np.nan\n",
        "\n",
        "        if not np.isnan(slope) and not np.isnan(intercept):\n",
        "            if abs(slope - 1) < 0.1 and abs(intercept) < 0.05:\n",
        "                print(\"   âœ… Well calibrated (slope â‰ˆ 1, intercept â‰ˆ 0)\")\n",
        "            else:\n",
        "                issues = []\n",
        "                if abs(slope - 1) >= 0.1:\n",
        "                    issues.append(f\"slope deviation ({slope:.3f})\")\n",
        "                if abs(intercept) >= 0.05:\n",
        "                    issues.append(f\"intercept deviation ({intercept:.3f})\")\n",
        "                print(f\"   âš ï¸  Calibration issues: {', '.join(issues)}\")\n",
        "\n",
        "\n",
        "            create_effect_size_summary(effect_corrected, out_dir)\n",
        "            print(f\"âœ… Corrected effect sizes and summary saved\")\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ No effect sizes computed (insufficient models or data)\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Effect size analysis failed: {e}\")\n",
        "        effect_df = None\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Evaluation complete! Results saved to: {os.path.abspath(out_dir)}\")\n",
        "    print(f\"   ðŸ“Š {len(results)} models evaluated successfully\")\n",
        "    if 'Accuracy' in perf_df.columns:\n",
        "        print(f\"   ðŸ“ˆ Best model: {perf_df['Accuracy'].idxmax()} ({perf_df['Accuracy'].max():.1%})\")\n",
        "    print(f\"\\nðŸ“ Generated files:\")\n",
        "    print(f\"   - model_performance_with_ci.csv: Comprehensive metrics with confidence intervals\")\n",
        "    print(f\"   - pairwise_effect_sizes.csv: Raw effect size comparisons\")\n",
        "    print(f\"   - pairwise_effect_sizes_corrected.csv: Effect sizes with Bonferroni/FDR corrections\")\n",
        "    print(f\"   - mcnemar_results.csv: Raw McNemar test results\")\n",
        "    print(f\"   - mcnemar_results_corrected.csv: McNemar with Bonferroni/FDR corrections\")\n",
        "    print(f\"   - effect_size_summary.txt: Human-readable effect size interpretation\")\n",
        "    print(f\"   - Various plots and visualization files\")\n",
        "\n",
        "    return results, perf_df, effect_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from your_training_module import Config, setup_logger\n",
        "        cfg_obj = Config.from_env()\n",
        "        CONFIG = cfg_obj.to_dict()\n",
        "        log = setup_logger(cfg_obj)\n",
        "        print(\"âœ… Loaded configuration from training module\")\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Could not import training config, using defaults\")\n",
        "        CONFIG = {\n",
        "            'output_dirs': {\n",
        "                'models': Path(\"nested_cv_output/logloss/models\")\n",
        "             },\n",
        "             'random_state': 42\n",
        "        }\n",
        "\n",
        "        logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "        log = logging.getLogger(\"validation\")\n",
        "\n",
        "\n",
        "        validation_csv_path = \"validation_set.csv\"\n",
        "        if not Path(validation_csv_path).exists():\n",
        "            log.critical(f\"Validation data file '{validation_csv_path}' not found.\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        val_df = pd.read_csv(validation_csv_path)\n",
        "        print(f\"âœ… Loaded validation data: {val_df.shape}\")\n",
        "\n",
        "\n",
        "        print(\"\\nStarting Model Evaluation...\")\n",
        "\n",
        "        results, perf_df, effect_df = run_evaluation(\n",
        "            val_df,\n",
        "            cfg=CONFIG,\n",
        "            out_dir=\"evaluation\"\n",
        "        )\n",
        "\n",
        "        log.info(\"Evaluation complete. Results processed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.critical(f\"A critical error occurred during evaluation:\\n{traceback.format_exc()}\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "id": "4fGQUBvujcJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Threshold Optimization**  \n",
        "Determine the optimal probability thresholds using metrics such as Youdenâ€™s J, F1-score, and balanced accuracy."
      ],
      "metadata": {
        "id": "XQwgFzSKjfiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import csv\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import warnings\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, balanced_accuracy_score, confusion_matrix,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Callable\n",
        "from types import MappingProxyType\n",
        "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
        "from typing import Dict, Any\n",
        "\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "\n",
        "log_filename = f'eval_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_filename),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "\n",
        "def setup_logger(config):\n",
        "\n",
        "    return logging.getLogger(\"Placeholder\")\n",
        "\n",
        "class NestedCVPipeline:\n",
        "\n",
        "\n",
        "    def __init__(self, X: pd.DataFrame, y: pd.Series, cfg: Dict[str, Any]):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.cfg = cfg\n",
        "\n",
        "        pass\n",
        "\n",
        "class HybridFeatureSelector:\n",
        "\n",
        "    def __init__(self, cfg: Dict[str, Any]):\n",
        "\n",
        "        self.n_features = cfg['feature_selection']['n_features']\n",
        "        self.shap_weight = cfg['feature_selection']['shap_weight']\n",
        "        self.tree_weight = cfg['feature_selection']['tree_weight']\n",
        "        self.max_samples_shap = cfg['feature_selection']['max_samples_shap']\n",
        "        self.random_state = cfg['random_state']\n",
        "        self.selected_features_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        if self.selected_features_ is not None and isinstance(X, pd.DataFrame):\n",
        "            return X[self.selected_features_]\n",
        "\n",
        "\n",
        "        return X\n",
        "\n",
        "def calculate_metrics(y_true: np.ndarray,\n",
        "                      y_pred_proba: np.ndarray,\n",
        "                      thresholds: np.ndarray) -> Dict[str, list]:\n",
        "\n",
        "    metrics = {\n",
        "        'thresholds': list(thresholds), 'f1_scores': [], 'accuracy': [],\n",
        "        'sensitivity': [], 'specificity': [], 'youden_j': [],\n",
        "        'balanced_accuracies': [], 'TN': [], 'FP': [], 'FN': [], 'TP': [],\n",
        "        'NPV': [], 'PPV': [], 'FPR': [], 'FNR': []\n",
        "    }\n",
        "\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_pred_proba >= t).astype(int)\n",
        "        try:\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
        "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
        "        except ValueError:\n",
        "            sensitivity, specificity, npv, ppv, fpr, fnr = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "            tn = fp = fn = tp = 0\n",
        "\n",
        "        metrics['f1_scores'].append(f1_score(y_true, y_pred, zero_division=0))\n",
        "        metrics['accuracy'].append(accuracy_score(y_true, y_pred))\n",
        "        metrics['sensitivity'].append(sensitivity)\n",
        "        metrics['specificity'].append(specificity)\n",
        "        metrics['youden_j'].append(sensitivity + specificity - 1)\n",
        "        metrics['balanced_accuracies'].append(balanced_accuracy_score(y_true, y_pred))\n",
        "        metrics['TN'].append(tn); metrics['FP'].append(fp)\n",
        "        metrics['FN'].append(fn); metrics['TP'].append(tp)\n",
        "        metrics['NPV'].append(npv); metrics['PPV'].append(ppv)\n",
        "        metrics['FPR'].append(fpr); metrics['FNR'].append(fnr)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "def sensitivity_score(y_true_bs, y_pred_bs):\n",
        "    try:\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true_bs, y_pred_bs, labels=[0, 1]).ravel()\n",
        "        return tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    except ValueError: return 0.0\n",
        "\n",
        "def specificity_score(y_true_bs, y_pred_bs):\n",
        "    try:\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true_bs, y_pred_bs, labels=[0, 1]).ravel()\n",
        "        return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    except ValueError: return 0.0\n",
        "\n",
        "F1_FUNC = lambda yt, yp: f1_score(yt, yp, zero_division=0)\n",
        "METRIC_FUNCTIONS = {\n",
        "    'f1': F1_FUNC,\n",
        "    'accuracy': accuracy_score,\n",
        "    'sensitivity': sensitivity_score,\n",
        "    'specificity': specificity_score\n",
        "}\n",
        "\n",
        "def _bootstrap_single_metric(metric_func: Callable, y_true: np.ndarray, y_pred: np.ndarray, seed: int) -> float:\n",
        "\n",
        "    try:\n",
        "        indices = resample(np.arange(len(y_true)), replace=True, n_samples=len(y_true), random_state=seed)\n",
        "        if len(np.unique(y_true[indices])) < 2: return 0.0\n",
        "        return metric_func(y_true[indices], y_pred[indices])\n",
        "    except Exception: return 0.0\n",
        "\n",
        "\n",
        "def _bootstrap_single_score_metric(metric_func: Callable, y_true: np.ndarray, y_score: np.ndarray, seed: int) -> float:\n",
        "\n",
        "    try:\n",
        "        indices = resample(np.arange(len(y_true)), replace=True, n_samples=len(y_true), random_state=seed)\n",
        "        if len(np.unique(y_true[indices])) < 2: return 0.0\n",
        "        return metric_func(y_true[indices], y_score[indices])\n",
        "    except Exception: return 0.0\n",
        "\n",
        "def bootstrap_metric_ci(metric_func: Callable, y_true: np.ndarray, y_score: np.ndarray, threshold: float,\n",
        "                        n_bootstrap: int = 1000, n_jobs: int = -1, seed: int = 42) -> Tuple[float, float]:\n",
        "\n",
        "    y_pred = (y_score >= threshold).astype(int)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    seeds = rng.randint(np.iinfo(np.int32).max, size=n_bootstrap)\n",
        "\n",
        "    boot_metrics = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_bootstrap_single_metric)(metric_func, y_true, y_pred, seed=s) for s in seeds\n",
        "    )\n",
        "    return (np.percentile(boot_metrics, 2.5), np.percentile(boot_metrics, 97.5))\n",
        "\n",
        "\n",
        "def bootstrap_score_metric_ci(metric_func: Callable, y_true: np.ndarray, y_score: np.ndarray,\n",
        "                              n_bootstrap: int = 1000, n_jobs: int = -1, seed: int = 42) -> Tuple[float, float]:\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    seeds = rng.randint(np.iinfo(np.int32).max, size=n_bootstrap)\n",
        "    boot_metrics = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_bootstrap_single_score_metric)(metric_func, y_true, y_score, seed=s) for s in seeds\n",
        "    )\n",
        "    valid_metrics = [m for m in boot_metrics if not np.isnan(m)]\n",
        "    if not valid_metrics: return (0.0, 0.0)\n",
        "    return (np.percentile(valid_metrics, 2.5), np.percentile(valid_metrics, 97.5))\n",
        "\n",
        "def bootstrap_all_cis(y_true: np.ndarray, y_pred_proba: np.ndarray, threshold: float,\n",
        "                      n_bootstrap: int = 1000, n_jobs: int = -1, seed: int = 42) -> Dict[str, Tuple[float, float]]:\n",
        "\n",
        "    f1_func = lambda yt, yp: f1_score(yt, yp, zero_division=0)\n",
        "\n",
        "\n",
        "    metric_functions = {\n",
        "        'f1': f1_func,\n",
        "        'accuracy': accuracy_score,\n",
        "        'sensitivity': sensitivity_score,\n",
        "        'specificity': specificity_score\n",
        "    }\n",
        "\n",
        "    ci_results = {}\n",
        "\n",
        "    for name, func in metric_functions.items():\n",
        "        try:\n",
        "            ci_results[name] = bootstrap_metric_ci(\n",
        "                func, y_true, y_pred_proba, threshold, n_bootstrap, n_jobs, seed\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to calculate CI for {name}: {e}\")\n",
        "            ci_results[name] = (0.0, 1.0)\n",
        "\n",
        "    return ci_results\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_names_from_column_transformer(column_transformer) -> List[str]:\n",
        "\n",
        "    names = []\n",
        "    for name, pipe, cols in column_transformer.transformers_:\n",
        "        if name == 'remainder' and pipe == 'drop': continue\n",
        "        if name == 'cat' and hasattr(pipe, 'named_steps') and 'onehot' in pipe.named_steps:\n",
        "            try:\n",
        "                cats = pipe.named_steps['onehot'].categories_\n",
        "                for c, cat_list in zip(cols, cats):\n",
        "                    names.extend([f\"{c}_{v}\" for v in cat_list])\n",
        "            except Exception: names.extend(cols)\n",
        "        else: names.extend(cols)\n",
        "    return names\n",
        "\n",
        "def get_model_predictions(model_artifact: Dict, X_val: pd.DataFrame, model_name: str = \"\") -> Tuple[Optional[np.ndarray], int, List[str]]:\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Inspecting artifact '{model_name}'. Found keys: {list(model_artifact.keys())}\")\n",
        "        model = model_artifact.get('calibrated_model')\n",
        "        preprocessor = model_artifact.get('preprocessor')\n",
        "        metadata = model_artifact.get('metadata', {})\n",
        "\n",
        "        if not all([model, preprocessor]):\n",
        "            missing = [k for k,v in {'calibrated':model, 'preprocessor':preprocessor}.items() if not v]\n",
        "            raise ValueError(f\"Missing required components in '{model_name}': {missing}\")\n",
        "\n",
        "        selected_features = metadata.get('selected_features', [])\n",
        "        all_features = metadata.get('all_features', [])\n",
        "\n",
        "        try:\n",
        "            X_val_preprocessed = preprocessor.transform(X_val)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Preprocessing failed for '{model_name}': {e}\")\n",
        "            return None, 0, []\n",
        "\n",
        "\n",
        "        if not all_features:\n",
        "            try:\n",
        "                all_features = get_feature_names_from_column_transformer(preprocessor)\n",
        "            except Exception:\n",
        "                all_features = [f'feature_{i}' for i in range(X_val_preprocessed.shape[1])]\n",
        "\n",
        "\n",
        "\n",
        "        if hasattr(X_val_preprocessed, 'toarray'):\n",
        "            X_val_preprocessed = X_val_preprocessed.toarray()\n",
        "\n",
        "\n",
        "        current_cols_count = X_val_preprocessed.shape[1]\n",
        "\n",
        "        if current_cols_count != len(all_features):\n",
        "             logging.warning(f\"Feature count mismatch for '{model_name}'. Expected {len(all_features)}, got {current_cols_count}. Attempting re-indexing.\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            X_val_df_full = pd.DataFrame(X_val_preprocessed, columns=all_features, index=X_val.index)\n",
        "        except ValueError:\n",
        "\n",
        "            placeholder_cols = [f'_col_{i}' for i in range(current_cols_count)]\n",
        "            X_val_df_temp = pd.DataFrame(X_val_preprocessed, columns=placeholder_cols, index=X_val.index)\n",
        "\n",
        "            X_val_df_full = X_val_df_temp.reindex(columns=all_features, fill_value=0.0)\n",
        "\n",
        "\n",
        "        if selected_features:\n",
        "\n",
        "            X_val_final = X_val_df_full[selected_features].copy()\n",
        "\n",
        "\n",
        "            missing_features = set(selected_features) - set(X_val_final.columns)\n",
        "            if missing_features:\n",
        "                 logging.warning(f\"Model '{model_name}' missing {len(missing_features)} features in final set.\")\n",
        "\n",
        "            num_features_used = len(selected_features)\n",
        "            logging.info(f\"Selected {num_features_used} features for '{model_name}'\")\n",
        "\n",
        "        else:\n",
        "            logging.warning(f\"No selected features stored for '{model_name}', using all features.\")\n",
        "            X_val_final = X_val_df_full\n",
        "            num_features_used = X_val_final.shape[1]\n",
        "\n",
        "\n",
        "        X_val_final = X_val_final.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "        if X_val_final.shape[1] == 0:\n",
        "            logging.error(f\"No features available for prediction in '{model_name}'\")\n",
        "            return None, 0, []\n",
        "\n",
        "\n",
        "        try:\n",
        "            if \"lightgbm\" in str(type(model)).lower():\n",
        "                y_pred_proba = model.predict_proba(X_val_final, predict_disable_shape_check=True)[:, 1]\n",
        "            else: y_pred_proba = model.predict_proba(X_val_final)[:, 1]\n",
        "            logging.info(f\"âœ… Predictions generated for '{model_name}' using {num_features_used} features.\")\n",
        "            return y_pred_proba, num_features_used, list(X_val_final.columns)\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                logging.warning(\"Retrying prediction with numpy array\")\n",
        "                y_pred_proba = model.predict_proba(X_val_final.values)[:, 1]\n",
        "                logging.info(f\"âœ… Predictions generated for '{model_name}' using {num_features_used} features (array input).\")\n",
        "                return y_pred_proba, num_features_used, list(X_val_final.columns)\n",
        "            except Exception as e2:\n",
        "                logging.error(f\"âŒ Prediction failed for '{model_name}': {e2}\")\n",
        "                return None, 0, []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"âŒ Failed to get predictions for '{model_name}': {str(e)}\", exc_info=True)\n",
        "        return None, 0, []\n",
        "\n",
        "\n",
        "def create_subgroup_masks(df_processed: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
        "\n",
        "    masks = {}\n",
        "    if 'Age' in df_processed.columns:\n",
        "        age_median = df_processed['Age'].median()\n",
        "        masks['Age < Median'] = (df_processed['Age'] < age_median).values\n",
        "        masks['Age >= Median'] = (df_processed['Age'] >= age_median).values\n",
        "    categorical_features = [\n",
        "        'Institution Type_Lab', 'Institution Type_Hospital',\n",
        "        'Healthcare Sector_Governmental', 'Healthcare Sector_Private',\n",
        "        'Bacteria type_E. coli', 'Bacteria type_Klebsiella Spp', 'Bacteria type_Pseudomonas Spp',\n",
        "        'Gender_F', 'Gender_M'\n",
        "    ]\n",
        "    for feat in [f for f in categorical_features if f in df_processed.columns]:\n",
        "        masks[f'{feat} = 1'] = (df_processed[feat] == 1).values\n",
        "        masks[f'{feat} = 0'] = (df_processed[feat] == 0).values\n",
        "    return masks\n",
        "\n",
        "\n",
        "def evaluate_subgroup(y_true: np.ndarray, y_pred_proba: np.ndarray, mask: np.ndarray,\n",
        "                      thresholds: np.ndarray, n_bootstrap: int, n_jobs: int, seed: int) -> Optional[Dict]:\n",
        "\n",
        "    if not mask.any() or mask.sum() < 10:\n",
        "        return None\n",
        "    y_true_g, y_pred_proba_g = y_true[mask], y_pred_proba[mask]\n",
        "    if len(np.unique(y_true_g)) < 2:\n",
        "        return None\n",
        "\n",
        "    metrics = calculate_metrics(y_true_g, y_pred_proba_g, thresholds)\n",
        "\n",
        "\n",
        "    opt_idx = np.argmax(metrics['youden_j'])\n",
        "    opt_thr = float(thresholds[opt_idx])\n",
        "\n",
        "    results = {\n",
        "        'n_samples': int(mask.sum()),\n",
        "        'threshold_youden': opt_thr,\n",
        "        'youden_j': metrics['youden_j'][opt_idx],\n",
        "        'f1': metrics['f1_scores'][opt_idx],\n",
        "        'accuracy': metrics['accuracy'][opt_idx],\n",
        "        'sensitivity': metrics['sensitivity'][opt_idx],\n",
        "        'specificity': metrics['specificity'][opt_idx],\n",
        "        'balanced_accuracy': metrics['balanced_accuracies'][opt_idx],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        ci = bootstrap_all_cis(y_true_g, y_pred_proba_g, opt_thr, n_bootstrap, n_jobs, seed)\n",
        "        results.update({\n",
        "            'youden_j_ci': f\"[{ci.get('youden_j', (0,0))[0]:.3f}, {ci.get('youden_j', (0,0))[1]:.3f}]\",\n",
        "            'f1_ci': f\"[{ci['f1'][0]:.3f}, {ci['f1'][1]:.3f}]\",\n",
        "            'accuracy_ci': f\"[{ci['accuracy'][0]:.3f}, {ci['accuracy'][1]:.3f}]\",\n",
        "            'sensitivity_ci': f\"[{ci['sensitivity'][0]:.3f}, {ci['sensitivity'][1]:.3f}]\",\n",
        "            'specificity_ci': f\"[{ci['specificity'][0]:.3f}, {ci['specificity'][1]:.3f}]\",\n",
        "        })\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Bootstrap CI calculation failed: {e}\")\n",
        "        results.update({\n",
        "            'f1_ci': \"[0.000, 1.000]\",\n",
        "            'accuracy_ci': \"[0.000, 1.000]\",\n",
        "            'sensitivity_ci': \"[0.000, 1.000]\",\n",
        "            'specificity_ci': \"[0.000, 1.000]\",\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_subgroup_analysis(y_pred_proba: np.ndarray, y_true: np.ndarray, df_processed: pd.DataFrame,\n",
        "                          thresholds: np.ndarray, n_bootstrap: int, n_jobs: int, seed: int) -> Dict[str, Dict]:\n",
        "\n",
        "    masks = create_subgroup_masks(df_processed)\n",
        "    results = {}\n",
        "    for group_name, mask in masks.items():\n",
        "        subgroup_result = evaluate_subgroup(y_true, y_pred_proba, mask, thresholds, n_bootstrap, n_jobs, seed)\n",
        "        if subgroup_result:\n",
        "            results[group_name] = subgroup_result\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "def _compute_ci_for_threshold(y_true, y_pred_proba, bootstrap_indices, metric_key, threshold):\n",
        "    boot_vals = []\n",
        "    for idx in bootstrap_indices:\n",
        "        try:\n",
        "            y_true_b, y_proba_b = y_true[idx], y_pred_proba[idx]\n",
        "            y_pred_b = (y_proba_b >= threshold).astype(int)\n",
        "            if metric_key == 'f1_scores': value = f1_score(y_true_b, y_pred_b, zero_division=0)\n",
        "            elif metric_key == 'accuracy': value = accuracy_score(y_true_b, y_pred_b)\n",
        "            elif metric_key == 'sensitivity':\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true_b, y_pred_b, labels=[0,1]).ravel()\n",
        "                value = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            elif metric_key == 'specificity':\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true_b, y_pred_b, labels=[0,1]).ravel()\n",
        "                value = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "            elif metric_key == 'youden_j':\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true_b, y_pred_b, labels=[0,1]).ravel()\n",
        "                sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "                spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "                value = sens + spec - 1\n",
        "            elif metric_key == 'balanced_accuracies': value = balanced_accuracy_score(y_true_b, y_pred_b)\n",
        "            else: value = 0.0\n",
        "            boot_vals.append(value)\n",
        "        except Exception: boot_vals.append(0.0)\n",
        "    if boot_vals: return np.percentile(boot_vals, 2.5), np.percentile(boot_vals, 97.5)\n",
        "    else: return 0.0, 1.0\n",
        "\n",
        "def plot_threshold_metrics(metrics: dict, optimal_thresholds: dict, metrics_at_optimal: dict, model_name: str,\n",
        "                           output_path: str, y_true: np.ndarray, y_pred_proba: np.ndarray,\n",
        "                           n_bootstrap_plot: int = 1000, seed: int = 42) -> None:\n",
        "\n",
        "\n",
        "    matplotlib.rcParams.update({'font.family': 'DejaVu Sans', 'font.size': 7, 'axes.titlesize': 7, 'axes.labelsize': 7, 'xtick.labelsize': 6, 'ytick.labelsize': 6, 'legend.fontsize': 7, 'lines.linewidth': 1.1, 'axes.linewidth': 0.7, 'figure.dpi': 600})\n",
        "    color_palette = sns.color_palette(\"colorblind\", n_colors=6)\n",
        "    metric_map = {'f1_scores': ('F1 score', color_palette[0]), 'accuracy': ('Accuracy', color_palette[1]), 'sensitivity': ('Sensitivity', color_palette[2]), 'specificity': ('Specificity', color_palette[3]), 'youden_j': (\"Youden's J\", color_palette[4]), 'balanced_accuracies': ('Balanced Acc.', color_palette[5])}\n",
        "    rng = np.random.RandomState(seed)\n",
        "    bootstrap_indices = [rng.choice(len(y_true), size=len(y_true), replace=True) for _ in range(n_bootstrap_plot)]\n",
        "    thresholds = metrics['thresholds']\n",
        "    ci_bands = {}\n",
        "    for metric_key, (label, color) in metric_map.items():\n",
        "        results = Parallel(n_jobs=-1)(delayed(_compute_ci_for_threshold)(y_true, y_pred_proba, bootstrap_indices, metric_key, t) for t in thresholds)\n",
        "        lower, upper = [r[0] for r in results], [r[1] for r in results]\n",
        "        ci_bands[metric_key] = (lower, upper)\n",
        "    fig, ax = plt.subplots(figsize=(6.5, 3.4))\n",
        "    for key, (label, color) in metric_map.items():\n",
        "        ax.plot(thresholds, metrics[key], label=label, color=color, linewidth=1.2)\n",
        "        ax.fill_between(thresholds, ci_bands[key][0], ci_bands[key][1], color=color, alpha=0.15, linewidth=0)\n",
        "    line_styles = {'F1': ('--', 'black'), 'YoudenJ': (':', color_palette[4]), 'BalancedAcc': ('-.', color_palette[5])}\n",
        "    for name, (linestyle, col) in line_styles.items():\n",
        "        thr = optimal_thresholds.get(name)\n",
        "        if thr is not None:\n",
        "            display_label = {\"F1\": \"F1\", \"YoudenJ\": \"Youden's J\", \"BalancedAcc\": \"Balanced Acc\"}[name]\n",
        "            ax.axvline(thr, ls=linestyle, color=col, label=f\"{display_label} thr: {thr:.2f}\", linewidth=1.1)\n",
        "    ax.set_xlabel(\"Threshold\", labelpad=4); ax.set_ylabel(\"Metric value\", labelpad=4)\n",
        "    ax.set_ylim(-0.03, 1.03); ax.set_xlim(thresholds[0], thresholds[-1])\n",
        "    ax.xaxis.set_major_locator(MultipleLocator(0.2)); ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
        "    ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
        "    ax.grid(axis='y', linestyle=\"--\", color=\"gray\", alpha=0.14, linewidth=0.65)\n",
        "    ax.tick_params(direction='out', length=4.5, width=0.75)\n",
        "    for spine in ['right', 'top']: ax.spines[spine].set_visible(False)\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1.01, 0.51), frameon=False, borderaxespad=0)\n",
        "    fig.suptitle(f\"Threshold-wise performance curves ({model_name})\", y=1.01, fontsize=8.8, fontweight='regular')\n",
        "    plt.tight_layout(rect=(0, 0, 1, 0.965))\n",
        "    output_path = Path(output_path)\n",
        "    if output_path.suffix.lower() != '.png': output_path = output_path.with_suffix('.png')\n",
        "    fig.savefig(str(output_path), dpi=600, bbox_inches='tight', facecolor='white')\n",
        "    fig.savefig(str(output_path.with_suffix('.pdf')), bbox_inches='tight', facecolor='white')\n",
        "    plt.close(fig)\n",
        "    print(f\"threshold metrics plot saved to: {output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def print_summary_table(title: str, results_dict: dict, is_global: bool = False):\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"ðŸ” {title}\")\n",
        "    print(\"=\" * 80)\n",
        "    if is_global:\n",
        "        header = f\"{'Metric':<20} {'Value':<10} {'95% CI':<20} {'Optimal Threshold'}\"\n",
        "        print(header)\n",
        "        print(\"-\" * len(header))\n",
        "        g = results_dict\n",
        "\n",
        "        print(f\"{'Youden\\'s J':<20} {g['youden_j']:<10.3f} {'-':<20} {g['threshold_youdenj']:.2f} (Youden)\")\n",
        "        print(f\"{'Sensitivity':<20} {g['sensitivity']:<10.3f} {g['sensitivity_ci']}\")\n",
        "        print(f\"{'Specificity':<20} {g['specificity']:<10.3f} {g['specificity_ci']}\")\n",
        "        print(f\"{'F1 Score':<20} {g['f1']:<10.3f} {g['f1_ci']}\")\n",
        "        print(f\"{'Accuracy':<20} {g['accuracy']:<10.3f} {g['accuracy_ci']}\")\n",
        "        print(f\"{'Balanced Accuracy':<20} {g['balanced_accuracy']:<10.3f}\")\n",
        "    else:\n",
        "\n",
        "        header = f\"{'Subgroup':<35} {'N':<6} {'Youden':<8} {'Sens':<8} {'Spec':<8} {'Threshold':<10}\"\n",
        "        print(header)\n",
        "        print(\"-\" * len(header))\n",
        "        for group, metrics in results_dict.items():\n",
        "            print(f\"{group:<35} {metrics['n_samples']:<6} {metrics['youden_j']:<8.3f} \"\n",
        "                  f\"{metrics['sensitivity']:<8.3f} {metrics['specificity']:<8.3f} {metrics['threshold_youden']:<10.2f}\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "def save_results_to_csv(data: List[Dict], output_path: Path):\n",
        "\n",
        "    if not data:\n",
        "        logging.warning(f\"No data to save to {output_path}.\")\n",
        "        return\n",
        "    try:\n",
        "        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "        logging.info(f\"ðŸ’¾ Saved results to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save CSV to {output_path}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    models_dir = Path(args.models_dir)\n",
        "    output_dir = Path(args.output_dir)\n",
        "    plots_dir = output_dir / 'plots'; plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "    reports_dir = output_dir / 'reports'; reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        validation_data = pd.read_csv(args.data_path)\n",
        "        if args.target_col not in validation_data.columns: raise ValueError(f\"Target column '{args.target_col}' not found.\")\n",
        "        X_val = validation_data.drop(columns=[args.target_col])\n",
        "        y_val = validation_data[args.target_col].values\n",
        "        logging.info(f\"Loaded validation data: {X_val.shape[0]} samples from {args.data_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal: Failed to load data. {e}\", exc_info=True)\n",
        "        return\n",
        "    global_results_list = []\n",
        "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
        "    model_files = list(models_dir.glob('*_final_calibrated_model.pkl'))\n",
        "    if not model_files:\n",
        "        logging.error(f\"No final model files (*_final_calibrated_model.pkl) found in '{models_dir}'.\")\n",
        "        return\n",
        "    for model_file in model_files:\n",
        "        model_name = model_file.stem.replace('_final_calibrated_model', '')\n",
        "        logging.info(f\"\\n{'='*40}\\nEVALUATING MODEL: {model_name}\\n{'='*40}\")\n",
        "        try:\n",
        "            artifact = joblib.load(model_file)\n",
        "            y_pred_proba, n_features, sel_features = get_model_predictions(artifact, X_val, model_name)\n",
        "            if y_pred_proba is None: continue\n",
        "\n",
        "\n",
        "            global_metrics = calculate_metrics(y_val, y_pred_proba, thresholds)\n",
        "\n",
        "\n",
        "            opt_f1_idx = np.argmax(global_metrics['f1_scores'])\n",
        "            opt_thr_f1 = float(thresholds[opt_f1_idx])\n",
        "\n",
        "            opt_youdenj_idx = np.argmax(global_metrics['youden_j'])\n",
        "            opt_thr_youdenj = float(thresholds[opt_youdenj_idx])\n",
        "\n",
        "            opt_balacc_idx = np.argmax(global_metrics['balanced_accuracies'])\n",
        "            opt_thr_balacc = float(thresholds[opt_balacc_idx])\n",
        "\n",
        "\n",
        "            global_ci = bootstrap_all_cis(y_val, y_pred_proba, opt_thr_youdenj, args.n_bootstrap, args.n_jobs, args.seed)\n",
        "\n",
        "            metrics_at_f1 = {'f1': global_metrics['f1_scores'][opt_f1_idx], 'accuracy': global_metrics['accuracy'][opt_f1_idx], 'sensitivity': global_metrics['sensitivity'][opt_f1_idx], 'specificity': global_metrics['specificity'][opt_f1_idx], 'youden_j': global_metrics['youden_j'][opt_f1_idx], 'balanced_accuracy': global_metrics['balanced_accuracies'][opt_f1_idx]}\n",
        "            metrics_at_youdenj = {'f1': global_metrics['f1_scores'][opt_youdenj_idx], 'accuracy': global_metrics['accuracy'][opt_youdenj_idx], 'sensitivity': global_metrics['sensitivity'][opt_youdenj_idx], 'specificity': global_metrics['specificity'][opt_youdenj_idx], 'youden_j': global_metrics['youden_j'][opt_youdenj_idx], 'balanced_accuracy': global_metrics['balanced_accuracies'][opt_youdenj_idx]}\n",
        "            metrics_at_balacc = {'f1': global_metrics['f1_scores'][opt_balacc_idx], 'accuracy': global_metrics['accuracy'][opt_balacc_idx], 'sensitivity': global_metrics['sensitivity'][opt_balacc_idx], 'specificity': global_metrics['specificity'][opt_balacc_idx], 'youden_j': global_metrics['youden_j'][opt_balacc_idx], 'balanced_accuracy': global_metrics['balanced_accuracies'][opt_balacc_idx]}\n",
        "\n",
        "            optimal_thresholds = MappingProxyType({'F1': opt_thr_f1, 'YoudenJ': opt_thr_youdenj, 'BalancedAcc': opt_thr_balacc})\n",
        "            metrics_at_optimal = {'F1': metrics_at_f1, 'YoudenJ': metrics_at_youdenj, 'BalancedAcc': metrics_at_balacc}\n",
        "\n",
        "\n",
        "            result_row = {\n",
        "                'model': model_name,\n",
        "                'n_features': n_features,\n",
        "                'threshold_f1': opt_thr_f1,\n",
        "                'threshold_youdenj': opt_thr_youdenj,\n",
        "                'threshold_balancedacc': opt_thr_balacc,\n",
        "\n",
        "                'youden_j': metrics_at_youdenj['youden_j'],\n",
        "                'f1': metrics_at_youdenj['f1'],\n",
        "                'f1_ci': f\"[{global_ci['f1'][0]:.3f}, {global_ci['f1'][1]:.3f}]\",\n",
        "                'accuracy': metrics_at_youdenj['accuracy'],\n",
        "                'accuracy_ci': f\"[{global_ci['accuracy'][0]:.3f}, {global_ci['accuracy'][1]:.3f}]\",\n",
        "                'balanced_accuracy': metrics_at_youdenj['balanced_accuracy'],\n",
        "                'sensitivity': metrics_at_youdenj['sensitivity'],\n",
        "                'sensitivity_ci': f\"[{global_ci['sensitivity'][0]:.3f}, {global_ci['sensitivity'][1]:.3f}]\",\n",
        "                'specificity': metrics_at_youdenj['specificity'],\n",
        "                'specificity_ci': f\"[{global_ci['specificity'][0]:.3f}, {global_ci['specificity'][1]:.3f}]\",\n",
        "                'selected_features_preview': str(sel_features[:3]) + '...' if len(sel_features) > 3 else str(sel_features)\n",
        "            }\n",
        "            global_results_list.append(result_row)\n",
        "            print_summary_table(f\"GLOBAL RESULTS: {model_name}\", result_row, is_global=True)\n",
        "\n",
        "            preprocessor = artifact.get('preprocessor')\n",
        "            if preprocessor:\n",
        "                X_val_processed = pd.DataFrame(preprocessor.transform(X_val), columns=get_feature_names_from_column_transformer(preprocessor))\n",
        "                subgroup_results = run_subgroup_analysis(y_pred_proba, y_val, X_val_processed, thresholds, args.n_bootstrap, args.n_jobs, args.seed)\n",
        "                if subgroup_results:\n",
        "                    print_summary_table(f\"SUBGROUP RESULTS: {model_name}\", subgroup_results)\n",
        "                    subgroup_data = []\n",
        "                    for k, v in subgroup_results.items():\n",
        "                        row = {\n",
        "                            'model': model_name,\n",
        "                            'group': k,\n",
        "\n",
        "                            'opt_youden_threshold': v['threshold_youden'],\n",
        "                            **v\n",
        "                        }\n",
        "                        subgroup_data.append(row)\n",
        "                    save_results_to_csv(subgroup_data, reports_dir / f\"{model_name}_subgroups_optimal_thresholds_summary.csv\")\n",
        "            else: logging.warning(f\"Could not find a preprocessor for '{model_name}'. Skipping subgroup analysis.\")\n",
        "\n",
        "            if args.plot_results:\n",
        "                plot_path = plots_dir / f\"{model_name}_threshold_metrics_plot.png\"\n",
        "                plot_threshold_metrics(global_metrics, dict(optimal_thresholds), metrics_at_optimal, model_name, plot_path, y_val, y_pred_proba, n_bootstrap_plot=args.n_bootstrap_plot, seed=args.seed)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Unexpected error while processing model '{model_name}': {e}\", exc_info=True)\n",
        "            continue\n",
        "    if global_results_list:\n",
        "        save_results_to_csv(global_results_list, reports_dir / \"all_models_global_results.csv\")\n",
        "    logging.info(f\"\\n=== EVALUATION COMPLETE ===\\nProcessed {len(global_results_list)} models successfully.\")\n",
        "    logging.info(f\"All outputs saved in '{output_dir}'. Log file: '{log_filename}'\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    NCV_DEFAULT_MODELS_PATH = str(Path(\"nested_cv_output\") / \"logloss\" / \"models\")\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Enhanced Model Evaluation Pipeline with Comprehensive Subgroup Analysis.\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument('--data_path', type=str, default='validation_set.csv', help='Path to the validation data CSV file.')\n",
        "\n",
        "    parser.add_argument('--models_dir', type=str, default=NCV_DEFAULT_MODELS_PATH,\n",
        "                        help='Directory containing trained model .pkl files (e.g., nested_cv_output/logloss/models).')\n",
        "\n",
        "    parser.add_argument('--output_dir', type=str, default='Threshold_results', help='Directory to save all outputs.')\n",
        "    parser.add_argument('--target_col', type=str, default='MDR status', help='Name of the target/outcome column in the data file.')\n",
        "    parser.add_argument('--n_bootstrap', type=int, default=1000, help='Number of bootstrap iterations for confidence intervals.')\n",
        "    parser.add_argument('--n_bootstrap_plot', type=int, default=1000, help='Number of bootstrap iterations for plot CI bands.')\n",
        "    parser.add_argument('--n_jobs', type=int, default=-1, help='Number of CPU cores for parallel processing (-1 means all).')\n",
        "    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility of bootstrap sampling.')\n",
        "    parser.add_argument('--no-plots', action='store_false', dest='plot_results', help='Add this flag to disable plot generation.')\n",
        "\n",
        "    if 'ipykernel' in sys.modules and 'google.colab' in sys.modules:\n",
        "        logging.info(\"Running in Colab environment - using default arguments.\")\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "gDgmsgELjmgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Model Re-evaluation (Post-Threshold Optimization)**          \n",
        "Reassess model performance on the validation set after applying optimized thresholds."
      ],
      "metadata": {
        "id": "drUabLV_jqPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install -q statsmodels mlxtend joblib seaborn matplotlib pandas numpy scikit-learn scipy\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "import joblib\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, average_precision_score, balanced_accuracy_score,\n",
        "    brier_score_loss, cohen_kappa_score, confusion_matrix, f1_score,\n",
        "    matthews_corrcoef, precision_score, recall_score, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, log_loss\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from scipy.special import logit\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from mlxtend.evaluate import mcnemar_table\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from dataclasses import dataclass\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict\n",
        "from scipy.special import expit as sigmoid_func\n",
        "from scipy.special import expit\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from scipy.special import expit, logit\n",
        "\n",
        "\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels not available - some statistical functions will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "    MULTITEST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MULTITEST_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.multitest not available - multiple comparison corrections will be limited\")\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.contingency_tables import mcnemar\n",
        "    MCNEMAR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MCNEMAR_AVAILABLE = False\n",
        "    warnings.warn(\"statsmodels.stats.contingency_tables not available - McNemar test will be limited\")\n",
        "\n",
        "try:\n",
        "    from mlxtend.evaluate import mcnemar_table\n",
        "    MLXTEND_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLXTEND_AVAILABLE = False\n",
        "    warnings.warn(\"mlxtend not available - McNemar table functionality will be limited\")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, brier_score_loss,\n",
        "    matthews_corrcoef, cohen_kappa_score, roc_curve,\n",
        "    precision_recall_curve, balanced_accuracy_score, log_loss, confusion_matrix\n",
        ")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Environment setup complete!\")\n",
        "\n",
        "\n",
        "\n",
        "for folder in (\"Evaluation2\", \"model_comparison\"):\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class MetricResult:\n",
        "    value: float\n",
        "    cilow: float\n",
        "    cihigh: float\n",
        "    name: str = \"\"\n",
        "    description: str = \"\"\n",
        "\n",
        "class HybridFeatureSelector:\n",
        "\n",
        "\n",
        "    def __init__(self, selected_features=None):\n",
        "        self.selected_features_ = selected_features or []\n",
        "        self.selected_features = selected_features or []\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        if hasattr(X, 'columns'):\n",
        "            return X[self.selected_features_] if self.selected_features_ else X\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "\n",
        "def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
        "\n",
        "    metrics = {}\n",
        "    n_bins = 10\n",
        "\n",
        "\n",
        "    metrics['AUC_ROC'] = roc_auc_score(y_true, y_prob)\n",
        "    metrics['AUPRC'] = average_precision_score(y_true, y_prob)\n",
        "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['Balanced_Accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
        "    metrics['Precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['Recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['Specificity'] = specificity_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['F1'] = f1_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['MCC'] = matthews_corrcoef(y_true, y_pred)\n",
        "    metrics['Kappa'] = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "    metrics['LogLoss'] = log_loss(y_true, y_prob)\n",
        "    metrics['Brier'] = brier_score_loss(y_true, y_prob)\n",
        "\n",
        "\n",
        "    metrics['ECE'] = calculate_ece(y_true, y_prob, n_bins=n_bins, method='quantile')\n",
        "    metrics['MCE'] = calculate_mce(y_true, y_prob, n_bins=n_bins, method='quantile')\n",
        "\n",
        "\n",
        "    intercept, slope = calibration_slope_intercept(y_true, y_prob)\n",
        "    metrics['Cal_Intercept'] = intercept\n",
        "    metrics['Cal_Slope'] = slope\n",
        "\n",
        "\n",
        "\n",
        "    z_statistic, z_p_value = np.nan, np.nan\n",
        "    hl_statistic, hl_p_value = np.nan, np.nan\n",
        "\n",
        "    try:\n",
        "\n",
        "        z_statistic, z_p_value = z_test_standard(y_true, y_prob)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Z-test failed in metric calculation: {e}. Defaulting to NaN.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        hl_statistic, hl_p_value = hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=n_bins)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"HL-test failed in metric calculation: {e}. Defaulting to NaN.\")\n",
        "\n",
        "    metrics['Spiegelhalter_Z'] = z_statistic\n",
        "    metrics['Spiegelhalter_P_Value'] = z_p_value\n",
        "\n",
        "    metrics['HL_Statistic'] = hl_statistic\n",
        "    metrics['HL_P_Value'] = hl_p_value\n",
        "\n",
        "    extended_metrics = classification_extended_metrics(y_true, y_pred)\n",
        "    metrics.update(extended_metrics)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def classification_extended_metrics(y_true, y_pred):\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
        "        'NPV': npv, 'PPV': ppv, 'FPR': fpr, 'FNR': fnr\n",
        "    }\n",
        "\n",
        "def bootstrap_metrics_ci_for_table(y_true, y_pred, y_prob, n_bootstrap=1000) -> Dict[str, float]:\n",
        "\n",
        "    ci_results = {}\n",
        "\n",
        "\n",
        "    z_stat, z_p = np.nan, np.nan\n",
        "    hl_stat, hl_p = np.nan, np.nan\n",
        "\n",
        "\n",
        "    point_intercept, point_slope = np.nan, np.nan\n",
        "    intercept_ci_low, intercept_ci_high = np.nan, np.nan\n",
        "    slope_ci_low, slope_ci_high = np.nan, np.nan\n",
        "\n",
        "\n",
        "    def _add_ci(metric_name, metric_fn, use_y_pred=False, **kwargs):\n",
        "\n",
        "\n",
        "        y_score = y_pred if use_y_pred else y_prob\n",
        "\n",
        "\n",
        "        _, cilow, cihigh = bootstrap_metric_ci(\n",
        "            y_true=y_true,\n",
        "            y_score=y_score,\n",
        "            metric_fn=metric_fn,\n",
        "            n_bootstrap=n_bootstrap,\n",
        "            **kwargs\n",
        "        )\n",
        "        ci_results[f'{metric_name}_CI_Low'] = cilow\n",
        "        ci_results[f'{metric_name}_CI_High'] = cihigh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    _add_ci('AUC_ROC', roc_auc_score)\n",
        "    _add_ci('AUPRC', average_precision_score)\n",
        "\n",
        "\n",
        "    _add_ci('F1', f1_score, use_y_pred=True, zero_division=0)\n",
        "    _add_ci('Accuracy', accuracy_score, use_y_pred=True)\n",
        "    _add_ci('Precision', precision_score, use_y_pred=True, zero_division=0)\n",
        "    _add_ci('Recall', recall_score, use_y_pred=True, zero_division=0)\n",
        "    _add_ci('Specificity', specificity_score, use_y_pred=True, zero_division=0)\n",
        "    _add_ci('MCC', matthews_corrcoef, use_y_pred=True)\n",
        "    _add_ci('Kappa', cohen_kappa_score, use_y_pred=True)\n",
        "    _add_ci('Balanced_Accuracy', balanced_accuracy_score, use_y_pred=True)\n",
        "\n",
        "\n",
        "    _add_ci('Brier', brier_score_loss)\n",
        "    _add_ci('LogLoss', log_loss)\n",
        "\n",
        "\n",
        "    _add_ci('ECE', calculate_ece, n_bins=10, method='quantile')\n",
        "    _add_ci('MCE', calculate_mce, n_bins=10, method='quantile')\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        point_intercept, point_slope, i_ci, s_ci = calibration_slope_intercept_ci(y_true, y_prob, n_bootstrap=n_bootstrap)\n",
        "\n",
        "\n",
        "        intercept_ci_low, intercept_ci_high = i_ci\n",
        "        slope_ci_low, slope_ci_high = s_ci\n",
        "        point_intercept, point_slope = point_intercept, point_slope\n",
        "\n",
        "\n",
        "        z_stat, z_p = z_test_standard(y_true, y_prob)\n",
        "        hl_stat, hl_p = hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=10)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        logging.warning(f\"Failed to calculate specialized CIs/Stats: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    ci_results['Cal_Intercept_CI_Low'] = intercept_ci_low\n",
        "    ci_results['Cal_Intercept_CI_High'] = intercept_ci_high\n",
        "    ci_results['Cal_Slope_CI_Low'] = slope_ci_low\n",
        "    ci_results['Cal_Slope_CI_High'] = slope_ci_high\n",
        "\n",
        "\n",
        "    ci_results['Spiegelhalter_P_Value'] = z_p\n",
        "    ci_results['HL_P_Value'] = hl_p\n",
        "\n",
        "    return ci_results\n",
        "\n",
        "\n",
        "\n",
        "def enforce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df.dropna(axis=0, how=\"all\", inplace=True)\n",
        "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_feature_names_from_column_transformer(ct) -> list:\n",
        "\n",
        "    feature_names = []\n",
        "    if not hasattr(ct, 'transformers_'):\n",
        "        return []\n",
        "    for name, transformer, cols in ct.transformers_:\n",
        "        if name == \"remainder\" and transformer == \"drop\":\n",
        "            continue\n",
        "        if hasattr(transformer, \"steps\"):\n",
        "            transformer = transformer.steps[-1][1]\n",
        "        if hasattr(transformer, \"get_feature_names_out\"):\n",
        "            try:\n",
        "                names = transformer.get_feature_names_out(cols)\n",
        "                feature_names.extend(names)\n",
        "                continue\n",
        "            except Exception:\n",
        "                pass\n",
        "        if hasattr(transformer, \"categories_\"):\n",
        "            cats = transformer.categories_\n",
        "            for i, col in enumerate(cols):\n",
        "                feature_names.extend([f\"{col}_{cat}\" for cat in cats[i]])\n",
        "        else:\n",
        "            feature_names.extend(cols if isinstance(cols, list) else [cols])\n",
        "    return feature_names\n",
        "\n",
        "def normalize_model_name(name: str) -> str:\n",
        "\n",
        "    name = re.sub(r'(_FINAL)?(_final)?_?calibrated_?model\\.?pkl?', '', name, flags=re.IGNORECASE)\n",
        "    name = re.sub(r'\\bv\\d+\\b', '', name)\n",
        "    name = re.sub(r'[-_]', ' ', name)\n",
        "\n",
        "    model_map = {\n",
        "        \"catboost\": \"CatBoost\",\n",
        "        \"lightgbm\": \"LightGBM\",\n",
        "        \"logisticregression\": \"LogisticRegression\",\n",
        "        \"logistic regression\": \"LogisticRegression\",\n",
        "        \"randomforest\": \"RandomForest\",\n",
        "        \"random forest\": \"RandomForest\",\n",
        "        \"svm\": \"SVM\",\n",
        "        \"support vector machine\": \"SVM\",\n",
        "        \"xgboost\": \"XGBoost\"\n",
        "    }\n",
        "\n",
        "    name_lower = name.lower().strip()\n",
        "    for key, normalized in model_map.items():\n",
        "        if key in name_lower:\n",
        "            return normalized\n",
        "    return name.strip().title()\n",
        "\n",
        "def get_model_predictions_comprehensive(artifact: dict, x_val: pd.DataFrame) -> Tuple[Optional[np.ndarray], int, List[str]]:\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Inspecting artifact: Found keys: {list(artifact.keys())}\")\n",
        "        model = artifact[\"calibrated_model\"]\n",
        "        preprocessor = artifact[\"preprocessor\"]\n",
        "        selector = artifact.get(\"selector\")\n",
        "        metadata = artifact.get(\"metadata\", {})\n",
        "        predefined = metadata.get(\"selected_features\")\n",
        "\n",
        "        if not all([model, preprocessor]):\n",
        "            missing = [k for k, v in {'calibrated': model, 'preprocessor': preprocessor}.items() if not v]\n",
        "            raise ValueError(f\"Missing required components: {missing}\")\n",
        "\n",
        "\n",
        "        x_proc = preprocessor.transform(x_val)\n",
        "\n",
        "\n",
        "        if hasattr(x_proc, 'toarray'):\n",
        "            x_proc = x_proc.toarray()\n",
        "\n",
        "        feat_names = get_feature_names_from_column_transformer(preprocessor)\n",
        "\n",
        "\n",
        "        x_df_full = pd.DataFrame(x_proc, index=x_val.index)\n",
        "\n",
        "\n",
        "        if x_df_full.shape[1] == len(feat_names):\n",
        "             x_df_full.columns = feat_names\n",
        "        else:\n",
        "\n",
        "             x_df_full.columns = [f'__temp_f_{i}' for i in range(x_df_full.shape[1])]\n",
        "\n",
        "\n",
        "        x_df_aligned = x_df_full.reindex(columns=feat_names, fill_value=0.0)\n",
        "\n",
        "\n",
        "        if selector is not None and hasattr(selector, \"transform\"):\n",
        "            x_sel = selector.transform(x_df_aligned)\n",
        "            sel_names = list(getattr(selector, 'selected_features_', []))\n",
        "        elif predefined:\n",
        "\n",
        "            x_sel = x_df_aligned[predefined].copy()\n",
        "            sel_names = predefined\n",
        "\n",
        "\n",
        "            missing = set(predefined) - set(x_sel.columns)\n",
        "            if missing:\n",
        "                logging.error(f\"Missing critical features: {list(missing)[:3]}...\")\n",
        "                return None, 0, []\n",
        "        else:\n",
        "            logging.warning(\"No feature selection applied, using all aligned features.\")\n",
        "            x_sel = x_df_aligned\n",
        "            sel_names = feat_names\n",
        "\n",
        "\n",
        "        x_sel = enforce_numeric(x_sel).fillna(0)\n",
        "        num_features_used = x_sel.shape[1]\n",
        "\n",
        "        if num_features_used == 0:\n",
        "            logging.error(\"No features available for prediction.\")\n",
        "            return None, 0, []\n",
        "\n",
        "        try:\n",
        "\n",
        "            if \"lightgbm\" in str(type(model)).lower():\n",
        "                y_pred_proba = model.predict_proba(x_sel, predict_disable_shape_check=True)[:, 1]\n",
        "            else:\n",
        "                y_pred_proba = model.predict_proba(x_sel)[:, 1]\n",
        "\n",
        "            logging.info(f\"âœ… Predictions generated using {num_features_used} features.\")\n",
        "            return y_pred_proba, num_features_used, sel_names\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            try:\n",
        "                logging.warning(\"Prediction failed with DataFrame input. Retrying with NumPy array.\")\n",
        "                y_pred_proba = model.predict_proba(x_sel.values)[:, 1]\n",
        "                return y_pred_proba, num_features_used, sel_names\n",
        "            except Exception as e2:\n",
        "                logging.error(f\"âŒ Final prediction failed: {e2}\")\n",
        "                return None, 0, []\n",
        "\n",
        "    except Exception as exc:\n",
        "        logging.error(f\"âŒ Failed to process model artifact: {str(exc)}\", exc_info=True)\n",
        "        return None, 0, []\n",
        "\n",
        "\n",
        "def load_validation_data(path=\"validation_set.csv\"):\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        logging.error(\"Validation data not found: %s\", path)\n",
        "        return None, None\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        return df.drop(columns=\"MDR status\"), df[\"MDR status\"].values\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading data: %s\", e)\n",
        "        return None, None\n",
        "\n",
        "def load_calibrated_models(model_dir=\"trained_models\") -> Dict[str, dict]:\n",
        "\n",
        "    models: Dict[str, dict] = {}\n",
        "    p = Path(model_dir)\n",
        "    if not p.exists():\n",
        "        logging.error(\"Model directory not found: %s\", model_dir)\n",
        "        return models\n",
        "\n",
        "    files = sorted(list(p.glob(\"*_FINAL_final_calibrated_model.pkl\")) +\n",
        "                   list(p.glob(\"*_final_calibrated_model.pkl\")))\n",
        "\n",
        "    if not files:\n",
        "        logging.error(\"No model files found in %s\", model_dir)\n",
        "        return models\n",
        "\n",
        "    for f in files:\n",
        "        try:\n",
        "            name = normalize_model_name(f.stem)\n",
        "            art = joblib.load(f)\n",
        "\n",
        "            if \"calibrated\" in art and \"calibrated_model\" not in art:\n",
        "                art[\"calibrated_model\"] = art[\"calibrated\"]\n",
        "            if \"selector\" in art and \"feature_selector\" not in art:\n",
        "                art[\"feature_selector\"] = art[\"selector\"]\n",
        "            if \"metadata\" in art and \"selected_features\" in art[\"metadata\"]:\n",
        "                art[\"selected_features\"] = art[\"metadata\"][\"selected_features\"]\n",
        "\n",
        "            required_keys = {\"calibrated_model\", \"preprocessor\"}\n",
        "            if not required_keys.issubset(art.keys()):\n",
        "                missing = required_keys - set(art.keys())\n",
        "                logging.warning(\"Skipping %s - missing keys: %s\", f.name, missing)\n",
        "                continue\n",
        "\n",
        "            models[name] = {\"artifact\": art}\n",
        "            logging.info(\"Loaded model: %s\", name)\n",
        "        except Exception as exc:\n",
        "            logging.error(\"Failed to load %s: %s\", f.name, exc)\n",
        "    return models\n",
        "\n",
        "def load_optimal_thresholds(path=\"Threshold_results/reports/all_models_global_results.csv\") -> Dict[str, float]:\n",
        "\n",
        "    paths = [\n",
        "        path,\n",
        "        \"Threshold_results/all_models_global_results.csv\",\n",
        "        \"all_models_global_results.csv\",\n",
        "    ]\n",
        "    for fp in paths:\n",
        "        if not os.path.exists(fp):\n",
        "            continue\n",
        "        try:\n",
        "            df = pd.read_csv(fp)\n",
        "            model_col = \"Model\" if \"Model\" in df.columns else \"model\"\n",
        "            if model_col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            df[\"Normalized_Model\"] = df[model_col].apply(normalize_model_name)\n",
        "\n",
        "            threshold_col = None\n",
        "            for col_name in [\"threshold_youdenj\", \"youden_threshold\", \"Threshold\", \"threshold\"]:\n",
        "                if col_name in df.columns:\n",
        "                    threshold_col = col_name\n",
        "                    break\n",
        "\n",
        "            if threshold_col:\n",
        "                print(f\"DEBUG: Loading thresholds from column '{threshold_col}'\")\n",
        "                return df.set_index(\"Normalized_Model\")[threshold_col].to_dict()\n",
        "            else:\n",
        "                logging.warning(\"No threshold column found in %s\", fp)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Error loading thresholds from %s: %s\", fp, e)\n",
        "\n",
        "    logging.warning(\"Using default threshold 0.5\")\n",
        "    return {}\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap_metric_ci(y_true, y_score, metric_fn, n_bootstrap=1000, alpha=0.05, **kws):\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    vals = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "        if len(np.unique(y_true[idx])) < 2:\n",
        "            continue\n",
        "        try:\n",
        "            vals.append(metric_fn(y_true[idx], y_score[idx], **kws))\n",
        "        except Exception:\n",
        "            continue\n",
        "    vals = np.asarray(vals)\n",
        "    return (\n",
        "        np.nanmean(vals),\n",
        "        np.nanpercentile(vals, 100 * alpha / 2),\n",
        "        np.nanpercentile(vals, 100 * (1 - alpha / 2)),\n",
        "    )\n",
        "\n",
        "def calculate_ece(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    if method == 'quantile':\n",
        "\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "\n",
        "    else:\n",
        "\n",
        "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "\n",
        "    ece = 0.0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "\n",
        "        if i == n_bins - 1:\n",
        "\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            ece += (n_in_bin / total_samples) * np.abs(avg_pred - avg_true)\n",
        "\n",
        "    return ece\n",
        "\n",
        "def calculate_mce(y_true, y_prob, n_bins=10, method='uniform'):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        bin_edges[0] = 0.0\n",
        "        bin_edges[-1] = 1.0\n",
        "    else:\n",
        "\n",
        "        bin_edges = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
        "\n",
        "    max_error = 0.0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "\n",
        "        if i == n_bins - 1:\n",
        "            mask_bin = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask_bin = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask_bin)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask_bin])\n",
        "            avg_true = np.mean(y_true[mask_bin])\n",
        "            error = abs(avg_pred - avg_true)\n",
        "            if error > max_error:\n",
        "                max_error = error\n",
        "\n",
        "    return max_error if max_error > 0 else np.nan\n",
        "\n",
        "\n",
        "def calibration_slope_intercept(y_true, y_prob, method='logistic'):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob) | np.isnan(y_true))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_prob = np.clip(y_prob, 1e-6, 1 - 1e-6)\n",
        "    X_base = logit(y_prob) if method == 'logistic' else y_prob\n",
        "    X = sm.add_constant(X_base.reshape(-1, 1))\n",
        "\n",
        "    try:\n",
        "        mod = sm.GLM(y_true, X, family=sm.families.Binomial())\n",
        "        res = mod.fit(disp=0)\n",
        "        intercept, slope = float(res.params[0]), float(res.params[1])\n",
        "        return intercept, slope\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def calibration_slope_intercept_ci(y_true, y_prob, n_bootstrap=1000, alpha=0.05, seed=42):\n",
        "\n",
        "\n",
        "    intercept_point, slope_point = calibration_slope_intercept(y_true, y_prob)\n",
        "\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    slopes, intercepts = [], []\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        try:\n",
        "            indices = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "            if len(np.unique(y_true[indices])) < 2:\n",
        "                continue\n",
        "\n",
        "            int_b, slp_b = calibration_slope_intercept(y_true[indices], y_prob[indices])\n",
        "            if not (np.isnan(slp_b) or np.isnan(int_b)):\n",
        "                slopes.append(slp_b)\n",
        "                intercepts.append(int_b)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if slopes and intercepts:\n",
        "        slope_ci = (np.percentile(slopes, 100 * alpha/2), np.percentile(slopes, 100 * (1 - alpha/2)))\n",
        "        intercept_ci = (np.percentile(intercepts, 100 * alpha/2), np.percentile(intercepts, 100 * (1 - alpha/2)))\n",
        "    else:\n",
        "\n",
        "        slope_ci = (np.nan, np.nan)\n",
        "        intercept_ci = (np.nan, np.nan)\n",
        "\n",
        "    return intercept_point, slope_point, intercept_ci, slope_ci\n",
        "\n",
        "def z_test_standard(y_true, y_prob):\n",
        "\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = np.sum(y_true - y_prob)\n",
        "\n",
        "\n",
        "    denominator = np.sqrt(np.sum(y_prob * (1 - y_prob)))\n",
        "\n",
        "    if denominator < 1e-8:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    z_stat = numerator / denominator\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return z_stat, p_value\n",
        "\n",
        "def hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=10, min_expected_freq=5):\n",
        "\n",
        "\n",
        "    if not STATSMODELS_AVAILABLE:\n",
        "        warnings.warn(\"statsmodels not available - returning NaN for Hosmer-Lemeshow test\")\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 20 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
        "    try:\n",
        "        df['bin'] = pd.qcut(df['y_prob'], n_bins, labels=False, duplicates='drop')\n",
        "    except ValueError:\n",
        "        df['bin'] = np.floor(df['y_prob'] * n_bins).astype(int)\n",
        "        df.loc[df['bin'] == n_bins, 'bin'] = n_bins - 1\n",
        "\n",
        "\n",
        "    summary = df.groupby('bin').agg(\n",
        "        observed=('y_true', 'sum'),\n",
        "        expected=('y_prob', 'sum'),\n",
        "        n_total=('y_true', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    while True:\n",
        "        sparse_bins = summary[summary['expected'] < min_expected_freq]\n",
        "        if sparse_bins.empty or len(summary) <= 2:\n",
        "            break\n",
        "\n",
        "\n",
        "        merge_idx = sparse_bins.index[0]\n",
        "        if merge_idx == 0:\n",
        "            summary.loc[1, ['observed', 'expected', 'n_total']] += summary.loc[0, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(0).reset_index(drop=True)\n",
        "        else:\n",
        "            summary.loc[merge_idx - 1, ['observed', 'expected', 'n_total']] += summary.loc[merge_idx, ['observed', 'expected', 'n_total']]\n",
        "            summary = summary.drop(merge_idx).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    g = len(summary)\n",
        "    summary['variance'] = summary['expected'] * (1 - summary['expected'] / summary['n_total'])\n",
        "    hl_statistic = ((summary['observed'] - summary['expected'])**2 / (summary['variance'] + 1e-8)).sum()\n",
        "\n",
        "\n",
        "    df_hl = g - 2\n",
        "    if df_hl <= 0:\n",
        "        return hl_statistic, np.nan\n",
        "\n",
        "    p_value = 1 - chi2.cdf(hl_statistic, df_hl)\n",
        "\n",
        "    return hl_statistic, p_value\n",
        "\n",
        "def perform_mcnemar(y_true, y_pred1, y_pred2):\n",
        "\n",
        "    try:\n",
        "        table = mcnemar_table(y_target=y_true, y_model1=y_pred1, y_model2=y_pred2)\n",
        "        return mcnemar(table).pvalue, table\n",
        "    except Exception:\n",
        "        return np.nan, None\n",
        "\n",
        "\n",
        "def specificity_score(y_true, y_pred, zero_division=0):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape != (2, 2):\n",
        "        return 0.0\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    denom = tn + fp\n",
        "    if denom == 0:\n",
        "        return zero_division\n",
        "    return tn / denom\n",
        "\n",
        "\n",
        "def cohens_d(group1, group2):\n",
        "\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    if n1 < 2 or n2 < 2:\n",
        "        return np.nan\n",
        "    m1, m2 = np.mean(group1), np.mean(group2)\n",
        "    s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
        "    pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
        "    return np.nan if pooled_std == 0 else (m1 - m2) / pooled_std\n",
        "\n",
        "def cliffs_delta(group1, group2):\n",
        "\n",
        "    if len(group1) == 0 or len(group2) == 0:\n",
        "        return np.nan\n",
        "    dominance = sum(int(x > y) - int(x < y) for x in group1 for y in group2)\n",
        "    return dominance / (len(group1) * len(group2))\n",
        "\n",
        "def interpret_cohens_d(d):\n",
        "    d = abs(d)\n",
        "    if np.isnan(d):\n",
        "        return \"Unknown\"\n",
        "    return (\"Negligible\" if d < 0.2 else\n",
        "            \"Small\" if d < 0.5 else\n",
        "            \"Medium\" if d < 0.8 else \"Large\")\n",
        "\n",
        "def interpret_cliffs_delta(delta):\n",
        "    d = abs(delta)\n",
        "    if np.isnan(d):\n",
        "        return \"Unknown\"\n",
        "    return (\"Negligible\" if d < 0.147 else\n",
        "            \"Small\" if d < 0.33 else\n",
        "            \"Medium\" if d < 0.474 else \"Large\")\n",
        "\n",
        "def mcnemar_effect_size(table):\n",
        "\n",
        "    if table is None:\n",
        "        return np.nan\n",
        "    b, c = table[0, 1], table[1, 0]\n",
        "    if c == 0:\n",
        "        return np.inf if b > 0 else np.nan\n",
        "    return b / c\n",
        "\n",
        "def interpret_odds_ratio(odds_ratio):\n",
        "    if np.isnan(odds_ratio):\n",
        "        return \"Unknown\"\n",
        "    if np.isinf(odds_ratio):\n",
        "        return \"Complete dominance\"\n",
        "    return (\"Large\" if odds_ratio >= 3 or odds_ratio <= 1/3 else\n",
        "            \"Medium\" if odds_ratio >= 1.5 or odds_ratio <= 1/1.5 else \"Small\")\n",
        "\n",
        "def bootstrap_effect_size_ci(group1, group2, effect_fn, n_bootstrap=1000, alpha=0.05):\n",
        "    rng = np.random.default_rng(42)\n",
        "    effects = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx1 = rng.choice(len(group1), len(group1), replace=True)\n",
        "        idx2 = rng.choice(len(group2), len(group2), replace=True)\n",
        "        eff = effect_fn(np.array(group1)[idx1], np.array(group2)[idx2])\n",
        "        if not np.isnan(eff):\n",
        "            effects.append(eff)\n",
        "    if not effects:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    effects = np.array(effects)\n",
        "    return effects.mean(), np.percentile(effects, 100*alpha/2), np.percentile(effects, 100*(1-alpha/2))\n",
        "\n",
        "\n",
        "def apply_multiple_comparisons_correction(df, pval_cols, alpha=0.05, method='both'):\n",
        "\n",
        "    df_corrected = df.copy()\n",
        "    for col in pval_cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        pvals = df[col].dropna().values\n",
        "        valid_idx = df[col].notna()\n",
        "        if len(pvals) == 0:\n",
        "            continue\n",
        "        if method in ['bonferroni', 'both']:\n",
        "            bonf_pvals = np.minimum(pvals * len(pvals), 1.0)\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni'] = bonf_pvals\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni_sig'] = bonf_pvals < alpha\n",
        "            df_corrected.loc[valid_idx, f'{col}_bonferroni_alpha'] = alpha / len(pvals)\n",
        "        if method in ['fdr', 'both']:\n",
        "            _, fdr_pvals, _, _ = multipletests(pvals, alpha=alpha, method='fdr_bh')\n",
        "            df_corrected.loc[valid_idx, f'{col}_fdr'] = fdr_pvals\n",
        "            df_corrected.loc[valid_idx, f'{col}_fdr_sig'] = fdr_pvals < alpha\n",
        "    return df_corrected\n",
        "\n",
        "def print_correction_summary(df_corrected, original_alpha=0.05):\n",
        "\n",
        "    print(\"\\nðŸ“Š Multiple Comparisons Correction Summary:\")\n",
        "    print(\"=\" * 60)\n",
        "    pval_cols = [col for col in df_corrected.columns\n",
        "                 if 'p_value' in col.lower() or 'pval' in col.lower()\n",
        "                 and not any(x in col for x in ['bonferroni', 'fdr'])]\n",
        "    for col in pval_cols:\n",
        "        if col in df_corrected.columns:\n",
        "            original_sig = (df_corrected[col] < original_alpha).sum()\n",
        "            total_tests = df_corrected[col].notna().sum()\n",
        "            print(f\"\\n{col.upper()}:\")\n",
        "            print(f\"  Original significant results (p < {original_alpha}): {original_sig}/{total_tests}\")\n",
        "            if f'{col}_bonferroni_sig' in df_corrected.columns:\n",
        "                bonf_sig = df_corrected[f'{col}_bonferroni_sig'].sum()\n",
        "                bonf_alpha = df_corrected[f'{col}_bonferroni_alpha'].iloc[0] if not df_corrected.empty else 'N/A'\n",
        "                print(f\"  Bonferroni significant (Î± = {bonf_alpha:.4f}): {bonf_sig}/{total_tests}\")\n",
        "            if f'{col}_fdr_sig' in df_corrected.columns:\n",
        "                fdr_sig = df_corrected[f'{col}_fdr_sig'].sum()\n",
        "                print(f\"  FDR significant (Î± = {original_alpha}): {fdr_sig}/{total_tests}\")\n",
        "\n",
        "def plot_significance_comparison(corrected_df, pval_cols, out_dir=\"Evaluation2\"):\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(pval_cols), figsize=(6*len(pval_cols), 5))\n",
        "    if len(pval_cols) == 1:\n",
        "        axes = [axes]\n",
        "    for i, col in enumerate(pval_cols):\n",
        "        if col not in corrected_df.columns:\n",
        "            continue\n",
        "        original_sig = (corrected_df[col] < 0.05).sum()\n",
        "        bonf_sig = corrected_df.get(f'{col}_bonferroni_sig', pd.Series([])).sum()\n",
        "        fdr_sig = corrected_df.get(f'{col}_fdr_sig', pd.Series([])).sum()\n",
        "        methods = ['Original', 'Bonferroni', 'FDR']\n",
        "        counts = [original_sig, bonf_sig, fdr_sig]\n",
        "        colors = ['lightblue', 'orange', 'lightgreen']\n",
        "        bars = axes[i].bar(methods, counts, color=colors, edgecolor='black', linewidth=0.7)\n",
        "        axes[i].set_title(f'Significant Results: {col}', fontsize=14)\n",
        "        axes[i].set_ylabel('Number of Significant Tests', fontsize=12)\n",
        "        for bar, count in zip(bars, counts):\n",
        "            height = bar.get_height()\n",
        "            axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                         f'{int(count)}', ha='center', va='bottom', fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{out_dir}/significance_comparison.png\", dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def pairwise_auprc_comparison_with_effect_size(y_true, model_probs, out=None):\n",
        "\n",
        "    keys = list(model_probs.keys())\n",
        "    results = []\n",
        "    for m1, m2 in combinations(keys, 2):\n",
        "        p1, p2 = model_probs[m1], model_probs[m2]\n",
        "        try:\n",
        "            auprc1 = average_precision_score(y_true, p1)\n",
        "            auprc2 = average_precision_score(y_true, p2)\n",
        "            rng, n_boot, diffs, ap1s, ap2s = np.random.default_rng(42), 1000, [], [], []\n",
        "            for _ in range(n_boot):\n",
        "                idx = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "                try:\n",
        "                    ap1 = average_precision_score(y_true[idx], p1[idx])\n",
        "                    ap2 = average_precision_score(y_true[idx], p2[idx])\n",
        "                    diffs.append(ap1 - ap2)\n",
        "                    ap1s.append(ap1)\n",
        "                    ap2s.append(ap2)\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if not diffs: continue\n",
        "            pval = 2 * min((np.array(diffs) > 0).mean(), (np.array(diffs) < 0).mean())\n",
        "            coh_d = cohens_d(np.array(ap1s), np.array(ap2s))\n",
        "            cl_delta = cliffs_delta(np.array(ap1s), np.array(ap2s))\n",
        "            cd_mean, cd_low, cd_high = bootstrap_effect_size_ci(np.array(ap1s), np.array(ap2s), cohens_d)\n",
        "            results.append({\n",
        "                \"Model_A\": m1, \"Model_B\": m2, \"AUPRC_A\": auprc1, \"AUPRC_B\": auprc2,\n",
        "                \"AUPRC_diff_mean\": np.mean(diffs), \"AUPRC_diff_std\": np.std(diffs),\n",
        "                \"AUPRC_p_value\": pval, \"Cohens_d\": coh_d, \"Cohens_d_CI_low\": cd_low,\n",
        "                \"Cohens_d_CI_high\": cd_high, \"Cohens_d_interpretation\": interpret_cohens_d(coh_d),\n",
        "                \"Cliffs_delta\": cl_delta, \"Cliffs_delta_interpretation\": interpret_cliffs_delta(cl_delta),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error comparing {m1} vs {m2}: {e}\")\n",
        "            continue\n",
        "    df = pd.DataFrame(results)\n",
        "    if not df.empty and 'AUPRC_p_value' in df.columns:\n",
        "        df_corrected = apply_multiple_comparisons_correction(df, ['AUPRC_p_value'], alpha=0.05, method='both')\n",
        "        if out:\n",
        "            df.to_csv(out, index=False)\n",
        "            corrected_out = str(out).replace('.csv', '_corrected.csv')\n",
        "            df_corrected.to_csv(corrected_out, index=False)\n",
        "            logging.info(f\"AUPRC comparison results saved to {out}\")\n",
        "        return df_corrected\n",
        "    return df\n",
        "\n",
        "def pairwise_mcnemar_with_effect_size(models, x_val, y_true, thresholds, out=None):\n",
        "\n",
        "    keys = list(models.keys())\n",
        "    results, model_predictions = [], {}\n",
        "    for name in keys:\n",
        "        try:\n",
        "            y_prob, _, _ = get_model_predictions_comprehensive(models[name]['artifact'], x_val)\n",
        "            if y_prob is not None:\n",
        "                thr = thresholds.get(name, 0.5)\n",
        "                pred = (y_prob >= thr).astype(int)\n",
        "                model_predictions[name] = pred\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to get predictions for {name}: {e}\")\n",
        "            continue\n",
        "    for m1, m2 in combinations(keys, 2):\n",
        "        if m1 not in model_predictions or m2 not in model_predictions:\n",
        "            continue\n",
        "        pred1, pred2 = model_predictions[m1], model_predictions[m2]\n",
        "        try:\n",
        "            p_val, table = perform_mcnemar(y_true, pred1, pred2)\n",
        "            odds_ratio = mcnemar_effect_size(table)\n",
        "            interp = interpret_odds_ratio(odds_ratio)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"McNemar test failed for {m1} vs {m2}: {e}\")\n",
        "            p_val, odds_ratio, interp = np.nan, np.nan, \"Unknown\"\n",
        "        results.append({\n",
        "            \"Model_A\": m1, \"Model_B\": m2, \"McNemar_p_value\": p_val,\n",
        "            \"Odds_ratio\": odds_ratio, \"Effect_size_interpretation\": interp\n",
        "        })\n",
        "    df = pd.DataFrame(results)\n",
        "    if not df.empty and 'McNemar_p_value' in df.columns:\n",
        "        df_corrected = apply_multiple_comparisons_correction(df, ['McNemar_p_value'], alpha=0.05, method='both')\n",
        "        if out:\n",
        "            df.to_csv(out, index=False)\n",
        "            corrected_out = str(out).replace('.csv', '_corrected.csv')\n",
        "            df_corrected.to_csv(corrected_out, index=False)\n",
        "            logging.info(f\"McNemar comparison results saved to {out}\")\n",
        "        return df_corrected\n",
        "    return df\n",
        "\n",
        "def create_effect_size_summary(auprc_df, mcnemar_df, out_dir):\n",
        "\n",
        "    summary_lines = [\"# Effect Size Analysis Summary\\n\"]\n",
        "    if auprc_df is not None and not auprc_df.empty and \"Cohens_d_interpretation\" in auprc_df:\n",
        "        summary_lines.append(\"## AUPRC Cohen's d:\")\n",
        "        counts = auprc_df['Cohens_d_interpretation'].value_counts().to_dict()\n",
        "        summary_lines.extend([f\"- {k}: {v} comparisons\" for k, v in counts.items()])\n",
        "    if mcnemar_df is not None and not mcnemar_df.empty and \"Effect_size_interpretation\" in mcnemar_df:\n",
        "        summary_lines.append(\"\\n## McNemar Odds Ratio Interpretation:\")\n",
        "        counts = mcnemar_df['Effect_size_interpretation'].value_counts().to_dict()\n",
        "        summary_lines.extend([f\"- {k}: {v} comparisons\" for k, v in counts.items()])\n",
        "    Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
        "    with open(Path(out_dir, \"effect_size_summary.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(summary_lines))\n",
        "    logging.info(f\"Effect size summary saved to {out_dir}/effect_size_summary.txt\")\n",
        "\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 6,\n",
        "    'axes.titlesize': 6,\n",
        "    'axes.labelsize': 6,\n",
        "    'xtick.labelsize': 6,\n",
        "    'ytick.labelsize': 6,\n",
        "    'legend.fontsize': 6,\n",
        "    'figure.dpi': 450,\n",
        "})\n",
        "\n",
        "def plot_heatmap(df: pd.DataFrame, title=\"Model Performance Heatmap\"):\n",
        "\n",
        "    df = enforce_numeric(df)\n",
        "    if df.empty:\n",
        "        logging.warning(\"No data for heatmap.\")\n",
        "        return\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    sns.heatmap(df, annot=True, fmt=\".3f\", cmap=\"viridis\", linewidths=0.5, cbar_kws={'shrink':0.7})\n",
        "    plt.title(title, fontsize=7)\n",
        "    plt.xlabel('Metrics', fontsize=7)\n",
        "    plt.ylabel('Models', fontsize=7)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Evaluation2/performance_heatmap.png\", dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_radar_chart(df: pd.DataFrame, title=\"Radar chart\"):\n",
        "    matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
        "    plt.rcParams['pdf.fonttype'] = 42\n",
        "    df = enforce_numeric(df)\n",
        "    if df.empty:\n",
        "        logging.warning(\"No data for radar chart.\")\n",
        "        return\n",
        "    df_norm = (df - df.min()) / (df.max() - df.min() + 1e-9)\n",
        "    n_metrics = len(df.columns)\n",
        "    angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "    fig, ax = plt.subplots(figsize=(3.5, 3.5), subplot_kw=dict(polar=True), dpi=450)\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(df_norm)))\n",
        "    for idx, (row_name, row) in enumerate(df_norm.iterrows()):\n",
        "        vals = row.tolist() + [row.iloc[0]]\n",
        "        ax.plot(angles, vals, label=str(row_name), color=colors[idx], lw=1)\n",
        "        ax.fill(angles, vals, alpha=0.13, color=colors[idx])\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), df.columns, fontsize=5, fontfamily='DejaVu Sans')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=5, fontfamily='DejaVu Sans')\n",
        "    ax.grid(True, alpha=0.27, linewidth=0.45)\n",
        "    ax.spines['polar'].set_color('black')\n",
        "    ax.spines['polar'].set_linewidth(0.6)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fontsize=6, frameon=False, borderaxespad=0, ncol=3)\n",
        "    plt.title(title, fontsize=6, pad=13, fontfamily='DejaVu Sans')\n",
        "    plt.tight_layout(pad=0.3)\n",
        "    plt.savefig(\"Evaluation2/metrics_radar_chart.png\", dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curves(details: Dict[str, dict]):\n",
        "\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    colors = list(sns.color_palette(\"colorblind\"))\n",
        "\n",
        "    for idx, (m, res) in enumerate(details.items()):\n",
        "        if \"roc_curve\" not in res:\n",
        "            continue\n",
        "\n",
        "\n",
        "        roc_data = res[\"roc_curve\"]\n",
        "        if len(roc_data) == 3:\n",
        "            fpr, tpr, _ = roc_data\n",
        "        elif len(roc_data) == 2:\n",
        "            fpr, tpr = roc_data\n",
        "        else:\n",
        "            print(f\"âš ï¸ Unexpected ROC curve format for {m}, skipping\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        auc = res.get(\"Metrics\", {}).get(\"AUC_ROC\", np.nan)\n",
        "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], label=f\"{m} (AUC = {auc:.3f})\", lw=1)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5, lw=1)\n",
        "    plt.xlabel(\"False Positive Rate\", fontsize=6)\n",
        "    plt.ylabel(\"True Positive Rate\", fontsize=6)\n",
        "    plt.title(\"ROC Curves\", fontsize=7)\n",
        "    plt.legend(fontsize=5, loc='lower right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Evaluation2/roc_curves.png\", dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_pr_curves(details: Dict[str, dict]):\n",
        "\n",
        "    plt.figure(figsize=(3.5, 3.5))\n",
        "    colors = list(sns.color_palette(\"colorblind\"))\n",
        "\n",
        "    for idx, (m, res) in enumerate(details.items()):\n",
        "        if \"pr_curve\" not in res:\n",
        "            continue\n",
        "\n",
        "\n",
        "        pr_data = res[\"pr_curve\"]\n",
        "        if len(pr_data) == 3:\n",
        "            precision, recall, _ = pr_data\n",
        "        elif len(pr_data) == 2:\n",
        "            precision, recall = pr_data\n",
        "        else:\n",
        "            print(f\"âš ï¸ Unexpected PR curve format for {m}, skipping\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        auprc = res.get(\"Metrics\", {}).get(\"AUPRC\", np.nan)\n",
        "        plt.plot(recall, precision, color=colors[idx % len(colors)], label=f\"{m} (AUPRC = {auprc:.3f})\", lw=1)\n",
        "\n",
        "    plt.xlabel(\"Recall\", fontsize=6)\n",
        "    plt.ylabel(\"Precision\", fontsize=6)\n",
        "    plt.title(\"Precision-Recall Curves\", fontsize=7)\n",
        "    plt.legend(fontsize=5, loc='lower left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Evaluation2/pr_curves.png\", dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_calibration_curves(results, n_bins=10, save_path=\"Evaluation2/calibration_plots.png\", show_bin_counts=True):\n",
        "\n",
        "    colors = plt.get_cmap('tab10').colors\n",
        "    model_names = list(results.keys())\n",
        "    n_models = len(model_names)\n",
        "    if n_models == 0: return\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 6), sharey=True, squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, model in enumerate(model_names):\n",
        "        res = results[model]\n",
        "        y_true, y_prob, metrics = np.asarray(res['y_true']), np.asarray(res['y_prob']), res.get('metrics', {})\n",
        "        try:\n",
        "            df = pd.DataFrame({'prob': y_prob, 'target': y_true})\n",
        "            df['bin'], bin_edges = pd.qcut(df['prob'], n_bins, labels=False, retbins=True, duplicates='drop')\n",
        "        except Exception as e:\n",
        "            print(f\"Quantile binning failed for {model}: {e}, using uniform bins.\")\n",
        "            bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "            df['bin'] = np.digitize(df['prob'], bin_edges) - 1\n",
        "        means, observed, ci_lowers, ci_uppers, bin_counts = [], [], [], [], []\n",
        "        for b in range(len(bin_edges)-1):\n",
        "            in_bin = (df['bin'] == b)\n",
        "            if not np.any(in_bin):\n",
        "                means.append((bin_edges[b] + bin_edges[b+1]) / 2)\n",
        "                observed.append(np.nan); ci_lowers.append(np.nan); ci_uppers.append(np.nan); bin_counts.append(0)\n",
        "                continue\n",
        "            prob_mean, obs_mean = df.loc[in_bin, 'prob'].mean(), df.loc[in_bin, 'target'].mean()\n",
        "            n_bin, pos_bin = in_bin.sum(), int(df.loc[in_bin, 'target'].sum())\n",
        "            ci_lo, ci_hi = proportion_confint(pos_bin, n_bin, method='wilson')\n",
        "            means.append(prob_mean); observed.append(obs_mean); ci_lowers.append(ci_lo)\n",
        "            ci_uppers.append(ci_hi); bin_counts.append(n_bin)\n",
        "\n",
        "        ax = axes[i]\n",
        "        color = colors[i % len(colors)]\n",
        "        ax.plot(means, observed, marker='o', color=color, label=\"Calibration\", lw=3, markersize=8)\n",
        "        ax.fill_between(means, ci_lowers, ci_uppers, color=color, alpha=0.18, label=\"95% Wilson CI\")\n",
        "        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfect')\n",
        "\n",
        "        thresh = res.get('youden_threshold', 0.5)\n",
        "        ax.axvline(x=thresh, color='gray', ls='--', lw=2, label=f\"Thresh={thresh:.3f}\")\n",
        "\n",
        "        metric_text = (f\"ECE: {metrics.get('ECE', np.nan):.3f}\\nMCE: {metrics.get('MCE', np.nan):.3f}\\n\"\n",
        "                       f\"Slope: {metrics.get('Slope', np.nan):.2f}\\nIntercept: {metrics.get('Intercept', np.nan):.2f}\\n\"\n",
        "                       f\"Brier: {metrics.get('Brier', np.nan):.3f}\")\n",
        "        ax.text(0.62, 0.19, metric_text, color=color, fontsize=11,\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), transform=ax.transAxes)\n",
        "\n",
        "        if show_bin_counts:\n",
        "            for mx, oy, count in zip(means, observed, bin_counts):\n",
        "                if not np.isnan(oy):\n",
        "                    ax.annotate(f\"n={int(count)}\", (mx, oy), textcoords=\"offset points\",\n",
        "                                xytext=(0,-16), ha='center', fontsize=10, color='dimgray')\n",
        "        ax.set_xlabel('Mean Predicted Probability', fontsize=14)\n",
        "        if i == 0: ax.set_ylabel('Fraction of Positives', fontsize=14)\n",
        "        ax.set_title(f'Calibration: {model}', fontsize=16)\n",
        "        ax.legend(loc='upper left', fontsize=10); ax.set_xlim([0, 1]); ax.set_ylim([0, 1])\n",
        "        ax.grid(True, alpha=0.3); ax.set_aspect('equal', 'box'); ax.tick_params(labelsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_overlaid_calibration_curves(results, n_bins=10, save_path=\"Evaluation2/calibration_overlay.png\", show_thresholds=True):\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.family\": \"DejaVu Sans\", \"font.size\": 7, \"axes.titlesize\": 7,\n",
        "        \"axes.labelsize\": 7, \"xtick.labelsize\": 6, \"ytick.labelsize\": 6,\n",
        "        \"legend.fontsize\": 6, \"lines.linewidth\": 1, \"figure.dpi\": 450, \"axes.linewidth\": 0.7,\n",
        "    })\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
        "    model_names = list(results.keys())\n",
        "    plt.figure(figsize=(3.5, 3.5), dpi=450)\n",
        "    threshold_lines, threshold_labels = [], []\n",
        "\n",
        "    for i, model in enumerate(model_names):\n",
        "        res = results[model]\n",
        "        y_true, y_prob = np.asarray(res['y_true']), np.asarray(res['y_prob'])\n",
        "        threshold = res.get('youden_threshold', 0.5)\n",
        "        df = pd.DataFrame({'prob': y_prob, 'target': y_true})\n",
        "        try:\n",
        "            df['bin'], bin_edges = pd.qcut(df['prob'], n_bins, labels=False, retbins=True, duplicates='drop')\n",
        "        except Exception:\n",
        "            bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "            df['bin'] = np.digitize(df['prob'], bin_edges) - 1\n",
        "        means, observed = [], []\n",
        "        for b in range(len(bin_edges) - 1):\n",
        "            in_bin = (df['bin'] == b)\n",
        "            if np.any(in_bin):\n",
        "                means.append(df.loc[in_bin, 'prob'].mean())\n",
        "                observed.append(df.loc[in_bin, 'target'].mean())\n",
        "            else:\n",
        "                means.append((bin_edges[b] + bin_edges[b+1]) / 2)\n",
        "                observed.append(np.nan)\n",
        "        means, observed = np.array(means), np.array(observed)\n",
        "        valid_idx = ~np.isnan(observed)\n",
        "        if np.any(valid_idx):\n",
        "            plt.plot(means[valid_idx], observed[valid_idx], marker='o', color=colors[i % len(colors)],\n",
        "                     label=model, lw=1, markersize=2, alpha=0.92)\n",
        "            if show_thresholds:\n",
        "                thresh_line = plt.axvline(x=threshold, color=colors[i % len(colors)],\n",
        "                                          linestyle=':', alpha=0.7, lw=1)\n",
        "                threshold_lines.append(thresh_line)\n",
        "                threshold_labels.append(f\"{model} thr: {threshold:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", lw=1, label=\"Perfect calibration\")\n",
        "    plt.xlabel('Mean predicted probability'); plt.ylabel('Fraction of positives')\n",
        "    plt.title('Calibration curves (all models)'); plt.xlim([0, 1]); plt.ylim([0, 1])\n",
        "    if show_thresholds and threshold_lines:\n",
        "        main_legend = plt.legend(fontsize=6, loc='upper left', frameon=False, borderaxespad=0.1)\n",
        "        threshold_legend = plt.legend(threshold_lines, threshold_labels, fontsize=5, loc='lower right',\n",
        "                                      frameon=False, title=\"Thresholds\", title_fontsize=6)\n",
        "        plt.gca().add_artist(main_legend)\n",
        "        plt.gca().add_artist(threshold_legend)\n",
        "    else:\n",
        "        plt.legend(fontsize=6, loc='upper left', frameon=False, borderaxespad=0.1)\n",
        "    plt.grid(True, alpha=0.16, lw=0.4, linestyle='--'); plt.tick_params(labelsize=6, width=0.7, length=3)\n",
        "    plt.tight_layout(pad=0.35); plt.savefig(save_path, dpi=450, bbox_inches='tight'); plt.close()\n",
        "\n",
        "\n",
        "def plot_threshold_histograms(details: Dict[str, dict], thresholds: Dict[str, float]):\n",
        "\n",
        "    print(f\"ðŸ”Ž Starting plotting function. Number of models received: {len(details)}\")\n",
        "\n",
        "    n_models = len(details)\n",
        "    if n_models == 0:\n",
        "        print(\"âŒ Error: The 'details' dictionary is empty. No plots will be generated.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5), squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (model, res) in enumerate(details.items()):\n",
        "        if \"y_prob\" not in res:\n",
        "            print(f\"âš ï¸ Warning: '{model}' has no 'y_prob' key. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        y_prob = res[\"y_prob\"]\n",
        "        print(f\"  - Plotting for '{model}':\")\n",
        "\n",
        "\n",
        "        y_prob = pd.to_numeric(np.asarray(y_prob).flatten(), errors='coerce')\n",
        "        y_prob = y_prob[~np.isnan(y_prob)]\n",
        "\n",
        "\n",
        "        if len(y_prob) == 0:\n",
        "            print(f\"    âŒ Error: 'y_prob' for '{model}' is empty after cleaning. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"    - Plotting with {len(y_prob)} valid probability values.\")\n",
        "\n",
        "        if len(np.unique(y_prob)) < 2:\n",
        "            print(f\"    âŒ Error: Data for '{model}' has only one unique value after cleaning. Cannot plot KDE. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        thr = thresholds.get(model, 0.5)\n",
        "        ax = axes[idx]\n",
        "\n",
        "\n",
        "        sns.histplot(x=y_prob, bins=20, kde=False, ax=ax, stat=\"density\",\n",
        "                     label=\"Probability Distribution\", color=\"skyblue\", alpha=0.6)\n",
        "        sns.kdeplot(x=y_prob, ax=ax, fill=True,\n",
        "                    label=\"KDE with 95% CI\", color=\"navy\", lw=2)\n",
        "        ax.axvline(thr, color='r', linestyle='--', lw=2, label=f'Optimal Thr: {thr:.3f}')\n",
        "\n",
        "        ax.set_title(f\"Probability Distribution: {model}\", fontsize=14)\n",
        "        ax.set_xlabel(\"Predicted Probability\", fontsize=12)\n",
        "        ax.set_ylabel(\"Density\", fontsize=12)\n",
        "        ax.legend(fontsize=11)\n",
        "        ax.tick_params(labelsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = \"Evaluation2/threshold_histograms_with_ci.png\"\n",
        "    plt.savefig(save_path, dpi=450, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"âœ… Figure saved to: {save_path}\")\n",
        "\n",
        "\n",
        "def evaluate_models_and_compile_results(models, X_test, y_test, thresholds):\n",
        "\n",
        "    all_eval_results = {}\n",
        "\n",
        "    for name, model_info in models.items():\n",
        "        try:\n",
        "\n",
        "            y_prob, num_feat, sel_names = get_model_predictions_comprehensive(model_info['artifact'], X_test)\n",
        "            if y_prob is None:\n",
        "                continue\n",
        "\n",
        "            y_true = y_test\n",
        "            thr = thresholds.get(name, 0.5)\n",
        "            y_pred = (y_prob >= thr).astype(int)\n",
        "\n",
        "\n",
        "            ci = {}\n",
        "            metrics = {}\n",
        "\n",
        "\n",
        "            metrics = calculate_all_metrics(y_true, y_pred, y_prob)\n",
        "\n",
        "\n",
        "            ci = bootstrap_metrics_ci_for_table(y_true, y_pred, y_prob, n_bootstrap=1000)\n",
        "            metrics.update(ci)\n",
        "\n",
        "\n",
        "            all_eval_results[name] = {\n",
        "                \"Metrics\": metrics,\n",
        "                \"y_prob\": y_prob,\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred\": y_pred,\n",
        "                \"num_features\": num_feat,\n",
        "                \"youden_threshold\": thr,\n",
        "\n",
        "                \"roc_curve\": roc_curve(y_true, y_prob)[:2],\n",
        "                \"pr_curve\": precision_recall_curve(y_true, y_prob)[:2],\n",
        "                \"calibration_data\": calibration_curve(y_true, y_prob, n_bins=10)\n",
        "            }\n",
        "            print(f\"âœ… Successfully processed {name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process and compile metrics for {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    if not all_eval_results:\n",
        "        print(\"âŒ No models were successfully processed\")\n",
        "        return {}, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    full_metrics_list = []\n",
        "    for name, data in all_eval_results.items():\n",
        "        row = {'Model': name, 'N_Features': data['num_features']}\n",
        "        row.update(data['Metrics'])\n",
        "        full_metrics_list.append(row)\n",
        "\n",
        "\n",
        "    full_metrics_df = pd.DataFrame(full_metrics_list)\n",
        "\n",
        "\n",
        "    full_metrics_path = Path(\"Evaluation2/full_metrics_with_ci.csv\")\n",
        "    full_metrics_df.to_csv(full_metrics_path, index=False)\n",
        "    print(f\"ðŸ’¾ Full metrics saved â†’ {full_metrics_path}\")\n",
        "\n",
        "\n",
        "    summary_metrics = ['AUPRC', 'AUC_ROC', 'F1', 'Accuracy', 'Balanced_Accuracy', 'MCC', 'Kappa', 'ECE', 'MCE']\n",
        "    available_metrics = [m for m in summary_metrics if m in full_metrics_df.columns]\n",
        "\n",
        "    summary_df = full_metrics_df[['Model', 'N_Features'] + available_metrics].set_index('Model').round(4)\n",
        "\n",
        "\n",
        "    summary_path = Path(\"Evaluation2/summary_metrics_with_features.csv\")\n",
        "    summary_df.to_csv(summary_path)\n",
        "    print(f\"ðŸ’¾ Summary metrics saved â†’ {summary_path}\")\n",
        "\n",
        "    return all_eval_results, summary_df, full_metrics_df\n",
        "\n",
        "\n",
        "def evaluate_models(models, x_val, y_true, thresholds, out_dir=\"Evaluation2\"):\n",
        "\n",
        "    Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
        "    metrics_summary, full_metrics, model_probs, details = {}, [], {}, {}\n",
        "\n",
        "    for name, obj in models.items():\n",
        "        logging.info(\"Evaluating %s\", name)\n",
        "        y_prob, n_feat, sel = get_model_predictions_comprehensive(obj[\"artifact\"], x_val)\n",
        "        if y_prob is None:\n",
        "            logging.error(\"Skipping %s due to prediction failure\", name)\n",
        "            continue\n",
        "\n",
        "\n",
        "        youden_threshold = thresholds.get(name, 0.5)\n",
        "        y_pred = (y_prob >= youden_threshold).astype(int)\n",
        "        model_probs[name] = y_prob\n",
        "\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tn, fp, fn, tp = (cm.ravel() if cm.shape == (2,2) else (0,0,0,0))\n",
        "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "\n",
        "        auprc, auprc_lo, auprc_hi = bootstrap_metric_ci(y_true, y_prob, average_precision_score)\n",
        "        auc, auc_lo, auc_hi = bootstrap_metric_ci(y_true, y_prob, roc_auc_score)\n",
        "        f1, f1_lo, f1_hi = bootstrap_metric_ci(y_true, y_pred, f1_score)\n",
        "        spec, spec_lo, spec_hi = bootstrap_metric_ci(y_true, y_pred, specificity_score)\n",
        "        sens, sens_lo, sens_hi = bootstrap_metric_ci(y_true, y_pred, recall_score)\n",
        "        prec, prec_lo, prec_hi = bootstrap_metric_ci(y_true, y_pred, precision_score)\n",
        "        brier, b_lo, b_hi = bootstrap_metric_ci(y_true, y_prob, brier_score_loss)\n",
        "        kappa, k_lo, k_hi = bootstrap_metric_ci(y_true, y_pred, cohen_kappa_score)\n",
        "        acc, acc_lo, acc_hi = bootstrap_metric_ci(y_true, y_pred, accuracy_score)\n",
        "        mcc, mcc_lo, mcc_hi = bootstrap_metric_ci(y_true, y_pred, matthews_corrcoef)\n",
        "        bal_acc, ba_lo, ba_hi = bootstrap_metric_ci(y_true, y_pred, balanced_accuracy_score)\n",
        "        logloss, ll_lo, ll_hi = bootstrap_metric_ci(y_true, y_prob, log_loss)\n",
        "        ece, ece_lo, ece_hi = bootstrap_metric_ci(y_true, y_prob, calculate_ece, n_bins=10)\n",
        "        mce, mce_lo, mce_hi = bootstrap_metric_ci(y_true, y_prob, calculate_mce, n_bins=10)\n",
        "\n",
        "\n",
        "        slope_result, intercept_result = calibration_slope_intercept_ci(y_true, y_prob)\n",
        "\n",
        "\n",
        "        slope, slope_lo, slope_hi = slope_result.value, slope_result.cilow, slope_result.cihigh\n",
        "        intercept, intercept_lo, intercept_hi = intercept_result.value, intercept_result.cilow, intercept_result.cihigh\n",
        "\n",
        "        z_stat, z_p = z_test_standard(y_true, y_prob)\n",
        "        hl_stat, hl_p = hosmer_lemeshow_test_advanced(y_true, y_prob, n_bins=10)\n",
        "\n",
        "\n",
        "        roc_fpr, roc_tpr, _ = roc_curve(y_true, y_prob)\n",
        "        pr_curve_vals = precision_recall_curve(y_true, y_prob)[:2]\n",
        "        cal_mean, cal_frac = calibration_curve(y_true, y_prob, n_bins=10, strategy=\"uniform\")\n",
        "\n",
        "\n",
        "        model_metrics = {\n",
        "            \"Model\": name, \"n_features_used\": n_feat, \"Youden_Threshold\": youden_threshold,\n",
        "\n",
        "            \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp, \"NPV\": npv, \"PPV\": ppv, \"FPR\": fpr, \"FNR\": fnr,\n",
        "\n",
        "\n",
        "            \"AUPRC\": auprc, \"AUPRC_CI_low\": auprc_lo, \"AUPRC_CI_high\": auprc_hi,\n",
        "            \"AUC_ROC\": auc, \"AUC_ROC_CI_low\": auc_lo, \"AUC_ROC_CI_high\": auc_hi,\n",
        "            \"F1\": f1, \"F1_CI_low\": f1_lo, \"F1_CI_high\": f1_hi,\n",
        "            \"Specificity\": spec, \"Specificity_CI_low\": spec_lo, \"Specificity_CI_high\": spec_hi,\n",
        "            \"Sensitivity\": sens, \"Sensitivity_CI_low\": sens_lo, \"Sensitivity_CI_high\": sens_hi,\n",
        "            \"Precision\": prec, \"Precision_CI_low\": prec_lo, \"Precision_CI_high\": prec_hi,\n",
        "            \"Brier\": brier, \"Brier_CI_low\": b_lo, \"Brier_CI_high\": b_hi,\n",
        "            \"Kappa\": kappa, \"Kappa_CI_low\": k_lo, \"Kappa_CI_high\": k_hi,\n",
        "            \"Accuracy\": acc, \"Accuracy_CI_low\": acc_lo, \"Accuracy_CI_high\": acc_hi,\n",
        "            \"MCC\": mcc, \"MCC_CI_low\": mcc_lo, \"MCC_CI_high\": mcc_hi,\n",
        "            \"Balanced_Accuracy\": bal_acc, \"Balanced_Accuracy_CI_low\": ba_lo, \"Balanced_Accuracy_CI_high\": ba_hi,\n",
        "            \"Log_Loss\": logloss, \"Log_Loss_CI_low\": ll_lo, \"Log_Loss_CI_high\": ll_hi,\n",
        "            \"ECE\": ece, \"ECE_CI_low\": ece_lo, \"ECE_CI_high\": ece_hi,\n",
        "            \"MCE\": mce, \"MCE_CI_low\": mce_lo, \"MCE_CI_high\": mce_hi,\n",
        "\n",
        "\n",
        "            \"calibration_intercept\": intercept,\n",
        "            \"calibration_intercept_CI_low\": intercept_lo,\n",
        "            \"calibration_intercept_CI_high\": intercept_hi,\n",
        "\n",
        "            \"calibration_slope\": slope,\n",
        "            \"calibration_slope_CI_low\": slope_lo,\n",
        "            \"calibration_slope_CI_high\": slope_hi,\n",
        "\n",
        "            \"z_statistic\": sp_z, \"z_p_value\": sp_p,\n",
        "            \"hl_statistic\": hl_stat, \"hl_p_value\": hl_p,\n",
        "        }\n",
        "        full_metrics.append(model_metrics)\n",
        "\n",
        "\n",
        "        metrics_summary[name] = {\n",
        "            \"N_Features_Used\": n_feat, \"Youden_Threshold\": youden_threshold,\n",
        "            \"AUPRC\": auprc, \"AUC_ROC\": auc, \"F1\": f1, \"Balanced_Accuracy\": bal_acc,\n",
        "            \"Kappa\": kappa, \"Accuracy\": acc, \"MCC\": mcc, \"ECE\": ece, \"MCE\": mce,\n",
        "\n",
        "            \"calibration_intercept\": intercept,\n",
        "            \"calibration_slope\": slope,\n",
        "        }\n",
        "\n",
        "\n",
        "    full_metrics_df = pd.DataFrame(full_metrics)\n",
        "    full_metrics_path = Path(out_dir, \"full_metrics_with_ci.csv\")\n",
        "    full_metrics_df.to_csv(full_metrics_path, index=False)\n",
        "    logging.info(\"Full metrics saved â†’ %s\", full_metrics_path)\n",
        "\n",
        "    summary_df = pd.DataFrame(metrics_summary).T\n",
        "    summary_path = Path(out_dir, \"summary_metrics_with_features.csv\")\n",
        "    summary_df.to_csv(summary_path)\n",
        "    logging.info(\"Summary metrics saved â†’ %s\", summary_path)\n",
        "\n",
        "\n",
        "\n",
        "    if model_probs:\n",
        "        print(\"\\nðŸ”§ Performing statistical comparisons with multiple testing corrections...\")\n",
        "\n",
        "        effect_size_results = pairwise_auprc_comparison_with_effect_size(\n",
        "            y_true, model_probs, out=Path(out_dir, \"auprc_pairwise_with_effect_size.csv\")\n",
        "        )\n",
        "        mcnemar_effect_results = pairwise_mcnemar_with_effect_size(\n",
        "            models, x_val, y_true, thresholds, out=Path(out_dir, \"mcnemar_with_effect_size.csv\")\n",
        "        )\n",
        "\n",
        "        if not effect_size_results.empty:\n",
        "            print_correction_summary(effect_size_results)\n",
        "            plot_significance_comparison(effect_size_results, ['AUPRC_p_value'], out_dir)\n",
        "\n",
        "        if not mcnemar_effect_results.empty:\n",
        "            print_correction_summary(mcnemar_effect_results)\n",
        "            plot_significance_comparison(mcnemar_effect_results, ['McNemar_p_value'], out_dir)\n",
        "\n",
        "        create_effect_size_summary(effect_size_results, mcnemar_effect_results, out_dir)\n",
        "\n",
        "\n",
        "    return details, summary_df, full_metrics_df\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸš€ Starting Enhanced Model Evaluation with Feature Analysis\")\n",
        "\n",
        "\n",
        "    X_val, y_val = load_validation_data()\n",
        "    if X_val is None or y_val is None:\n",
        "        print(\"âŒ Validation data loading failed - aborting.\")\n",
        "        return\n",
        "\n",
        "    model_search_path = Path(\"trained_models\")\n",
        "    if Path(\"nested_cv_output\").exists():\n",
        "        model_search_path = Path(\"nested_cv_output\") / \"logloss\" / \"models\"\n",
        "\n",
        "    models_dict = load_calibrated_models(model_dir=str(model_search_path))\n",
        "    if not models_dict:\n",
        "        print(\"âŒ No models loaded - aborting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nLoaded models for evaluation:\")\n",
        "    for model_name in models_dict.keys():\n",
        "        print(f\" Â - {model_name}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        opt_thresh = load_optimal_thresholds()\n",
        "        print(\"\\nApplying thresholds to models:\")\n",
        "        final_thresholds = {}\n",
        "        for name in models_dict.keys():\n",
        "\n",
        "            if name in opt_thresh:\n",
        "                final_thresholds[name] = opt_thresh[name]\n",
        "                print(f\" Â âœ“ {name}: {opt_thresh[name]:.4f} (direct match)\")\n",
        "                continue\n",
        "\n",
        "            matched = False\n",
        "            for threshold_name, thresh_value in opt_thresh.items():\n",
        "                if name.lower() == threshold_name.lower():\n",
        "                    final_thresholds[name] = thresh_value\n",
        "                    print(f\" Â â‰ˆ {name} matched to '{threshold_name}': {thresh_value:.4f}\")\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                final_thresholds[name] = 0.5\n",
        "                print(f\"âš ï¸ Â No threshold found for '{name}', using default 0.5\")\n",
        "\n",
        "    except Exception as exc:\n",
        "        logging.warning(\"Threshold loading failed: %s - using 0.5 for all models\", exc)\n",
        "        final_thresholds = {name: 0.5 for name in models_dict.keys()}\n",
        "\n",
        "\n",
        "    print(\"\\nðŸ”¬ Calculating comprehensive metrics for each model...\")\n",
        "    details, summary_df, full_metrics_df = evaluate_models_and_compile_results(\n",
        "        models_dict, X_val, y_val, final_thresholds\n",
        "    )\n",
        "\n",
        "    if not details:\n",
        "        print(\"âŒ No successful evaluations - aborting.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    print(\"\\nðŸ“Š Performing pairwise statistical comparisons (McNemar and AUPRC)...\")\n",
        "\n",
        "\n",
        "    model_probs = {m: d.get('y_prob') for m, d in details.items() if 'y_prob' in d}\n",
        "\n",
        "\n",
        "    auprc_df_corrected = pairwise_auprc_comparison_with_effect_size(\n",
        "        y_val, model_probs, out=\"Evaluation2/auprc_pairwise_with_effect_size.csv\"\n",
        "    )\n",
        "    mcnemar_df_corrected = pairwise_mcnemar_with_effect_size(\n",
        "        models_dict, X_val, y_val, final_thresholds, out=\"Evaluation2/mcnemar_with_effect_size.csv\"\n",
        "    )\n",
        "\n",
        "\n",
        "    comparison_dfs = [df for df in [auprc_df_corrected, mcnemar_df_corrected] if df is not None and not df.empty]\n",
        "    if comparison_dfs:\n",
        "        pval_cols = ['AUPRC_p_value', 'McNemar_p_value']\n",
        "\n",
        "        print_correction_summary(comparison_dfs[0], original_alpha=0.05)\n",
        "        plot_significance_comparison(comparison_dfs[0].rename(columns={'AUPRC_p_value': 'AUPRC', 'McNemar_p_value': 'McNemar'}),\n",
        "                                     pval_cols=[c for c in pval_cols if c in comparison_dfs[0].columns])\n",
        "\n",
        "\n",
        "        create_effect_size_summary(auprc_df_corrected, mcnemar_df_corrected, out_dir=\"Evaluation2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nðŸ” Verifying thresholds stored in details:\")\n",
        "    for model_name, model_data in details.items():\n",
        "        stored_threshold = model_data.get('youden_threshold', 'NOT FOUND')\n",
        "        print(f\" Â {model_name}: {stored_threshold:.4f}\" if isinstance(stored_threshold, float) else f\" Â {model_name}: {stored_threshold}\")\n",
        "\n",
        "    print(\"ðŸŽ¨ Generating comprehensive visualizations...\")\n",
        "\n",
        "\n",
        "    core_metrics = [\"AUPRC\", \"AUC_ROC\", \"F1\", \"Balanced_Accuracy\",\n",
        "                    \"Kappa\", \"Accuracy\", \"MCC\", \"ECE\", \"MCE\"]\n",
        "    available_metrics = [m for m in summary_df.columns if m in core_metrics]\n",
        "\n",
        "    if available_metrics:\n",
        "        plot_heatmap(summary_df[available_metrics], \"Model Performance Heatmap\")\n",
        "        radar_metrics = [\"AUPRC\", \"AUC_ROC\", \"F1\", \"Accuracy\", \"Kappa\", \"MCC\"]\n",
        "        radar_cols = [m for m in radar_metrics if m in summary_df.columns]\n",
        "        if radar_cols and len(radar_cols) >= 3:\n",
        "            plot_radar_chart(summary_df[radar_cols], \"Radar Chart - Model Performance\")\n",
        "\n",
        "    plot_roc_curves(details)\n",
        "    plot_pr_curves(details)\n",
        "    plot_calibration_curves(details)\n",
        "    plot_overlaid_calibration_curves(details, show_thresholds=True)\n",
        "    plot_threshold_histograms(details, final_thresholds)\n",
        "\n",
        "    print(\"\\nâœ… Evaluation complete - Results saved in 'Evaluation2/' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bC2w0_NpkYYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Best Model Selection**  \n",
        "Select the best-performing model using composite metrics (e.g., AUC, Brier score, calibration)."
      ],
      "metadata": {
        "id": "s4FgDbhmkfYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "input_path = Path(\"Evaluation2/full_metrics_with_ci.csv\")\n",
        "if not input_path.exists():\n",
        "    print(f\"âŒ Error: {input_path} not found.\")\n",
        "else:\n",
        "    metrics_df = pd.read_csv(input_path)\n",
        "\n",
        "    if 'Unnamed: 0' in metrics_df.columns:\n",
        "        metrics_df = metrics_df.rename(columns={'Unnamed: 0': 'Model'})\n",
        "\n",
        "\n",
        "    column_aliases = {\n",
        "        'Spiegelhalter_p': 'Spiegel_p',\n",
        "        'Spiegelhalter_P_Value': 'Spiegel_p',\n",
        "        'HL_p_value': 'HL_p',\n",
        "        'HL_P_Value': 'HL_p',\n",
        "        'Recall': 'Sensitivity',\n",
        "        'AUC': 'AUC_ROC',\n",
        "        'AUROC': 'AUC_ROC',\n",
        "        'Brier_Score': 'Brier'\n",
        "    }\n",
        "\n",
        "    new_columns = []\n",
        "    for col in metrics_df.columns:\n",
        "        renamed = col\n",
        "\n",
        "\n",
        "        sorted_aliases = sorted(column_aliases.keys(), key=len, reverse=True)\n",
        "\n",
        "        for old_name in sorted_aliases:\n",
        "            new_name = column_aliases[old_name]\n",
        "\n",
        "\n",
        "            if col == new_name or col.startswith(new_name + \"_\") or col.startswith(new_name + \" \"):\n",
        "                renamed = col\n",
        "                break\n",
        "\n",
        "            if col == old_name:\n",
        "                renamed = new_name\n",
        "                break\n",
        "\n",
        "\n",
        "            if col.startswith(old_name + \"_\") or col.startswith(old_name + \" \"):\n",
        "                 renamed = col.replace(old_name, new_name, 1)\n",
        "                 break\n",
        "\n",
        "        new_columns.append(renamed)\n",
        "\n",
        "    metrics_df.columns = new_columns\n",
        "\n",
        "\n",
        "    print(\"Columns after renaming:\", metrics_df.columns.tolist())\n",
        "\n",
        "    weights = {\n",
        "        'AUPRC': 0.20,\n",
        "        'AUC_ROC': 0.20,\n",
        "        'F1': 0.15,\n",
        "        'Sensitivity': 0.10,\n",
        "        'Balanced_Accuracy': 0.10,\n",
        "        'MCC': 0.05,\n",
        "        'Brier': 0.05,\n",
        "        'ECE': 0.05,\n",
        "        'Spiegel_p': 0.05,\n",
        "        'HL_p': 0.05\n",
        "    }\n",
        "\n",
        "\n",
        "    valid_weights = {k: v for k, v in weights.items() if k in metrics_df.columns}\n",
        "    metric_names = list(valid_weights.keys())\n",
        "\n",
        "    print(f\"âœ… Valid metrics found: {metric_names}\")\n",
        "    missing = set(weights.keys()) - set(valid_weights.keys())\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ Warning: The following metrics were not found: {missing}\")\n",
        "\n",
        "\n",
        "    normalized = metrics_df[metric_names].copy()\n",
        "\n",
        "\n",
        "    for metric in ['ECE', 'Brier', 'LogLoss', 'MCE']:\n",
        "        if metric in normalized.columns:\n",
        "            normalized[metric] = 1 - MinMaxScaler().fit_transform(normalized[[metric]])\n",
        "\n",
        "\n",
        "    for metric in ['AUPRC', 'AUC_ROC', 'F1', 'Accuracy', 'Sensitivity',\n",
        "                   'Balanced_Accuracy', 'MCC', 'Spiegel_p', 'HL_p']:\n",
        "        if metric in normalized.columns:\n",
        "            normalized[metric] = MinMaxScaler().fit_transform(normalized[[metric]])\n",
        "\n",
        "\n",
        "    total_weight = sum(valid_weights.values())\n",
        "    final_weights = {k: v / total_weight for k, v in valid_weights.items()}\n",
        "\n",
        "    metrics_df['CompositeScore'] = 0\n",
        "    for metric, weight in final_weights.items():\n",
        "        metrics_df['CompositeScore'] += normalized[metric] * weight\n",
        "\n",
        "\n",
        "    output_columns = ['Model', 'CompositeScore']\n",
        "\n",
        "\n",
        "    ci_suffix_pairs = [\n",
        "        ('_CI_Low', '_CI_High'),\n",
        "        ('_CI_low', '_CI_high'),\n",
        "        ('_low', '_high'),\n",
        "        ('_CI_Lower', '_CI_Upper'),\n",
        "        ('_lower', '_upper')\n",
        "    ]\n",
        "\n",
        "    for metric in metric_names:\n",
        "        output_columns.append(metric)\n",
        "\n",
        "\n",
        "        found_ci = False\n",
        "        for low_suff, high_suff in ci_suffix_pairs:\n",
        "            low_col = f\"{metric}{low_suff}\"\n",
        "            high_col = f\"{metric}{high_suff}\"\n",
        "\n",
        "            if low_col in metrics_df.columns and high_col in metrics_df.columns:\n",
        "                output_columns.extend([low_col, high_col])\n",
        "                found_ci = True\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "    best_models = metrics_df[output_columns].sort_values('CompositeScore', ascending=False)\n",
        "\n",
        "\n",
        "    display_df = best_models.copy()\n",
        "\n",
        "    for metric in metric_names:\n",
        "\n",
        "        cols = [c for c in best_models.columns if c.startswith(metric)]\n",
        "\n",
        "        ci_cols = [c for c in cols if c != metric]\n",
        "        if len(ci_cols) == 2:\n",
        "            c1, c2 = ci_cols\n",
        "\n",
        "            if 'high' in c1.lower() or 'upper' in c1.lower():\n",
        "                high_col, low_col = c1, c2\n",
        "            else:\n",
        "                low_col, high_col = c1, c2\n",
        "\n",
        "            display_df[f\"{metric} (95% CI)\"] = display_df.apply(\n",
        "                lambda x: f\"{x[metric]:.3f} ({x[low_col]:.3f}-{x[high_col]:.3f})\",\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            display_df = display_df.drop(columns=[low_col, high_col, metric])\n",
        "\n",
        "\n",
        "    best_models.to_csv(\"Evaluation2/best_models_ranked_with_CI.csv\", index=False)\n",
        "    display_df.to_csv(\"Evaluation2/best_models_ranked_display.csv\", index=False)\n",
        "\n",
        "    print(\"\\nTop Models (Composite Score):\")\n",
        "    cols_to_show = ['Model', 'CompositeScore'] + [c for c in display_df.columns if c not in ['Model', 'CompositeScore']]\n",
        "    print(display_df[cols_to_show].head().to_string(index=False))"
      ],
      "metadata": {
        "id": "fdjYUVF2kjNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Visual Model Comparison (Composite Score, ROC, PR, Calibration)**              \n",
        "Generate visual comparisons across models including ROC, PR, calibration curves, and composite scores."
      ],
      "metadata": {
        "id": "yZj3cBl6ugq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 8,\n",
        "    'axes.titlesize': 9,\n",
        "    'axes.labelsize': 8,\n",
        "    'xtick.labelsize': 7,\n",
        "    'ytick.labelsize': 7,\n",
        "    'legend.fontsize': 7,\n",
        "    'figure.dpi': 600,\n",
        "    'axes.linewidth': 0.8,\n",
        "    'lines.linewidth': 1.2,\n",
        "})\n",
        "\n",
        "\n",
        "input_path = Path(\"Evaluation2/best_models_ranked_with_CI.csv\")\n",
        "if not input_path.exists():\n",
        "\n",
        "    input_path = Path(\"Evaluation2/full_metrics_with_ci.csv\")\n",
        "\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.rename(columns={'Unnamed: 0': 'Model'})\n",
        "\n",
        "if 'CompositeScore' not in df.columns:\n",
        "    if 'AUPRC' in df.columns:\n",
        "        df = df.sort_values('AUPRC', ascending=False)\n",
        "        df['CompositeScore'] = df['AUPRC']\n",
        "    else:\n",
        "        df['CompositeScore'] = 0\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.4], wspace=0.25, hspace=0.35)\n",
        "\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "sns.barplot(x='CompositeScore', y='Model', data=df, palette='viridis', edgecolor='black', linewidth=0.7, ax=ax1)\n",
        "ax1.set_title('A) Overall Model Ranking (Composite Score)', fontsize=10, weight='bold', loc='left')\n",
        "ax1.set_xlabel('Composite Score (0-1)', fontsize=8)\n",
        "ax1.set_ylabel('')\n",
        "ax1.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "\n",
        "\n",
        "for i, v in enumerate(df['CompositeScore']):\n",
        "    ax1.text(v + 0.01, i, f\"{v:.3f}\", va='center', fontsize=7)\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "\n",
        "\n",
        "metric_aliases = {\n",
        "    'AUPRC': ['AUPRC'],\n",
        "    'Sensitivity': ['Sensitivity', 'Recall'],\n",
        "    'F1': ['F1', 'F1_Score'],\n",
        "    'Brier': ['Brier', 'Brier_Score']\n",
        "}\n",
        "\n",
        "colors = sns.color_palette(\"deep\", len(metric_aliases))\n",
        "y_positions = np.arange(len(df))\n",
        "offset_step = 0.15\n",
        "start_offset = -((len(metric_aliases)-1) * offset_step) / 2\n",
        "\n",
        "for idx, (display_name, aliases) in enumerate(metric_aliases.items()):\n",
        "\n",
        "    col_name = next((a for a in aliases if a in df.columns), None)\n",
        "\n",
        "    if col_name:\n",
        "\n",
        "        low_col = next((c for c in df.columns if c.startswith(col_name) and ('low' in c or 'Low' in c)), None)\n",
        "        high_col = next((c for c in df.columns if c.startswith(col_name) and ('high' in c or 'High' in c)), None)\n",
        "\n",
        "        y_err = None\n",
        "        if low_col and high_col:\n",
        "            y_err = [df[col_name] - df[low_col], df[high_col] - df[col_name]]\n",
        "\n",
        "        ax2.errorbar(\n",
        "            x=df[col_name],\n",
        "            y=y_positions + start_offset + (idx * offset_step),\n",
        "            xerr=y_err,\n",
        "            fmt='o',\n",
        "            color=colors[idx],\n",
        "            capsize=3,\n",
        "            label=display_name,\n",
        "            alpha=0.9\n",
        "        )\n",
        "\n",
        "ax2.set_yticks(y_positions)\n",
        "ax2.set_yticklabels(df['Model'])\n",
        "ax2.set_title('B) Key Performance Metrics (95% CI)', fontsize=10, weight='bold', loc='left')\n",
        "ax2.set_xlabel('Metric Value', fontsize=8)\n",
        "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), frameon=False, ncol=4, fontsize=8)\n",
        "ax2.grid(axis='x', color='gray', alpha=0.2, linestyle='--')\n",
        "ax2.invert_yaxis()\n",
        "\n",
        "\n",
        "ax3 = fig.add_subplot(gs[1, :])\n",
        "\n",
        "\n",
        "heatmap_metrics_map = {\n",
        "    'AUPRC': 'AUPRC',\n",
        "    'AUC_ROC': 'AUC',\n",
        "    'F1': 'F1',\n",
        "    'ECE': 'ECE',\n",
        "    'Cal_Slope': 'Slope',\n",
        "    'Cal_Intercept': 'Intercept',\n",
        "    'Spiegelhalter_P_Value': 'Spiegel P',\n",
        "    'Brier': 'Brier'\n",
        "}\n",
        "\n",
        "\n",
        "available_metrics = {}\n",
        "for k, label in heatmap_metrics_map.items():\n",
        "\n",
        "    if k in df.columns:\n",
        "        available_metrics[k] = label\n",
        "\n",
        "    elif k == 'Cal_Slope' and 'Slope' in df.columns: available_metrics['Slope'] = label\n",
        "    elif k == 'Cal_Intercept' and 'Intercept' in df.columns: available_metrics['Intercept'] = label\n",
        "    elif k == 'Spiegelhalter_P_Value' and 'Spiegel_p' in df.columns: available_metrics['Spiegel_p'] = label\n",
        "    elif k == 'Brier' and 'Brier_Score' in df.columns: available_metrics['Brier_Score'] = label\n",
        "\n",
        "heatmap_data = df.set_index('Model')[list(available_metrics.keys())]\n",
        "heatmap_labels = heatmap_data.rename(columns=available_metrics)\n",
        "\n",
        "\n",
        "heatmap_norm = heatmap_data.copy()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "for col in heatmap_norm.columns:\n",
        "\n",
        "    if any(x in col for x in ['ECE', 'Brier', 'LogLoss', 'MCE']):\n",
        "        heatmap_norm[col] = 1 - scaler.fit_transform(heatmap_norm[[col]])\n",
        "\n",
        "    elif 'Slope' in col:\n",
        "\n",
        "        dist = abs(heatmap_norm[col] - 1)\n",
        "        heatmap_norm[col] = 1 - scaler.fit_transform(dist.values.reshape(-1,1))\n",
        "\n",
        "    elif 'Intercept' in col:\n",
        "        dist = abs(heatmap_norm[col])\n",
        "        heatmap_norm[col] = 1 - scaler.fit_transform(dist.values.reshape(-1,1))\n",
        "\n",
        "    else:\n",
        "        heatmap_norm[col] = scaler.fit_transform(heatmap_norm[[col]])\n",
        "\n",
        "sns.heatmap(\n",
        "    heatmap_norm,\n",
        "    annot=heatmap_labels.round(3),\n",
        "    fmt='',\n",
        "    cmap=\"RdYlBu\",\n",
        "    cbar_kws={'label': 'Relative Performance (Normalized)'},\n",
        "    linewidths=1,\n",
        "    linecolor='white',\n",
        "    ax=ax3,\n",
        "    annot_kws={\"size\": 8}\n",
        ")\n",
        "\n",
        "ax3.set_title('C) Detailed Performance Heatmap (Blue = Better)', fontsize=10, weight='bold', loc='left')\n",
        "ax3.set_ylabel('')\n",
        "ax3.set_xlabel('')\n",
        "ax3.set_xticklabels(list(available_metrics.values()), rotation=0)\n",
        "ax3.tick_params(axis='both', length=0)\n",
        "\n",
        "\n",
        "plt.tight_layout(pad=1.5)\n",
        "save_png = \"Evaluation2/model_comparison_visualization.png\"\n",
        "save_pdf = \"Evaluation2/model_comparison_visualization.pdf\"\n",
        "plt.savefig(save_png, dpi=600, bbox_inches='tight')\n",
        "plt.savefig(save_pdf, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"âœ… Visualization saved to: {save_png}\")"
      ],
      "metadata": {
        "id": "wkYc4kpAuhly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Model Persistence (Save Best Model**)           \n",
        "Save the final best-performing model for reproducibility and future use.\n"
      ],
      "metadata": {
        "id": "cv_3JDZ-uoEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "import traceback\n",
        "import joblib\n",
        "import json\n",
        "import csv\n",
        "import argparse\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from itertools import combinations\n",
        "from typing import Dict, List, Tuple, Optional, Callable\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, average_precision_score, balanced_accuracy_score,\n",
        "    brier_score_loss, cohen_kappa_score, confusion_matrix, f1_score,\n",
        "    matthews_corrcoef, precision_score, recall_score, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, log_loss\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import resample\n",
        "from scipy.special import logit\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from mlxtend.evaluate import mcnemar_table\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "NCV_BASE_OUT_DIR = \"nested_cv_output\"\n",
        "NCV_METRIC_DIR = \"logloss\"\n",
        "NCV_MODEL_DIR = \"models\"\n",
        "\n",
        "\n",
        "\n",
        "def validate_file_path(file_path: str | Path, description: str) -> Path:\n",
        "\n",
        "    path = Path(file_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"âŒ {description} not found at: {path.absolute()}\")\n",
        "    return path\n",
        "\n",
        "def extract_model_components(artifact: dict, model_name: str) -> Tuple[Dict, Dict]:\n",
        "\n",
        "    components = {}\n",
        "\n",
        "\n",
        "    components[\"calibrated_model\"] = artifact.get(\"calibrated_model\") or artifact.get(\"calibrated\")\n",
        "    if components[\"calibrated_model\"] is None:\n",
        "        logging.warning(f\"No calibrated model found for {model_name}\")\n",
        "\n",
        "\n",
        "    components[\"base_model\"] = artifact.get(\"base_model\") or artifact.get(\"model\")\n",
        "    if components[\"base_model\"] is None:\n",
        "        logging.warning(f\"No base model found for {model_name}\")\n",
        "\n",
        "    components[\"preprocessor\"] = artifact.get(\"preprocessor\")\n",
        "    components[\"feature_selector\"] = artifact.get(\"selector\") or artifact.get(\"feature_selector\")\n",
        "\n",
        "    metadata = artifact.get(\"metadata\", {})\n",
        "    components[\"selected_features\"] = metadata.get(\"selected_features\")\n",
        "\n",
        "    return components, metadata\n",
        "\n",
        "def robust_load_artifact_builder(model_artifacts_path: Path):\n",
        "\n",
        "    def robust_load_artifact(model_name: str) -> dict:\n",
        "        base_filename = f\"{model_name.replace(' ', '_')}_final_calibrated_model.pkl\"\n",
        "        artifact_path = model_artifacts_path / base_filename\n",
        "\n",
        "        if not artifact_path.exists():\n",
        "             raise FileNotFoundError(f\"Model artifact not found for {model_name} at {artifact_path}\")\n",
        "\n",
        "        return joblib.load(artifact_path)\n",
        "    return robust_load_artifact\n",
        "\n",
        "def normalize_model_name_for_matching(name):\n",
        "\n",
        "    name = str(name).strip().lower()\n",
        "\n",
        "    name = name.replace(' ', '').replace('-', '').replace('_', '')\n",
        "    if 'catboost' in name: return 'catboost'\n",
        "    if 'lightgbm' in name: return 'lightgbm'\n",
        "    if 'logistic' in name: return 'logisticregression'\n",
        "    if 'randomforest' in name: return 'randomforest'\n",
        "    if 'svm' in name: return 'svm'\n",
        "    if 'xgboost' in name: return 'xgboost'\n",
        "    return name\n",
        "\n",
        "def get_model_threshold(thresholds_df, model_name, threshold_col, default=0.5):\n",
        "\n",
        "    normalized_target = normalize_model_name_for_matching(model_name)\n",
        "\n",
        "\n",
        "    print(f\"ðŸ” DEBUG: Looking for model '{model_name}' (normalized: '{normalized_target}')\")\n",
        "    print(f\"ðŸ” DEBUG: Available columns in thresholds file: {thresholds_df.columns.tolist()}\")\n",
        "\n",
        "    model_col = None\n",
        "    for possible_col in ['Model', 'model', 'Algorithm', 'algorithm', 'name']:\n",
        "        if possible_col in thresholds_df.columns:\n",
        "            model_col = possible_col\n",
        "            break\n",
        "\n",
        "    if model_col is None:\n",
        "        first_col = thresholds_df.columns[0]\n",
        "        print(f\"âš ï¸ No standard model column found. Using first column: '{first_col}'\")\n",
        "        model_col = first_col\n",
        "\n",
        "    print(f\"ðŸ” DEBUG: Using model column: '{model_col}'\")\n",
        "    print(f\"ðŸ” DEBUG: Available models in thresholds: {thresholds_df[model_col].tolist()}\")\n",
        "\n",
        "\n",
        "    for idx, row in thresholds_df.iterrows():\n",
        "        current_model_name = str(row[model_col])\n",
        "        if normalize_model_name_for_matching(current_model_name) == normalized_target:\n",
        "            threshold_value = row[threshold_col]\n",
        "            print(f\"âœ… Found threshold for {model_name}: {threshold_value}\")\n",
        "            return float(threshold_value)\n",
        "\n",
        "    print(f\"âš ï¸ No threshold found for {model_name}, using default {default}\")\n",
        "    return default\n",
        "\n",
        "\n",
        "def save_best_and_lr_models_with_youdenj_and_group_thresholds(ncv_base_dir: str = NCV_BASE_OUT_DIR, ncv_metric_dir: str = NCV_METRIC_DIR):\n",
        "\n",
        "    base_output_path = Path(ncv_base_dir) / ncv_metric_dir\n",
        "    model_artifacts_path = base_output_path / NCV_MODEL_DIR\n",
        "    evaluation_path = Path(\"Evaluation2\")\n",
        "    output_dir = Path(\"deployment_artifacts\"); output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    robust_load_artifact = robust_load_artifact_builder(model_artifacts_path)\n",
        "\n",
        "    try:\n",
        "        print(\"ðŸš€ Starting model bundling process...\")\n",
        "\n",
        "        metrics_path = validate_file_path(evaluation_path / \"full_metrics_with_ci.csv\", \"Model metrics file\")\n",
        "        thresholds_path = validate_file_path(\"Threshold_results/reports/all_models_global_results.csv\", \"Threshold results file\")\n",
        "\n",
        "        metrics_df = pd.read_csv(metrics_path)\n",
        "\n",
        "        if 'Unnamed: 0' in metrics_df.columns:\n",
        "            metrics_df = metrics_df.rename(columns={'Unnamed: 0': 'Model'}).set_index(\"Model\")\n",
        "        elif 'Model' in metrics_df.columns:\n",
        "            metrics_df = metrics_df.set_index(\"Model\")\n",
        "        else:\n",
        "            first_col = metrics_df.columns[0]\n",
        "            metrics_df = metrics_df.set_index(first_col)\n",
        "\n",
        "        thresholds_df = pd.read_csv(thresholds_path)\n",
        "\n",
        "\n",
        "        if 'CompositeScore' in metrics_df.columns: primary_metric = 'CompositeScore'\n",
        "        elif 'AUPRC' in metrics_df.columns: primary_metric = 'AUPRC'\n",
        "        else: primary_metric = metrics_df.columns[0]\n",
        "\n",
        "        best_model_name = metrics_df[primary_metric].idxmax()\n",
        "        lr_model_name = 'LogisticRegression'\n",
        "        print(f\"ðŸ“Š Best model identified: {best_model_name} ({primary_metric}: {metrics_df.loc[best_model_name, primary_metric]:.4f})\")\n",
        "\n",
        "\n",
        "        threshold_col = None\n",
        "\n",
        "        possible_cols = ['threshold_youdenj', 'youden_threshold', 'threshold_optimal', 'Threshold', 'threshold']\n",
        "        for col in possible_cols:\n",
        "            if col in thresholds_df.columns:\n",
        "                threshold_col = col\n",
        "                break\n",
        "\n",
        "        if not threshold_col:\n",
        "            print(\"âš ï¸ No threshold column found, using default 0.5 for all models\")\n",
        "            best_model_threshold = 0.5\n",
        "            lr_threshold = 0.5\n",
        "        else:\n",
        "            print(f\"ðŸ” Using threshold column: '{threshold_col}'\")\n",
        "            best_model_threshold = get_model_threshold(thresholds_df, best_model_name, threshold_col)\n",
        "            lr_threshold = get_model_threshold(thresholds_df, lr_model_name, threshold_col)\n",
        "\n",
        "        saved_paths = []\n",
        "\n",
        "\n",
        "        print(f\"\\nðŸ”„ Processing best model: {best_model_name}\")\n",
        "        best_model_artifact = robust_load_artifact(best_model_name)\n",
        "        best_components, best_metadata = extract_model_components(best_model_artifact, best_model_name)\n",
        "\n",
        "        best_bundle = {\n",
        "            \"model\": best_components[\"calibrated_model\"],\n",
        "            \"calibrated_model\": best_components[\"calibrated_model\"],\n",
        "            \"base_model\": best_components[\"base_model\"],\n",
        "            \"preprocessor\": best_components[\"preprocessor\"],\n",
        "            \"feature_selector\": best_components[\"feature_selector\"],\n",
        "            \"selected_features\": best_components[\"selected_features\"],\n",
        "            \"metadata\": {\n",
        "                \"model_type\": \"best_model\",\n",
        "                \"model_name\": best_model_name,\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "                \"version\": \"1.0.0\",\n",
        "                \"primary_metric\": primary_metric,\n",
        "                \"primary_metric_value\": float(metrics_df.loc[best_model_name, primary_metric]),\n",
        "                \"selected_features\": best_components[\"selected_features\"],\n",
        "                \"n_features\": len(best_components[\"selected_features\"]) if best_components[\"selected_features\"] else None,\n",
        "                \"has_calibrated_model\": best_components[\"calibrated_model\"] is not None,\n",
        "                \"has_base_model\": best_components[\"base_model\"] is not None,\n",
        "                \"has_feature_selector\": best_components[\"feature_selector\"] is not None\n",
        "            },\n",
        "            \"youden_j_threshold\": float(best_model_threshold)\n",
        "        }\n",
        "\n",
        "\n",
        "        group_thresh_file = Path(\"Threshold_results/reports\") / f\"{best_model_name}_subgroups_optimal_thresholds_summary.csv\"\n",
        "        if group_thresh_file.exists():\n",
        "            print(f\"âœ… Found group-specific thresholds file: {group_thresh_file}\")\n",
        "            group_df = pd.read_csv(group_thresh_file)\n",
        "\n",
        "\n",
        "            sg_thresh_col = None\n",
        "            for col in ['threshold_youdenj', 'opt_youden_threshold', 'threshold_optimal', 'threshold', 'opt_f1_threshold']:\n",
        "                if col in group_df.columns:\n",
        "                    sg_thresh_col = col\n",
        "                    break\n",
        "\n",
        "            if sg_thresh_col:\n",
        "                print(f\"   Using subgroup threshold column: '{sg_thresh_col}'\")\n",
        "                group_thresholds = {\n",
        "                    row['group']: {\"threshold\": float(row[sg_thresh_col])}\n",
        "                    for _, row in group_df.iterrows()\n",
        "                }\n",
        "                best_bundle[\"group_specific_thresholds\"] = group_thresholds\n",
        "                print(f\"   Added {len(group_thresholds)} group-specific thresholds\")\n",
        "            else:\n",
        "                 print(f\"âš ï¸ Could not find a valid threshold column in {group_thresh_file.name}. Available: {group_df.columns.tolist()}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ No group-specific thresholds found for {best_model_name}\")\n",
        "\n",
        "        best_out_path = output_dir / f\"{best_model_name}_with_thresholds_v1.0.0.pkl\"\n",
        "        joblib.dump(best_bundle, best_out_path)\n",
        "        saved_paths.append(best_out_path)\n",
        "        print(f\"âœ… Saved best model bundle: {best_out_path}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nðŸ”„ Processing logistic regression model: {lr_model_name}\")\n",
        "        lr_artifact = robust_load_artifact(lr_model_name)\n",
        "        lr_components, lr_metadata = extract_model_components(lr_artifact, lr_model_name)\n",
        "\n",
        "        lr_bundle = {\n",
        "            \"model\": lr_components[\"calibrated_model\"],\n",
        "            \"calibrated_model\": lr_components[\"calibrated_model\"],\n",
        "            \"base_model\": lr_components[\"base_model\"],\n",
        "            \"preprocessor\": lr_components[\"preprocessor\"],\n",
        "            \"feature_selector\": lr_components[\"feature_selector\"],\n",
        "            \"selected_features\": lr_components[\"selected_features\"],\n",
        "            \"metadata\": {\n",
        "                \"model_type\": \"logistic_regression\",\n",
        "                \"model_name\": lr_model_name,\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "                \"version\": \"1.0.0\",\n",
        "                \"primary_metric\": primary_metric,\n",
        "                \"primary_metric_value\": float(metrics_df.loc[lr_model_name, primary_metric]),\n",
        "                \"selected_features\": lr_components[\"selected_features\"],\n",
        "                \"n_features\": len(lr_components[\"selected_features\"]) if lr_components[\"selected_features\"] else None,\n",
        "                \"has_calibrated_model\": lr_components[\"calibrated_model\"] is not None,\n",
        "                \"has_base_model\": lr_components[\"base_model\"] is not None,\n",
        "                \"has_feature_selector\": lr_components[\"feature_selector\"] is not None\n",
        "            },\n",
        "            \"youden_j_threshold\": float(lr_threshold)\n",
        "        }\n",
        "\n",
        "        lr_out_path = output_dir / f\"{lr_model_name}_with_thresholds_v1.0.0.pkl\"\n",
        "        joblib.dump(lr_bundle, lr_out_path)\n",
        "        saved_paths.append(lr_out_path)\n",
        "        print(f\"âœ… Saved LR model bundle: {lr_out_path}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nðŸ’¾ Saving metadata files...\")\n",
        "        for bundle, name, model_name in zip(\n",
        "            [best_bundle, lr_bundle],\n",
        "            [\"best_model\", \"logistic_regression\"],\n",
        "            [best_model_name, lr_model_name]\n",
        "        ):\n",
        "            meta_path = output_dir / f\"{name}_metadata.json\"\n",
        "            metadata_json = {\n",
        "                **bundle[\"metadata\"],\n",
        "                \"youden_j_threshold\": bundle[\"youden_j_threshold\"],\n",
        "                \"model_components\": {\n",
        "                    \"has_calibrated_model\": bundle[\"calibrated_model\"] is not None,\n",
        "                    \"has_base_model\": bundle[\"base_model\"] is not None,\n",
        "                    \"has_preprocessor\": bundle[\"preprocessor\"] is not None,\n",
        "                    \"has_feature_selector\": bundle[\"feature_selector\"] is not None\n",
        "                }\n",
        "            }\n",
        "            if \"group_specific_thresholds\" in bundle:\n",
        "                metadata_json[\"group_specific_thresholds\"] = bundle[\"group_specific_thresholds\"]\n",
        "\n",
        "            with open(meta_path, 'w') as f:\n",
        "                json.dump(metadata_json, f, indent=2)\n",
        "            print(f\"âœ… Saved metadata: {meta_path}\")\n",
        "\n",
        "        print(f\"\\nðŸŽ‰ MODEL BUNDLING COMPLETE!\")\n",
        "        print(f\"ðŸ“ Artifacts saved to: {output_dir.absolute()}\")\n",
        "        return saved_paths\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒâŒâŒ CRITICAL ERROR: Model bundling failed âŒâŒâŒ\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_best_and_lr_models_with_youdenj_and_group_thresholds(\n",
        "        ncv_base_dir=\"nested_cv_output\",\n",
        "        ncv_metric_dir=\"logloss\"\n",
        "    )"
      ],
      "metadata": {
        "id": "FwGLT-twuso3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Final Evaluation (Separated Test Set)**  \n",
        "Assess the generalizability of the selected model using the independent test set."
      ],
      "metadata": {
        "id": "Ic-B3-BWuxpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from pathlib import Path\n",
        "from scipy.stats import norm, chi2\n",
        "from scipy.special import logit\n",
        "from functools import wraps\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, f1_score,\n",
        "    precision_score, recall_score, confusion_matrix, roc_curve,\n",
        "    precision_recall_curve, brier_score_loss, balanced_accuracy_score,\n",
        "    matthews_corrcoef, cohen_kappa_score, log_loss, accuracy_score\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def validate_input_data(y_true, y_prob):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true = y_true[mask]\n",
        "    y_prob = y_prob[mask]\n",
        "\n",
        "    return y_true, y_prob\n",
        "\n",
        "def validation_required(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(y_true, y_prob, *args, **kwargs):\n",
        "        y_true, y_prob = validate_input_data(y_true, y_prob)\n",
        "        if len(y_true) < 2 or len(np.unique(y_true)) < 2:\n",
        "            return np.nan\n",
        "        return func(y_true, y_prob, *args, **kwargs)\n",
        "    return wrapper\n",
        "\n",
        "@validation_required\n",
        "def calculate_calibration_slope_intercept(y_true, y_prob):\n",
        "    try:\n",
        "        epsilon = 1e-7\n",
        "        p = np.clip(y_prob, epsilon, 1 - epsilon)\n",
        "        logit_p = logit(p)\n",
        "        X = sm.add_constant(logit_p)\n",
        "        model = sm.Logit(y_true, X).fit(disp=False)\n",
        "        return model.params[1], model.params[0]\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "\n",
        "def calculate_spiegelhalter_z(y_true, y_prob):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    numerator = np.sum(y_true - y_prob)\n",
        "\n",
        "    denominator = np.sqrt(np.sum(y_prob * (1 - y_prob)))\n",
        "\n",
        "    if denominator < 1e-8:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    z_stat = numerator / denominator\n",
        "\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return float(z_stat), float(p_value)\n",
        "\n",
        "def hosmer_lemeshow_test(y_true, y_prob, n_bins=10, min_expected_freq=5):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 20 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
        "\n",
        "\n",
        "    try:\n",
        "        df['bin'] = pd.qcut(df['y_prob'], n_bins, labels=False, duplicates='drop')\n",
        "    except ValueError:\n",
        "\n",
        "        df['bin'] = np.floor(df['y_prob'] * n_bins).astype(int)\n",
        "        df.loc[df['bin'] == n_bins, 'bin'] = n_bins - 1\n",
        "\n",
        "\n",
        "    summary = df.groupby('bin').agg(\n",
        "        observed=('y_true', 'sum'),\n",
        "        expected=('y_prob', 'sum'),\n",
        "        n_total=('y_true', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    while True:\n",
        "\n",
        "        sparse_bins = summary[summary['expected'] < min_expected_freq]\n",
        "\n",
        "        if sparse_bins.empty or len(summary) <= 2:\n",
        "            break\n",
        "\n",
        "        merge_idx = sparse_bins.index[0]\n",
        "\n",
        "        if merge_idx == 0:\n",
        "\n",
        "            target_idx = 1\n",
        "            source_idx = 0\n",
        "        else:\n",
        "\n",
        "            target_idx = merge_idx - 1\n",
        "            source_idx = merge_idx\n",
        "\n",
        "\n",
        "        summary.loc[target_idx, ['observed', 'expected', 'n_total']] += \\\n",
        "            summary.loc[source_idx, ['observed', 'expected', 'n_total']]\n",
        "\n",
        "        summary = summary.drop(source_idx).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    g = len(summary)\n",
        "    if g <= 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = (summary['observed'] - summary['expected'])**2\n",
        "    denominator = summary['expected'] * (1 - summary['expected'] / summary['n_total'])\n",
        "\n",
        "\n",
        "    denominator[denominator < 1e-8] = 1e-8\n",
        "\n",
        "    hl_statistic = (numerator / denominator).sum()\n",
        "\n",
        "    df_hl = g - 2\n",
        "    p_value = 1 - chi2.cdf(hl_statistic, df_hl)\n",
        "\n",
        "    return float(hl_statistic), float(p_value)\n",
        "\n",
        "def fixed_bin_calibration(y_true, y_prob, bin_edges):\n",
        "\n",
        "    inds = np.digitize(y_prob, bin_edges, right=False) - 1\n",
        "    inds = np.clip(inds, 0, len(bin_edges) - 2)\n",
        "\n",
        "    n_bins = len(bin_edges) - 1\n",
        "    prob_true = np.full(n_bins, np.nan)\n",
        "    prob_pred = np.full(n_bins, np.nan)\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        mask = inds == b\n",
        "        if np.any(mask):\n",
        "            prob_true[b] = np.mean(y_true[mask])\n",
        "            prob_pred[b] = np.mean(y_prob[mask])\n",
        "        else:\n",
        "            prob_pred[b] = (bin_edges[b] + bin_edges[b+1]) / 2.0\n",
        "\n",
        "    return prob_true, prob_pred\n",
        "\n",
        "@validation_required\n",
        "def calculate_ece(y_true, y_prob, n_bins=10):\n",
        "    bin_edges = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
        "    prob_true, prob_pred = fixed_bin_calibration(y_true, y_prob, bin_edges)\n",
        "\n",
        "\n",
        "    mask = ~np.isnan(prob_true)\n",
        "    if not np.any(mask): return np.nan\n",
        "\n",
        "\n",
        "    inds = np.digitize(y_prob, bin_edges, right=False) - 1\n",
        "    inds = np.clip(inds, 0, n_bins - 1)\n",
        "    counts = np.bincount(inds, minlength=n_bins)\n",
        "\n",
        "    weights = counts[mask] / len(y_true)\n",
        "    ece = np.sum(weights * np.abs(prob_pred[mask] - prob_true[mask]))\n",
        "    return float(ece)\n",
        "\n",
        "@validation_required\n",
        "def calculate_mce(y_true, y_prob, n_bins=10):\n",
        "    bin_edges = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
        "    prob_true, prob_pred = fixed_bin_calibration(y_true, y_prob, bin_edges)\n",
        "    mask = ~np.isnan(prob_true)\n",
        "    if not np.any(mask): return np.nan\n",
        "    return float(np.max(np.abs(prob_pred[mask] - prob_true[mask])))\n",
        "\n",
        "\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred, y_proba, calibration_bins=10):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2,2) else (0,0,0,0)\n",
        "\n",
        "    ece = calculate_ece(y_true, y_proba, n_bins=calibration_bins)\n",
        "    mce = calculate_mce(y_true, y_proba, n_bins=calibration_bins)\n",
        "    slope, intercept = calculate_calibration_slope_intercept(y_true, y_proba)\n",
        "    sz, sp = calculate_spiegelhalter_z(y_true, y_proba)\n",
        "    hl_stat, hl_p = hosmer_lemeshow_test(y_true, y_proba, n_bins=calibration_bins)\n",
        "\n",
        "    return {\n",
        "        'Test_Samples': len(y_true),\n",
        "        'Positive_Rate': np.mean(y_true),\n",
        "        'AUROC': roc_auc_score(y_true, y_proba),\n",
        "        'AUPRC': average_precision_score(y_true, y_proba),\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Balanced_Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
        "        'Sensitivity': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'Specificity': tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'F1': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'MCC': matthews_corrcoef(y_true, y_pred),\n",
        "        'Kappa': cohen_kappa_score(y_true, y_pred),\n",
        "        'Brier_Score': brier_score_loss(y_true, y_proba),\n",
        "        'Log_Loss': log_loss(y_true, y_proba),\n",
        "        'ECE': ece,\n",
        "        'MCE': mce,\n",
        "        'Calibration_Slope': slope,\n",
        "        'Calibration_Intercept': intercept,\n",
        "        'Spiegelhalter_Z': sz,\n",
        "        'Spiegelhalter_p': sp,\n",
        "        'HL_statistic': hl_stat,\n",
        "        'HL_p_value': hl_p,\n",
        "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
        "        'NPV': tn / (tn + fn) if (tn + fn) > 0 else 0.0,\n",
        "        'PPV': tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    }\n",
        "\n",
        "def bootstrap_metrics(y_true, y_pred, y_proba, n_bootstrap=1000):\n",
        "\n",
        "    boot_stats = []\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrapping metrics\", leave=False):\n",
        "        idx = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
        "\n",
        "        if len(np.unique(y_true[idx])) < 2: continue\n",
        "\n",
        "        try:\n",
        "            m = calculate_all_metrics(y_true[idx], y_pred[idx], y_proba[idx])\n",
        "            boot_stats.append(m)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    df_boot = pd.DataFrame(boot_stats)\n",
        "    ci_results = {}\n",
        "    for col in df_boot.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df_boot[col]):\n",
        "            ci_results[f\"{col}_CI_low\"] = df_boot[col].quantile(0.025)\n",
        "            ci_results[f\"{col}_CI_high\"] = df_boot[col].quantile(0.975)\n",
        "    return ci_results\n",
        "\n",
        "\n",
        "def compute_bootstrap_ci(y_true, y_proba, curve_kind, grid, n_bootstraps=1000, bin_edges=None):\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    boot_curves = []\n",
        "\n",
        "    def get_curve(yt, yp):\n",
        "        if curve_kind == 'roc':\n",
        "            x, y, _ = roc_curve(yt, yp)\n",
        "            return x, y\n",
        "        elif curve_kind == 'pr':\n",
        "            p, r, _ = precision_recall_curve(yt, yp)\n",
        "            return r, p\n",
        "        elif curve_kind == 'cal':\n",
        "            pt, pp = fixed_bin_calibration(yt, yp, bin_edges)\n",
        "            mask = ~np.isnan(pt)\n",
        "            return pp[mask], pt[mask]\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        idx = rng.integers(0, len(y_true), len(y_true))\n",
        "        if len(np.unique(y_true[idx])) < 2: continue\n",
        "\n",
        "        try:\n",
        "            x, y = get_curve(y_true[idx], y_proba[idx])\n",
        "\n",
        "            if len(x) > 1:\n",
        "\n",
        "                order = np.argsort(x)\n",
        "                x, y = x[order], y[order]\n",
        "                _, uidx = np.unique(x, return_index=True)\n",
        "                y_interp = np.interp(grid, x[uidx], y[uidx], left=y[0], right=y[-1])\n",
        "                boot_curves.append(y_interp)\n",
        "        except Exception: continue\n",
        "\n",
        "    boot_curves = np.array(boot_curves)\n",
        "    if len(boot_curves) == 0: return np.zeros_like(grid), np.zeros_like(grid), np.zeros_like(grid)\n",
        "\n",
        "    return np.mean(boot_curves, axis=0), np.percentile(boot_curves, 2.5, axis=0), np.percentile(boot_curves, 97.5, axis=0)\n",
        "\n",
        "def plot_comprehensive_evaluation(y_true, y_proba, model_name=\"CatBoost\", output_dir=\".\"):\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=600)\n",
        "\n",
        "\n",
        "    grid_roc = np.linspace(0, 1, 200)\n",
        "    mean_tpr, lo_tpr, hi_tpr = compute_bootstrap_ci(y_true, y_proba, 'roc', grid_roc)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "\n",
        "    ax1.plot(grid_roc, mean_tpr, color='tab:blue', lw=2, label=f'AUC = {auc:.3f}')\n",
        "    ax1.fill_between(grid_roc, lo_tpr, hi_tpr, color='tab:blue', alpha=0.2)\n",
        "    ax1.plot([0,1], [0,1], 'k--', lw=1)\n",
        "    ax1.set_title('ROC Curve')\n",
        "    ax1.set_xlabel('False Positive Rate')\n",
        "    ax1.set_ylabel('True Positive Rate')\n",
        "    ax1.legend(loc='lower right')\n",
        "\n",
        "\n",
        "    grid_pr = np.linspace(0, 1, 200)\n",
        "    mean_prec, lo_prec, hi_prec = compute_bootstrap_ci(y_true, y_proba, 'pr', grid_pr)\n",
        "    ap = average_precision_score(y_true, y_proba)\n",
        "    prev = np.mean(y_true)\n",
        "\n",
        "    ax2.plot(grid_pr, mean_prec, color='tab:green', lw=2, label=f'AP = {ap:.3f}')\n",
        "    ax2.fill_between(grid_pr, lo_prec, hi_prec, color='tab:green', alpha=0.2)\n",
        "    ax2.axhline(prev, color='k', ls='--', label=f'Baseline ({prev:.2f})')\n",
        "    ax2.set_title('Precision-Recall')\n",
        "    ax2.set_xlabel('Recall')\n",
        "    ax2.set_ylabel('Precision')\n",
        "    ax2.legend(loc='lower left')\n",
        "\n",
        "\n",
        "    n_bins = 10\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    grid_cal = np.linspace(0, 1, 50)\n",
        "    mean_cal, lo_cal, hi_cal = compute_bootstrap_ci(y_true, y_proba, 'cal', grid_cal, bin_edges=bin_edges)\n",
        "\n",
        "\n",
        "    pt, pp = fixed_bin_calibration(y_true, y_proba, bin_edges)\n",
        "    mask = ~np.isnan(pt)\n",
        "    ece = calculate_ece(y_true, y_proba)\n",
        "\n",
        "    ax3.plot(grid_cal, mean_cal, color='tab:orange', lw=1.5)\n",
        "    ax3.fill_between(grid_cal, lo_cal, hi_cal, color='tab:orange', alpha=0.2)\n",
        "    ax3.plot(pp[mask], pt[mask], 's', color='tab:orange', label=f'ECE = {ece:.3f}')\n",
        "    ax3.plot([0,1], [0,1], 'k--', lw=1)\n",
        "    ax3.set_title('Calibration')\n",
        "    ax3.set_xlabel('Predicted Probability')\n",
        "    ax3.set_ylabel('Observed Fraction')\n",
        "    ax3.legend(loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, f\"{model_name}_evaluation_plots.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def load_CatBoost_model(model_path):\n",
        "    if not os.path.exists(model_path): raise FileNotFoundError(f\"Missing: {model_path}\")\n",
        "    print(f\"ðŸ” Loading: {model_path}\")\n",
        "\n",
        "    artifact = joblib.load(model_path)\n",
        "\n",
        "\n",
        "    thresholds = {}\n",
        "    metadata = artifact.get('metadata', {})\n",
        "\n",
        "    for k in ['youden_j_threshold', 'threshold_youdenj', 'threshold']:\n",
        "        if k in artifact: thresholds['Youden_J'] = float(artifact[k])\n",
        "        if k in metadata: thresholds['Youden_J'] = float(metadata[k])\n",
        "\n",
        "\n",
        "    if not thresholds: thresholds['Default_0.5'] = 0.5\n",
        "\n",
        "    return artifact, thresholds, metadata\n",
        "\n",
        "def evaluate_CatBoost_test(\n",
        "        test_csv=\"test_set.csv\",\n",
        "        model_path=\"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\",\n",
        "        output_dir=\"test_evaluation_results\",\n",
        "        n_bootstrap=1000\n",
        "    ):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"ðŸš€ Starting Evaluation for {Path(model_path).stem}\")\n",
        "\n",
        "\n",
        "    df_test = pd.read_csv(test_csv)\n",
        "    target_col = 'MDR status' if 'MDR status' in df_test.columns else 'outcome'\n",
        "    X_test = df_test.drop(columns=[target_col])\n",
        "    y_test = df_test[target_col].values\n",
        "\n",
        "    artifact, thresholds, meta = load_CatBoost_model(model_path)\n",
        "    model = artifact['calibrated_model'] if 'calibrated_model' in artifact else artifact['model']\n",
        "    preprocessor = artifact['preprocessor']\n",
        "    sel_features = artifact.get('selected_features', meta.get('selected_features'))\n",
        "\n",
        "\n",
        "    print(\"ðŸ”® Generating predictions...\")\n",
        "    X_proc = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "    if sel_features:\n",
        "\n",
        "        def get_names(ct):\n",
        "            names = []\n",
        "            for nm, pipe, cols in ct.transformers_:\n",
        "                if nm == 'remainder': continue\n",
        "                if hasattr(pipe, 'get_feature_names_out'):\n",
        "                    names.extend(pipe.get_feature_names_out(cols))\n",
        "                elif hasattr(pipe, 'categories_'):\n",
        "                    for c, cats in zip(cols, pipe.categories_):\n",
        "                        names.extend([f\"{c}_{x}\" for x in cats])\n",
        "                else: names.extend(cols)\n",
        "            return names\n",
        "\n",
        "        all_feats = get_names(preprocessor)\n",
        "        if hasattr(X_proc, 'toarray'): X_proc = X_proc.toarray()\n",
        "\n",
        "\n",
        "        df_proc = pd.DataFrame(X_proc, columns=all_feats)\n",
        "\n",
        "        valid_feats = [f for f in sel_features if f in df_proc.columns]\n",
        "        X_final = df_proc[valid_feats]\n",
        "    else:\n",
        "        X_final = X_proc\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_final)[:, 1]\n",
        "    else:\n",
        "        y_proba = model.decision_function(X_final)\n",
        "        y_proba = 1 / (1 + np.exp(-y_proba))\n",
        "\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for thr_name, thr_val in thresholds.items():\n",
        "        print(f\"\\nðŸ“Š Evaluating @ {thr_name} = {thr_val:.4f}\")\n",
        "        y_pred = (y_proba >= thr_val).astype(int)\n",
        "\n",
        "\n",
        "        metrics = calculate_all_metrics(y_test, y_pred, y_proba)\n",
        "\n",
        "        ci = bootstrap_metrics(y_test, y_pred, y_proba, n_bootstrap)\n",
        "\n",
        "        combined = {'Threshold_Name': thr_name, 'Threshold_Value': thr_val, **metrics, **ci}\n",
        "        results_list.append(combined)\n",
        "\n",
        "        print(f\"   AUROC: {metrics['AUROC']:.3f} | F1: {metrics['F1']:.3f} | ECE: {metrics['ECE']:.3f}\")\n",
        "\n",
        "\n",
        "    res_df = pd.DataFrame(results_list)\n",
        "    res_df.to_csv(os.path.join(output_dir, \"CatBoost_Robust_Metrics.csv\"), index=False)\n",
        "\n",
        "    print(\"\\nðŸŽ¨ Generating Plots...\")\n",
        "    plot_comprehensive_evaluation(y_test, y_proba, \"CatBoost\", output_dir)\n",
        "\n",
        "    print(\"\\nâœ… Evaluation Complete.\")\n",
        "    return res_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if os.path.exists(\"test_set.csv\") and os.path.exists(\"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\"):\n",
        "        evaluate_CatBoost_test()\n",
        "    else:\n",
        "        print(\"âŒ Files not found. Please upload 'test_set.csv' and model artifact.\")"
      ],
      "metadata": {
        "id": "kzd7N6Lfu8JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Comparative Analysis (Best Model vs. Logistic Regression)**  \n",
        "Compare the chosen model against logistic regression as a baseline classifier."
      ],
      "metadata": {
        "id": "jy1GDfUbu-CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any\n",
        "from functools import wraps\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from scipy.stats import norm, chi2\n",
        "from scipy.special import logit\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, f1_score,\n",
        "    precision_score, recall_score, confusion_matrix, roc_curve,\n",
        "    precision_recall_curve, brier_score_loss, balanced_accuracy_score,\n",
        "    matthews_corrcoef, cohen_kappa_score, log_loss, accuracy_score\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from mlxtend.evaluate import mcnemar_table\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    'paths': {\n",
        "        'test_data': \"test_set.csv\",\n",
        "        'output_dir': \"model_comparison\",\n",
        "        'logs_dir': \"model_comparison/logs\",\n",
        "        'model_catboost': \"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\",\n",
        "        'model_lr': \"deployment_artifacts/LogisticRegression_with_thresholds_v1.0.0.pkl\"\n",
        "    },\n",
        "    'bootstrap': {\n",
        "        'n_bootstraps': 1000,\n",
        "        'n_jobs': -1,\n",
        "        'random_seed': 42\n",
        "    },\n",
        "    'calibration': {\n",
        "        'n_bins': 10,\n",
        "        'method': 'uniform'\n",
        "    },\n",
        "    'plotting': {\n",
        "        'dpi': 600,\n",
        "        'figsize': (10.5, 3.5),\n",
        "        'font_family': 'DejaVu Sans'\n",
        "    },\n",
        "    'logging': {\n",
        "        'level': 'INFO',\n",
        "        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"tab10\")\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def setup_logging() -> logging.Logger:\n",
        "\n",
        "\n",
        "    log_dir = Path(CONFIG['paths']['logs_dir'])\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = log_dir / f\"model_comparison_{timestamp}.log\"\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=getattr(logging, CONFIG['logging']['level']),\n",
        "        format=CONFIG['logging']['format'],\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, mode='w', encoding='utf-8'),\n",
        "            logging.StreamHandler(sys.stdout)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"Model Comparison Tool\")\n",
        "    logger.info(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    logger.info(f\"Python: {sys.version}\")\n",
        "    logger.info(f\"Log file: {log_file}\")\n",
        "    logger.info(\"=\" * 60)\n",
        "\n",
        "    return logger\n",
        "\n",
        "def validate_input_data(y_true: np.ndarray, y_prob: np.ndarray,\n",
        "                       context: str = \"\") -> Dict[str, Any]:\n",
        "\n",
        "    results = {\n",
        "        'is_valid': True,\n",
        "        'warnings': [],\n",
        "        'errors': [],\n",
        "        'y_true_clean': y_true.copy(),\n",
        "        'y_prob_clean': y_prob.copy(),\n",
        "        'n_original': len(y_true)\n",
        "    }\n",
        "\n",
        "\n",
        "    if not isinstance(y_true, np.ndarray):\n",
        "        results['errors'].append(\"y_true must be numpy array\")\n",
        "        results['is_valid'] = False\n",
        "\n",
        "    if not isinstance(y_prob, np.ndarray):\n",
        "        results['errors'].append(\"y_prob must be numpy array\")\n",
        "        results['is_valid'] = False\n",
        "\n",
        "    if not results['is_valid']:\n",
        "        return results\n",
        "\n",
        "\n",
        "    if y_true.shape != y_prob.shape:\n",
        "        results['errors'].append(\n",
        "            f\"Shape mismatch: y_true {y_true.shape}, y_prob {y_prob.shape}\"\n",
        "        )\n",
        "        results['is_valid'] = False\n",
        "        return results\n",
        "\n",
        "    nan_mask = np.isnan(y_prob) | np.isinf(y_prob)\n",
        "    nan_count = np.sum(nan_mask)\n",
        "\n",
        "    if nan_count > 0:\n",
        "        results['warnings'].append(f\"Removed {nan_count} NaN/Inf values\")\n",
        "        finite_mask = ~nan_mask\n",
        "        results['y_true_clean'] = y_true[finite_mask]\n",
        "        results['y_prob_clean'] = y_prob[finite_mask]\n",
        "\n",
        "    if len(results['y_prob_clean']) > 0:\n",
        "        out_of_bounds = np.sum(\n",
        "            (results['y_prob_clean'] < 0) | (results['y_prob_clean'] > 1)\n",
        "        )\n",
        "        if out_of_bounds > 0:\n",
        "            results['warnings'].append(\n",
        "                f\"Clipped {out_of_bounds} values outside [0, 1]\"\n",
        "            )\n",
        "            results['y_prob_clean'] = np.clip(results['y_prob_clean'], 0, 1)\n",
        "\n",
        "\n",
        "    if len(results['y_true_clean']) > 0:\n",
        "        unique_classes = np.unique(results['y_true_clean'])\n",
        "        if len(unique_classes) < 2:\n",
        "            results['errors'].append(\"Only one class present\")\n",
        "            results['is_valid'] = False\n",
        "\n",
        "\n",
        "        if len(results['y_true_clean']) < 20:\n",
        "            results['warnings'].append(\n",
        "                f\"Small sample size: {len(results['y_true_clean'])}\"\n",
        "            )\n",
        "\n",
        "\n",
        "    if results['warnings'] and context:\n",
        "        for warning in results['warnings']:\n",
        "            logger.warning(f\"{context}: {warning}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def validation_required(func):\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(y_true, y_prob, *args, **kwargs):\n",
        "        context = f\"{func.__name__}\"\n",
        "        validation = validate_input_data(y_true, y_prob, context)\n",
        "\n",
        "        if not validation['is_valid']:\n",
        "            error_msg = f\"Input validation failed: {validation['errors']}\"\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if len(validation['y_true_clean']) < 10:\n",
        "            logger.warning(f\"Insufficient data after cleaning: \"\n",
        "                          f\"{len(validation['y_true_clean'])} samples\")\n",
        "            return np.nan\n",
        "\n",
        "        return func(validation['y_true_clean'], validation['y_prob_clean'],\n",
        "                   *args, **kwargs)\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@validation_required\n",
        "def calculate_ece(y_true: np.ndarray, y_prob: np.ndarray,\n",
        "                 n_bins: int = 10, method: str = 'uniform') -> float:\n",
        "\n",
        "    if method == 'quantile':\n",
        "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_edges = np.percentile(y_prob, quantiles * 100)\n",
        "        bin_edges[0] = 0.0\n",
        "        bin_edges[-1] = 1.0\n",
        "    else:\n",
        "        bin_edges = np.linspace(0.0, 1.0 + 1e-8, n_bins + 1)\n",
        "\n",
        "    ece = 0.0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        if i == n_bins - 1:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            ece += (n_in_bin / total_samples) * np.abs(avg_pred - avg_true)\n",
        "\n",
        "    return ece\n",
        "\n",
        "@validation_required\n",
        "def calculate_mce(y_true: np.ndarray, y_prob: np.ndarray,\n",
        "                 n_bins: int = 10) -> float:\n",
        "\n",
        "    bin_edges = np.linspace(0.0, 1.0 + 1e-8, n_bins + 1)\n",
        "    max_error = 0.0\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        if i == n_bins - 1:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob <= bin_edges[i+1])\n",
        "        else:\n",
        "            mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
        "\n",
        "        n_in_bin = np.sum(mask)\n",
        "        if n_in_bin > 0:\n",
        "            avg_pred = np.mean(y_prob[mask])\n",
        "            avg_true = np.mean(y_true[mask])\n",
        "            error = abs(avg_pred - avg_true)\n",
        "            max_error = max(max_error, error)\n",
        "\n",
        "    return max_error if max_error > 0 else np.nan\n",
        "\n",
        "@validation_required\n",
        "def calculate_calibration_slope_intercept(y_true: np.ndarray,\n",
        "                                         y_prob: np.ndarray) -> Tuple[float, float]:\n",
        "\n",
        "    try:\n",
        "        epsilon = 1e-7\n",
        "        p = np.clip(y_prob, epsilon, 1 - epsilon)\n",
        "        logit_p = logit(p)\n",
        "        X = sm.add_constant(logit_p)\n",
        "\n",
        "        if len(np.unique(y_true)) < 2:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        model = sm.Logit(y_true, X).fit(disp=False, maxiter=100)\n",
        "        return float(model.params[1]), float(model.params[0])\n",
        "    except Exception as e:\n",
        "        logger.debug(f\"Calibration slope calculation failed: {e}\")\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "\n",
        "def calculate_spiegelhalter_z(y_true, y_prob):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = np.sum(y_true - y_prob)\n",
        "\n",
        "    denominator = np.sqrt(np.sum(y_prob * (1 - y_prob)))\n",
        "\n",
        "    if denominator < 1e-8:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    z_stat = numerator / denominator\n",
        "\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "    return float(z_stat), float(p_value)\n",
        "\n",
        "def hosmer_lemeshow_test(y_true, y_prob, n_bins=10, min_expected_freq=5):\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    mask = ~(np.isnan(y_prob) | np.isinf(y_prob))\n",
        "    y_true, y_prob = y_true[mask], y_prob[mask]\n",
        "\n",
        "    if len(y_true) < 20 or len(np.unique(y_true)) < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
        "\n",
        "\n",
        "    try:\n",
        "        df['bin'] = pd.qcut(df['y_prob'], n_bins, labels=False, duplicates='drop')\n",
        "    except ValueError:\n",
        "\n",
        "        df['bin'] = np.floor(df['y_prob'] * n_bins).astype(int)\n",
        "        df.loc[df['bin'] == n_bins, 'bin'] = n_bins - 1\n",
        "\n",
        "    summary = df.groupby('bin').agg(\n",
        "        observed=('y_true', 'sum'),\n",
        "        expected=('y_prob', 'sum'),\n",
        "        n_total=('y_true', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    while True:\n",
        "\n",
        "        sparse_bins = summary[summary['expected'] < min_expected_freq]\n",
        "\n",
        "        if sparse_bins.empty or len(summary) <= 2:\n",
        "            break\n",
        "\n",
        "        merge_idx = sparse_bins.index[0]\n",
        "\n",
        "        if merge_idx == 0:\n",
        "\n",
        "            target_idx = 1\n",
        "            source_idx = 0\n",
        "        else:\n",
        "\n",
        "            target_idx = merge_idx - 1\n",
        "            source_idx = merge_idx\n",
        "\n",
        "        summary.loc[target_idx, ['observed', 'expected', 'n_total']] += \\\n",
        "            summary.loc[source_idx, ['observed', 'expected', 'n_total']]\n",
        "\n",
        "        summary = summary.drop(source_idx).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    g = len(summary)\n",
        "    if g <= 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "\n",
        "    numerator = (summary['observed'] - summary['expected'])**2\n",
        "    denominator = summary['expected'] * (1 - summary['expected'] / summary['n_total'])\n",
        "\n",
        "\n",
        "    denominator[denominator < 1e-8] = 1e-8\n",
        "\n",
        "    hl_statistic = (numerator / denominator).sum()\n",
        "\n",
        "\n",
        "    df_hl = g - 2\n",
        "    p_value = 1 - chi2.cdf(hl_statistic, df_hl)\n",
        "\n",
        "    return float(hl_statistic), float(p_value)\n",
        "\n",
        "def fixed_bin_calibration(y_true: np.ndarray,\n",
        "                         y_prob: np.ndarray,\n",
        "                         bin_edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    inds = np.digitize(y_prob, bin_edges, right=False) - 1\n",
        "    inds = np.clip(inds, 0, len(bin_edges) - 2)\n",
        "\n",
        "    n_bins = len(bin_edges) - 1\n",
        "    prob_true = np.full(n_bins, np.nan)\n",
        "    prob_pred = np.full(n_bins, np.nan)\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        mask = inds == b\n",
        "        if np.any(mask):\n",
        "            prob_true[b] = np.mean(y_true[mask])\n",
        "            prob_pred[b] = np.mean(y_prob[mask])\n",
        "        else:\n",
        "\n",
        "            prob_pred[b] = (bin_edges[b] + bin_edges[b + 1]) / 2.0\n",
        "\n",
        "    return prob_true, prob_pred\n",
        "\n",
        "\n",
        "\n",
        "def compute_bootstrap_ci_parallel(\n",
        "    y_true: np.ndarray,\n",
        "    y_proba: np.ndarray,\n",
        "    curve_kind: str,\n",
        "    grid: np.ndarray,\n",
        "    n_bootstraps: int = 1000,\n",
        "    bin_edges: Optional[np.ndarray] = None,\n",
        "    n_jobs: int = -1,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "\n",
        "    logger.info(f\"Computing bootstrap CI for {curve_kind} \"\n",
        "               f\"({n_bootstraps} iterations, {n_jobs} jobs)\")\n",
        "\n",
        "    rng = np.random.default_rng(random_seed)\n",
        "    n = len(y_true)\n",
        "\n",
        "    def run_one_bootstrap(seed: int) -> np.ndarray:\n",
        "\n",
        "        local_rng = np.random.default_rng(seed)\n",
        "        idx = local_rng.integers(0, n, n)\n",
        "        yt = y_true[idx]\n",
        "        yp = y_proba[idx]\n",
        "\n",
        "        try:\n",
        "            if curve_kind == 'roc':\n",
        "                x, yv, _ = roc_curve(yt, yp)\n",
        "            elif curve_kind == 'pr':\n",
        "                precision, recall, _ = precision_recall_curve(yt, yp)\n",
        "                x, yv = recall, precision\n",
        "            elif curve_kind == 'cal':\n",
        "                if bin_edges is None:\n",
        "                    return np.full_like(grid, np.nan)\n",
        "                prob_true, prob_pred = fixed_bin_calibration(yt, yp, bin_edges)\n",
        "                mask = ~np.isnan(prob_true)\n",
        "                x, yv = prob_pred[mask], prob_true[mask]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown curve kind: {curve_kind}\")\n",
        "\n",
        "\n",
        "            if len(x) < 2:\n",
        "                return np.full_like(grid, np.nan)\n",
        "\n",
        "\n",
        "            order = np.argsort(x)\n",
        "            x, yv = x[order], yv[order]\n",
        "\n",
        "            xu, idx_unique = np.unique(x, return_index=True)\n",
        "            yu = yv[idx_unique]\n",
        "\n",
        "            if len(xu) > 1:\n",
        "                return np.interp(grid, xu, yu,\n",
        "                               left=yu[0], right=yu[-1])\n",
        "            else:\n",
        "                return np.full_like(grid, yu[0])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Bootstrap iteration failed: {e}\")\n",
        "            return np.full_like(grid, np.nan)\n",
        "\n",
        "\n",
        "    seeds = rng.integers(0, 1_000_000, n_bootstraps)\n",
        "\n",
        "\n",
        "    boot_curves = Parallel(n_jobs=n_jobs, verbose=0)(\n",
        "        delayed(run_one_bootstrap)(seed)\n",
        "        for seed in tqdm(seeds, desc=f\"Bootstrapping {curve_kind}\",\n",
        "                        disable=len(seeds) < 100)\n",
        "    )\n",
        "\n",
        "    boot_curves = np.array(boot_curves)\n",
        "\n",
        "    valid_mask = ~np.any(np.isnan(boot_curves), axis=1)\n",
        "    n_valid = np.sum(valid_mask)\n",
        "\n",
        "    if n_valid < n_bootstraps:\n",
        "        logger.warning(f\"{n_bootstraps - n_valid} bootstrap iterations failed\")\n",
        "\n",
        "    if n_valid == 0:\n",
        "        raise RuntimeError(\"All bootstrap iterations failed\")\n",
        "\n",
        "    boot_curves = boot_curves[valid_mask]\n",
        "\n",
        "\n",
        "    mean_curve = np.nanmean(boot_curves, axis=0)\n",
        "    lower_ci = np.nanpercentile(boot_curves, 2.5, axis=0)\n",
        "    upper_ci = np.nanpercentile(boot_curves, 97.5, axis=0)\n",
        "\n",
        "    return mean_curve, lower_ci, upper_ci\n",
        "\n",
        "def bootstrap_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    y_proba: np.ndarray,\n",
        "    n_bootstrap: int = 1000\n",
        ") -> Dict[str, float]:\n",
        "\n",
        "    logger.info(f\"Bootstrapping metrics ({n_bootstrap} iterations)\")\n",
        "\n",
        "    boot_metrics = []\n",
        "    rng = np.random.default_rng(CONFIG['bootstrap']['random_seed'])\n",
        "\n",
        "    for i in tqdm(range(n_bootstrap), desc=\"Bootstrapping metrics\"):\n",
        "        idx = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
        "        metrics = calculate_all_metrics(\n",
        "            y_true[idx], y_pred[idx], y_proba[idx]\n",
        "        )\n",
        "        boot_metrics.append(metrics)\n",
        "\n",
        "    df_boot = pd.DataFrame(boot_metrics)\n",
        "    ci_results = {}\n",
        "\n",
        "    for col in df_boot.columns:\n",
        "        if df_boot[col].notna().any():\n",
        "            ci_results[f\"{col}_lower\"] = float(df_boot[col].quantile(0.025))\n",
        "            ci_results[f\"{col}_upper\"] = float(df_boot[col].quantile(0.975))\n",
        "\n",
        "    return ci_results\n",
        "\n",
        "\n",
        "\n",
        "def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
        "\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    if n1 < 2 or n2 < 2: return np.nan\n",
        "\n",
        "    m1, m2 = np.mean(group1), np.mean(group2)\n",
        "    s1, s2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
        "\n",
        "    pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
        "\n",
        "    if pooled_std == 0: return np.nan\n",
        "    return (m1 - m2) / pooled_std\n",
        "\n",
        "def cliffs_delta(group1: np.ndarray, group2: np.ndarray) -> float:\n",
        "\n",
        "    if len(group1) == 0 or len(group2) == 0: return np.nan\n",
        "\n",
        "\n",
        "    comparisons = np.sign(np.subtract.outer(group1, group2))\n",
        "    dominance = np.sum(comparisons)\n",
        "\n",
        "    return dominance / (len(group1) * len(group2))\n",
        "\n",
        "def interpret_cohens_d(d: float) -> str:\n",
        "\n",
        "    abs_d = abs(d)\n",
        "    if np.isnan(abs_d): return \"Unknown\"\n",
        "    elif abs_d < 0.2: return \"Negligible\"\n",
        "    elif abs_d < 0.5: return \"Small\"\n",
        "    elif abs_d < 0.8: return \"Medium\"\n",
        "    else: return \"Large\"\n",
        "\n",
        "def interpret_cliffs_delta(delta: float) -> str:\n",
        "\n",
        "    abs_delta = abs(delta)\n",
        "    if np.isnan(abs_delta): return \"Unknown\"\n",
        "    elif abs_delta < 0.147: return \"Negligible\"\n",
        "    elif abs_delta < 0.33: return \"Small\"\n",
        "    elif abs_delta < 0.474: return \"Medium\"\n",
        "    else: return \"Large\"\n",
        "\n",
        "def calculate_power_analysis(cohens_d_val: float, n_sample: int, alpha: float = 0.05) -> Dict[str, Any]:\n",
        "\n",
        "    if np.isnan(cohens_d_val) or n_sample <= 0:\n",
        "        return {'observed_power': np.nan, 'power_interpretation': 'Cannot calculate', 'recommended_n_80_power': np.nan}\n",
        "\n",
        "    z_alpha = norm.ppf(1 - alpha / 2)\n",
        "    delta = abs(cohens_d_val) * np.sqrt(n_sample / 2)\n",
        "    power = 1 - norm.cdf(z_alpha - delta) + norm.cdf(-z_alpha - delta)\n",
        "\n",
        "    if power >= 0.8: interp = \"Adequate (â‰¥80%)\"\n",
        "    elif power >= 0.6: interp = \"Moderate (60-79%)\"\n",
        "    else: interp = \"Low (<60%)\"\n",
        "\n",
        "\n",
        "    if abs(cohens_d_val) > 1e-10:\n",
        "        n_80 = 2 * ((z_alpha + norm.ppf(0.8)) / abs(cohens_d_val)) ** 2\n",
        "        n_80 = int(np.ceil(n_80))\n",
        "    else:\n",
        "        n_80 = np.inf\n",
        "\n",
        "    return {\n",
        "        'observed_power': float(power),\n",
        "        'power_interpretation': interp,\n",
        "        'recommended_n_80_power': n_80\n",
        "    }\n",
        "\n",
        "def mcnemar_test_with_effect_size(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred1: np.ndarray,\n",
        "    y_pred2: np.ndarray\n",
        ") -> Tuple[float, float, str]:\n",
        "\n",
        "    try:\n",
        "        table = mcnemar_table(y_target=y_true, y_model1=y_pred1, y_model2=y_pred2)\n",
        "        result = mcnemar(table, exact=False, correction=True)\n",
        "\n",
        "        b, c = table[0, 1], table[1, 0]\n",
        "\n",
        "        if c == 0:\n",
        "            odds_ratio = 1.0 if b == 0 else np.inf\n",
        "        else:\n",
        "            odds_ratio = b / c\n",
        "\n",
        "\n",
        "        if np.isnan(odds_ratio):\n",
        "            interpretation = \"Unknown\"\n",
        "        elif np.isinf(odds_ratio):\n",
        "            interpretation = \"Complete Dominance\"\n",
        "        elif odds_ratio == 0:\n",
        "            interpretation = \"Complete Dominance (Negative)\"\n",
        "\n",
        "        elif odds_ratio >= 2.0 or odds_ratio <= 0.5:\n",
        "            interpretation = \"Large\"\n",
        "        elif odds_ratio >= 1.25 or odds_ratio <= 0.8:\n",
        "            interpretation = \"Medium\"\n",
        "        elif odds_ratio >= 1.05 or odds_ratio <= 0.95:\n",
        "            interpretation = \"Small\"\n",
        "        else:\n",
        "            interpretation = \"Negligible\"\n",
        "\n",
        "        return float(result.pvalue), float(odds_ratio), interpretation\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"McNemar test failed: {e}\")\n",
        "        return np.nan, np.nan, \"Unknown\"\n",
        "\n",
        "\n",
        "def plot_effect_sizes(\n",
        "    effect_results: Dict[str, Any],\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "\n",
        "    logger.info(\"Generating improved effect size plots...\")\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.family\": CONFIG['plotting']['font_family'],\n",
        "        \"font.size\": 9,\n",
        "        \"axes.titlesize\": 10,\n",
        "        \"axes.labelsize\": 9,\n",
        "        \"xtick.labelsize\": 8,\n",
        "        \"legend.fontsize\": 8,\n",
        "    })\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=CONFIG['plotting']['figsize'],\n",
        "                             dpi=CONFIG['plotting']['dpi'])\n",
        "    ax1, ax2, ax3 = axes\n",
        "\n",
        "    model_a = effect_results.get('Model_A', 'Model A')\n",
        "\n",
        "    color_map = {\n",
        "        'Negligible': '#D3D3D3', 'Small': '#87CEEB',\n",
        "        'Medium': '#FFA500', 'Large': '#FF6347',\n",
        "        'Complete Dominance': '#8B0000', 'Unknown': '#808080'\n",
        "    }\n",
        "\n",
        "\n",
        "    metrics = ['AUROC', 'AUPRC']\n",
        "    vals = [effect_results.get(f'{m}_Cohens_d', np.nan) for m in metrics]\n",
        "    interps = [effect_results.get(f'{m}_Cohens_d_interpretation', 'Unknown') for m in metrics]\n",
        "    colors = [color_map.get(i, '#808080') for i in interps]\n",
        "\n",
        "    bars1 = ax1.bar(metrics, vals, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=0.5)\n",
        "    ax1.axhline(0, color='black', lw=0.8)\n",
        "\n",
        "\n",
        "    for bar, val, interp in zip(bars1, vals, interps):\n",
        "        if not np.isnan(val):\n",
        "            y_pos = val + 0.1 if val >= 0 else val - 0.2\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
        "                     f'{val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "    ax1.set_ylabel(\"Cohen's d\")\n",
        "\n",
        "    ax1.set_title(f\"Effect Sizes (Cohen's d)\\nPositive favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "\n",
        "    max_val = max([abs(v) for v in vals if not np.isnan(v)] + [1])\n",
        "    ax1.set_ylim(-(max_val+0.5), max_val+0.5)\n",
        "    ax1.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "\n",
        "    vals = [effect_results.get(f'{m}_Cliffs_delta', np.nan) for m in metrics]\n",
        "    interps = [effect_results.get(f'{m}_Cliffs_delta_interpretation', 'Unknown') for m in metrics]\n",
        "    colors = [color_map.get(i, '#808080') for i in interps]\n",
        "\n",
        "    bars2 = ax2.bar(metrics, vals, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=0.5)\n",
        "    ax2.axhline(0, color='black', lw=0.8)\n",
        "\n",
        "    for bar, val, interp in zip(bars2, vals, interps):\n",
        "        if not np.isnan(val):\n",
        "            y_pos = val + 0.05 if val >= 0 else val - 0.1\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
        "                     f'{val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "    ax2.set_ylabel(\"Cliff's Delta\")\n",
        "\n",
        "    ax2.set_title(f\"Effect Sizes (Cliff's Delta)\\nPositive favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "    ax2.set_ylim(-1.2, 1.2)\n",
        "    ax2.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "\n",
        "    or_val = effect_results.get('McNemar_Odds_ratio', np.nan)\n",
        "    interp = effect_results.get('McNemar_effect_interpretation', 'Unknown')\n",
        "\n",
        "    if not np.isnan(or_val) and not np.isinf(or_val) and or_val > 0:\n",
        "        log_or = np.log(or_val)\n",
        "        color = color_map.get(interp, '#808080')\n",
        "\n",
        "        ax3.bar(['McNemar'], [log_or], color=color, alpha=0.85, width=0.5, edgecolor='black', linewidth=0.5)\n",
        "        ax3.axhline(0, color='black', lw=0.8, label='OR=1')\n",
        "\n",
        "\n",
        "        for thresh in [np.log(1.25), np.log(2.0)]:\n",
        "            ax3.axhline(thresh, color='gray', linestyle=':', alpha=0.3)\n",
        "            ax3.axhline(-thresh, color='gray', linestyle=':', alpha=0.3)\n",
        "\n",
        "        ax3.text(0, log_or + (0.1 if log_or>0 else -0.2),\n",
        "                 f'OR={or_val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "\n",
        "        ax3_twin = ax3.twinx()\n",
        "        ticks = [0.5, 0.8, 1, 1.25, 2, 3, 5]\n",
        "        ax3_twin.set_yticks(np.log(ticks))\n",
        "        ax3_twin.set_yticklabels(ticks, fontsize=7)\n",
        "        ax3_twin.set_ylabel(\"Odds Ratio\")\n",
        "\n",
        "    ax3.set_ylabel(\"Log(Odds Ratio)\")\n",
        "\n",
        "    ax3.set_title(f\"Classification Difference\\nOR > 1 favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if output_path:\n",
        "        plt.savefig(output_path, dpi=CONFIG['plotting']['dpi'], bbox_inches='tight')\n",
        "        plt.savefig(str(output_path).replace('.png', '.pdf'), bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"Effect size plot saved to {output_path}\")\n",
        "\n",
        "\n",
        "def get_onehot_feature_names_from_ct(column_transformer) -> List[str]:\n",
        "\n",
        "    names = []\n",
        "    for name, transformer, columns in column_transformer.transformers_:\n",
        "        if name == \"remainder\" and transformer == \"drop\":\n",
        "            continue\n",
        "\n",
        "        if (name == \"cat\" and hasattr(transformer, \"named_steps\") and\n",
        "            \"onehot\" in transformer.named_steps):\n",
        "            encoder = transformer.named_steps[\"onehot\"]\n",
        "            for col, categories in zip(columns, encoder.categories_):\n",
        "                names.extend([f\"{col}_{cat}\" for cat in categories])\n",
        "        else:\n",
        "            names.extend(list(columns))\n",
        "\n",
        "    return names\n",
        "\n",
        "def load_and_evaluate_model(\n",
        "    model_path: str,\n",
        "    X_raw: pd.DataFrame,\n",
        "    y_true: np.ndarray\n",
        ") -> Tuple[str, np.ndarray, np.ndarray, float]:\n",
        "\n",
        "    logger.info(f\"Loading model from {model_path}\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        bundle = joblib.load(model_path)\n",
        "\n",
        "        model_name = bundle.get(\"metadata\", {}).get(\n",
        "            \"model_name\",\n",
        "            Path(model_path).stem\n",
        "        )\n",
        "\n",
        "        model = bundle[\"model\"]\n",
        "        preprocessor = bundle.get(\"preprocessor\")\n",
        "        selected_features = bundle.get(\"selected_features\")\n",
        "        threshold = bundle.get(\"youden_j_threshold\", 0.5)\n",
        "\n",
        "        if preprocessor is None:\n",
        "            raise ValueError(f\"Model {model_name} missing preprocessor\")\n",
        "\n",
        "        if selected_features is None:\n",
        "            raise ValueError(f\"Model {model_name} missing selected_features\")\n",
        "\n",
        "\n",
        "        X_transformed = preprocessor.transform(X_raw)\n",
        "        feature_names = get_onehot_feature_names_from_ct(preprocessor)\n",
        "        X_df = pd.DataFrame(X_transformed, columns=feature_names, index=X_raw.index)\n",
        "\n",
        "\n",
        "        missing_features = [f for f in selected_features if f not in X_df.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(\n",
        "                f\"Model {model_name} missing features after transform: \"\n",
        "                f\"{missing_features[:5]}...\"\n",
        "            )\n",
        "\n",
        "        X_selected = X_df[selected_features]\n",
        "\n",
        "\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_proba = model.predict_proba(X_selected)[:, 1]\n",
        "        else:\n",
        "\n",
        "            scores = model.decision_function(X_selected)\n",
        "            y_proba = 1 / (1 + np.exp(-scores))\n",
        "\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "        logger.info(f\"Model '{model_name}' evaluated: \"\n",
        "                   f\"{len(y_proba)} samples, threshold={threshold:.3f}\")\n",
        "\n",
        "        return model_name, y_proba, y_pred, threshold\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load/evaluate model {model_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def calculate_all_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    y_proba: np.ndarray\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = 0\n",
        "\n",
        "\n",
        "    cal_slope, cal_intercept = calculate_calibration_slope_intercept(y_true, y_proba)\n",
        "    spiegel_z, spiegel_p = calculate_spiegelhalter_z(y_true, y_proba)\n",
        "    hl_stat, hl_p = hosmer_lemeshow_test(y_true, y_proba,\n",
        "                                        CONFIG['calibration']['n_bins'])\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'Test_Samples': len(y_true),\n",
        "        'Positive_Rate': float(np.mean(y_true)),\n",
        "        'Intercept': float(cal_intercept),\n",
        "        'Slope': float(cal_slope),\n",
        "        'MCC': float(matthews_corrcoef(y_true, y_pred)),\n",
        "        'Kappa': float(cohen_kappa_score(y_true, y_pred)),\n",
        "        'Brier_Score': float(brier_score_loss(y_true, y_proba)),\n",
        "        'Log_Loss': float(log_loss(y_true, y_proba)),\n",
        "        'ECE': float(calculate_ece(y_true, y_proba,\n",
        "                                  CONFIG['calibration']['n_bins'])),\n",
        "        'MCE': float(calculate_mce(y_true, y_proba,\n",
        "                                  CONFIG['calibration']['n_bins'])),\n",
        "        'Spiegelhalter_Z': float(spiegel_z),\n",
        "        'Spiegelhalter_p': float(spiegel_p),\n",
        "        'HL_statistic': float(hl_stat),\n",
        "        'HL_p_value': float(hl_p),\n",
        "        'AUROC': float(roc_auc_score(y_true, y_proba)),\n",
        "        'AUPRC': float(average_precision_score(y_true, y_proba)),\n",
        "        'F1': float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        'Precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        'Recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        'Accuracy': float(accuracy_score(y_true, y_pred)),\n",
        "        'Balanced_Accuracy': float(balanced_accuracy_score(y_true, y_pred)),\n",
        "        'TN': int(tn),\n",
        "        'FP': int(fp),\n",
        "        'FN': int(fn),\n",
        "        'TP': int(tp),\n",
        "    }\n",
        "\n",
        "\n",
        "    if (tn + fp) > 0:\n",
        "        metrics['Specificity'] = tn / (tn + fp)\n",
        "    else:\n",
        "        metrics['Specificity'] = np.nan\n",
        "\n",
        "    if (tn + fn) > 0:\n",
        "        metrics['NPV'] = tn / (tn + fn)\n",
        "    else:\n",
        "        metrics['NPV'] = np.nan\n",
        "\n",
        "    if (tp + fp) > 0:\n",
        "        metrics['PPV'] = tp / (tp + fp)\n",
        "    else:\n",
        "        metrics['PPV'] = np.nan\n",
        "\n",
        "    if (fp + tn) > 0:\n",
        "        metrics['FPR'] = fp / (fp + tn)\n",
        "    else:\n",
        "        metrics['FPR'] = np.nan\n",
        "\n",
        "    if (fn + tp) > 0:\n",
        "        metrics['FNR'] = fn / (fn + tp)\n",
        "    else:\n",
        "        metrics['FNR'] = np.nan\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def compare_models_effect_size(\n",
        "    model1_data: Tuple[str, np.ndarray, np.ndarray],\n",
        "    model2_data: Tuple[str, np.ndarray, np.ndarray],\n",
        "    y_true: np.ndarray,\n",
        "    n_bootstrap: int = 1000\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    name1, proba1, pred1 = model1_data\n",
        "    name2, proba2, pred2 = model2_data\n",
        "\n",
        "    logger.info(f\"Comparing models: {name1} vs {name2}\")\n",
        "\n",
        "    results = {\n",
        "        'Model_A': name1,\n",
        "        'Model_B': name2,\n",
        "        'N_samples': len(y_true)\n",
        "    }\n",
        "\n",
        "\n",
        "    try:\n",
        "        auroc1 = roc_auc_score(y_true, proba1)\n",
        "        auroc2 = roc_auc_score(y_true, proba2)\n",
        "        auprc1 = average_precision_score(y_true, proba1)\n",
        "        auprc2 = average_precision_score(y_true, proba2)\n",
        "\n",
        "        results.update({\n",
        "            'AUROC_A': float(auroc1), 'AUROC_B': float(auroc2),\n",
        "            'AUROC_diff': float(auroc1 - auroc2),\n",
        "            'AUPRC_A': float(auprc1), 'AUPRC_B': float(auprc2),\n",
        "            'AUPRC_diff': float(auprc1 - auprc2),\n",
        "        })\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to calculate point metrics: {e}\")\n",
        "\n",
        "\n",
        "    rng = np.random.default_rng(CONFIG['bootstrap']['random_seed'])\n",
        "    auroc_s1, auroc_s2 = [], []\n",
        "    auprc_s1, auprc_s2 = [], []\n",
        "\n",
        "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrapping effect sizes\"):\n",
        "        idx = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "\n",
        "        if len(np.unique(y_true[idx])) < 2: continue\n",
        "\n",
        "        try:\n",
        "            auroc_s1.append(roc_auc_score(y_true[idx], proba1[idx]))\n",
        "            auroc_s2.append(roc_auc_score(y_true[idx], proba2[idx]))\n",
        "            auprc_s1.append(average_precision_score(y_true[idx], proba1[idx]))\n",
        "            auprc_s2.append(average_precision_score(y_true[idx], proba2[idx]))\n",
        "        except Exception: continue\n",
        "\n",
        "\n",
        "    for metric, s1, s2 in [('AUROC', auroc_s1, auroc_s2), ('AUPRC', auprc_s1, auprc_s2)]:\n",
        "        if len(s1) == 0: continue\n",
        "\n",
        "        s1, s2 = np.array(s1), np.array(s2)\n",
        "        d_val = cohens_d(s1, s2)\n",
        "        delta_val = cliffs_delta(s1, s2)\n",
        "        power_res = calculate_power_analysis(d_val, len(y_true))\n",
        "\n",
        "        results[f'{metric}_Cohens_d'] = float(d_val)\n",
        "        results[f'{metric}_Cohens_d_interpretation'] = interpret_cohens_d(d_val)\n",
        "        results[f'{metric}_Cliffs_delta'] = float(delta_val)\n",
        "        results[f'{metric}_Cliffs_delta_interpretation'] = interpret_cliffs_delta(delta_val)\n",
        "\n",
        "\n",
        "        results[f'{metric}_observed_power'] = power_res['observed_power']\n",
        "        results[f'{metric}_power_interpretation'] = power_res['power_interpretation']\n",
        "        results[f'{metric}_recommended_n_80_power'] = power_res['recommended_n_80_power']\n",
        "\n",
        "\n",
        "    p_mcnemar, or_mcnemar, interp_mcnemar = mcnemar_test_with_effect_size(\n",
        "        y_true, pred1, pred2\n",
        "    )\n",
        "\n",
        "    results.update({\n",
        "        'McNemar_p_value': float(p_mcnemar),\n",
        "        'McNemar_Odds_ratio': float(or_mcnemar),\n",
        "        'McNemar_effect_interpretation': interp_mcnemar\n",
        "    })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_with_ci(\n",
        "    models_data: List[Tuple[str, np.ndarray, np.ndarray]],\n",
        "    y_true: np.ndarray,\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "\n",
        "    logger.info(\"Generating comparison plots\")\n",
        "\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.family\": CONFIG['plotting']['font_family'],\n",
        "        \"font.size\": 9,\n",
        "        \"axes.titlesize\": 10,\n",
        "        \"axes.labelsize\": 9,\n",
        "        \"xtick.labelsize\": 8,\n",
        "        \"ytick.labelsize\": 8,\n",
        "        \"legend.fontsize\": 8,\n",
        "        \"lines.linewidth\": 1.2,\n",
        "        \"axes.linewidth\": 0.8,\n",
        "        \"grid.linewidth\": 0.3,\n",
        "        \"grid.alpha\": 0.3,\n",
        "    })\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=CONFIG['plotting']['figsize'],\n",
        "                            dpi=CONFIG['plotting']['dpi'])\n",
        "    ax_roc, ax_pr, ax_cal = axes\n",
        "\n",
        "\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(models_data)))\n",
        "\n",
        "    grid_roc = np.linspace(0, 1, 201)\n",
        "    grid_pr = np.linspace(0, 1, 201)\n",
        "    grid_cal = np.linspace(0, 1, 100)\n",
        "\n",
        "    bin_edges = np.linspace(0, 1, CONFIG['calibration']['n_bins'] + 1)\n",
        "\n",
        "\n",
        "    logger.info(\"Plotting ROC curves...\")\n",
        "    for idx, (name, proba, _) in enumerate(models_data):\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, proba)\n",
        "        auc_val = roc_auc_score(y_true, proba)\n",
        "\n",
        "        mean_tpr, lo_tpr, hi_tpr = compute_bootstrap_ci_parallel(\n",
        "            y_true, proba, 'roc', grid_roc,\n",
        "            n_bootstraps=CONFIG['bootstrap']['n_bootstraps'],\n",
        "            n_jobs=CONFIG['bootstrap']['n_jobs'],\n",
        "            random_seed=CONFIG['bootstrap']['random_seed']\n",
        "        )\n",
        "\n",
        "        ax_roc.plot(fpr, tpr, '--', alpha=0.4, lw=0.8, color=colors[idx])\n",
        "        ax_roc.plot(grid_roc, mean_tpr, lw=1.5, label=f\"{name} (AUC={auc_val:.3f})\",\n",
        "                   color=colors[idx])\n",
        "        ax_roc.fill_between(grid_roc, lo_tpr, hi_tpr, alpha=0.2, color=colors[idx])\n",
        "\n",
        "    ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.3, lw=1)\n",
        "    ax_roc.set_xlabel(\"False Positive Rate\")\n",
        "    ax_roc.set_ylabel(\"True Positive Rate\")\n",
        "    ax_roc.set_title(\"ROC Curves with 95% CI\")\n",
        "    ax_roc.legend(loc=\"lower right\", frameon=True, fancybox=True,\n",
        "                 framealpha=0.8, fontsize=7)\n",
        "    ax_roc.grid(True, alpha=0.2)\n",
        "    ax_roc.set_xlim([-0.02, 1.02])\n",
        "    ax_roc.set_ylim([-0.02, 1.02])\n",
        "\n",
        "\n",
        "    logger.info(\"Plotting PR curves...\")\n",
        "    baseline = float(np.mean(y_true))\n",
        "\n",
        "    for idx, (name, proba, _) in enumerate(models_data):\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_true, proba)\n",
        "        ap = average_precision_score(y_true, proba)\n",
        "\n",
        "\n",
        "        mean_prec, lo_prec, hi_prec = compute_bootstrap_ci_parallel(\n",
        "            y_true, proba, 'pr', grid_pr,\n",
        "            n_bootstraps=CONFIG['bootstrap']['n_bootstraps'],\n",
        "            n_jobs=CONFIG['bootstrap']['n_jobs'],\n",
        "            random_seed=CONFIG['bootstrap']['random_seed']\n",
        "        )\n",
        "\n",
        "        ax_pr.step(recall, precision, '--', alpha=0.4, lw=0.8, color=colors[idx])\n",
        "        ax_pr.plot(grid_pr, mean_prec, lw=1.5, label=f\"{name} (AP={ap:.3f})\",\n",
        "                  color=colors[idx])\n",
        "        ax_pr.fill_between(grid_pr, lo_prec, hi_prec, alpha=0.2, color=colors[idx])\n",
        "\n",
        "    ax_pr.axhline(baseline, color='k', linestyle='--', alpha=0.5,\n",
        "                 lw=1, label=f'Baseline (P={baseline:.2f})')\n",
        "    ax_pr.set_xlabel(\"Recall\")\n",
        "    ax_pr.set_ylabel(\"Precision\")\n",
        "    ax_pr.set_title(\"Precision-Recall Curves with 95% CI\")\n",
        "    ax_pr.legend(loc=\"lower left\", frameon=True, fancybox=True,\n",
        "                framealpha=0.8, fontsize=7)\n",
        "    ax_pr.grid(True, alpha=0.2)\n",
        "    ax_pr.set_xlim([-0.02, 1.02])\n",
        "    ax_pr.set_ylim([-0.02, 1.02])\n",
        "\n",
        "    logger.info(\"Plotting calibration curves...\")\n",
        "    for idx, (name, proba, _) in enumerate(models_data):\n",
        "\n",
        "        prob_true, prob_pred = fixed_bin_calibration(y_true, proba, bin_edges)\n",
        "        ece = calculate_ece(y_true, proba, CONFIG['calibration']['n_bins'])\n",
        "\n",
        "        mean_cal, lo_cal, hi_cal = compute_bootstrap_ci_parallel(\n",
        "            y_true, proba, 'cal', grid_cal,\n",
        "            n_bootstraps=CONFIG['bootstrap']['n_bootstraps'],\n",
        "            bin_edges=bin_edges,\n",
        "            n_jobs=CONFIG['bootstrap']['n_jobs'],\n",
        "            random_seed=CONFIG['bootstrap']['random_seed']\n",
        "        )\n",
        "\n",
        "        mask = ~np.isnan(prob_true)\n",
        "        if np.any(mask):\n",
        "            ax_cal.plot(prob_pred[mask], prob_true[mask], 'o--', alpha=0.8,\n",
        "                       markersize=4, lw=1, color=colors[idx],\n",
        "                       label=f\"{name} (ECE={ece:.3f})\")\n",
        "\n",
        "        ax_cal.plot(grid_cal, mean_cal, lw=1.5, color=colors[idx])\n",
        "        ax_cal.fill_between(grid_cal, lo_cal, hi_cal, alpha=0.2, color=colors[idx])\n",
        "\n",
        "    ax_cal.plot([0, 1], [0, 1], 'k:', lw=1.5, label='Perfect calibration')\n",
        "    ax_cal.set_xlabel(\"Mean Predicted Probability\")\n",
        "    ax_cal.set_ylabel(\"Fraction of Positives\")\n",
        "    ax_cal.set_title(\"Calibration Curves with 95% CI\")\n",
        "    ax_cal.legend(loc=\"upper left\", frameon=True, fancybox=True,\n",
        "                 framealpha=0.8, fontsize=7)\n",
        "    ax_cal.grid(True, alpha=0.2)\n",
        "    ax_cal.set_xlim([-0.02, 1.02])\n",
        "    ax_cal.set_ylim([-0.02, 1.02])\n",
        "\n",
        "\n",
        "    plt.tight_layout(pad=2.0)\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = Path(CONFIG['paths']['output_dir']) / \"model_comparison_plots.png\"\n",
        "\n",
        "    plt.savefig(output_path, dpi=CONFIG['plotting']['dpi'], bbox_inches='tight')\n",
        "    plt.savefig(str(output_path).replace('.png', '.pdf'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    logger.info(f\"Plots saved to {output_path}\")\n",
        "\n",
        "def plot_effect_sizes(\n",
        "    effect_results: Dict[str, Any],\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "\n",
        "    logger.info(\"Generating improved effect size plots...\")\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.family\": CONFIG['plotting']['font_family'],\n",
        "        \"font.size\": 9,\n",
        "        \"axes.titlesize\": 10,\n",
        "        \"axes.labelsize\": 9,\n",
        "        \"xtick.labelsize\": 8,\n",
        "        \"legend.fontsize\": 8,\n",
        "    })\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=CONFIG['plotting']['figsize'],\n",
        "                             dpi=CONFIG['plotting']['dpi'])\n",
        "    ax1, ax2, ax3 = axes\n",
        "\n",
        "\n",
        "    model_a = effect_results.get('Model_A', 'Model A')\n",
        "\n",
        "\n",
        "    color_map = {\n",
        "        'Negligible': '#D3D3D3', 'Small': '#87CEEB',\n",
        "        'Medium': '#FFA500', 'Large': '#FF6347',\n",
        "        'Complete Dominance': '#8B0000', 'Unknown': '#808080'\n",
        "    }\n",
        "\n",
        "\n",
        "    metrics = ['AUROC', 'AUPRC']\n",
        "    vals = [effect_results.get(f'{m}_Cohens_d', np.nan) for m in metrics]\n",
        "    interps = [effect_results.get(f'{m}_Cohens_d_interpretation', 'Unknown') for m in metrics]\n",
        "    colors = [color_map.get(i, '#808080') for i in interps]\n",
        "\n",
        "    bars1 = ax1.bar(metrics, vals, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=0.5)\n",
        "    ax1.axhline(0, color='black', lw=0.8)\n",
        "\n",
        "\n",
        "    for bar, val, interp in zip(bars1, vals, interps):\n",
        "        if not np.isnan(val):\n",
        "            y_pos = val + 0.1 if val >= 0 else val - 0.2\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
        "                     f'{val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "    ax1.set_ylabel(\"Cohen's d\")\n",
        "\n",
        "    ax1.set_title(f\"Effect Sizes (Cohen's d)\\nPositive favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "\n",
        "    max_val = max([abs(v) for v in vals if not np.isnan(v)] + [1])\n",
        "    ax1.set_ylim(-(max_val+0.5), max_val+0.5)\n",
        "    ax1.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "\n",
        "    vals = [effect_results.get(f'{m}_Cliffs_delta', np.nan) for m in metrics]\n",
        "    interps = [effect_results.get(f'{m}_Cliffs_delta_interpretation', 'Unknown') for m in metrics]\n",
        "    colors = [color_map.get(i, '#808080') for i in interps]\n",
        "\n",
        "    bars2 = ax2.bar(metrics, vals, color=colors, alpha=0.85, width=0.6, edgecolor='black', linewidth=0.5)\n",
        "    ax2.axhline(0, color='black', lw=0.8)\n",
        "\n",
        "    for bar, val, interp in zip(bars2, vals, interps):\n",
        "        if not np.isnan(val):\n",
        "            y_pos = val + 0.05 if val >= 0 else val - 0.1\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
        "                     f'{val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "    ax2.set_ylabel(\"Cliff's Delta\")\n",
        "\n",
        "    ax2.set_title(f\"Effect Sizes (Cliff's Delta)\\nPositive favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "    ax2.set_ylim(-1.2, 1.2)\n",
        "    ax2.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "\n",
        "    or_val = effect_results.get('McNemar_Odds_ratio', np.nan)\n",
        "    interp = effect_results.get('McNemar_effect_interpretation', 'Unknown')\n",
        "\n",
        "    if not np.isnan(or_val) and not np.isinf(or_val) and or_val > 0:\n",
        "        log_or = np.log(or_val)\n",
        "        color = color_map.get(interp, '#808080')\n",
        "\n",
        "        ax3.bar(['McNemar'], [log_or], color=color, alpha=0.85, width=0.5, edgecolor='black', linewidth=0.5)\n",
        "        ax3.axhline(0, color='black', lw=0.8, label='OR=1')\n",
        "\n",
        "\n",
        "        for thresh in [np.log(1.25), np.log(2.0)]:\n",
        "            ax3.axhline(thresh, color='gray', linestyle=':', alpha=0.3)\n",
        "            ax3.axhline(-thresh, color='gray', linestyle=':', alpha=0.3)\n",
        "\n",
        "        ax3.text(0, log_or + (0.1 if log_or>0 else -0.2),\n",
        "                 f'OR={or_val:.2f}\\n({interp})', ha='center', fontsize=7)\n",
        "\n",
        "\n",
        "        ax3_twin = ax3.twinx()\n",
        "        ticks = [0.5, 0.8, 1, 1.25, 2, 3, 5]\n",
        "        ax3_twin.set_yticks(np.log(ticks))\n",
        "        ax3_twin.set_yticklabels(ticks, fontsize=7)\n",
        "        ax3_twin.set_ylabel(\"Odds Ratio\")\n",
        "\n",
        "    ax3.set_ylabel(\"Log(Odds Ratio)\")\n",
        "\n",
        "    ax3.set_title(f\"Classification Difference\\nOR > 1 favors {model_a}\", fontsize=8, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if output_path:\n",
        "        plt.savefig(output_path, dpi=CONFIG['plotting']['dpi'], bbox_inches='tight')\n",
        "        plt.savefig(str(output_path).replace('.png', '.pdf'), bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"Effect size plot saved to {output_path}\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "\n",
        "    logger = setup_logging()\n",
        "\n",
        "    try:\n",
        "\n",
        "        output_dir = Path(CONFIG['paths']['output_dir'])\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Starting model comparison analysis\")\n",
        "\n",
        "        logger.info(f\"Loading test data from {CONFIG['paths']['test_data']}\")\n",
        "        test_df = pd.read_csv(CONFIG['paths']['test_data'])\n",
        "\n",
        "        if len(test_df) == 0:\n",
        "            raise ValueError(\"Test dataset is empty\")\n",
        "\n",
        "\n",
        "        if 'MDR status' in test_df.columns:\n",
        "            test_df = test_df.rename(columns={'MDR status': 'outcome'})\n",
        "        elif 'outcome' not in test_df.columns:\n",
        "            raise ValueError(\"Test data must contain 'outcome' or 'MDR status' column\")\n",
        "\n",
        "        X_test = test_df.drop(columns=['outcome'])\n",
        "        y_test = test_df['outcome'].values\n",
        "\n",
        "        logger.info(f\"Test data loaded: {len(test_df)} samples, \"\n",
        "                   f\"positive rate = {y_test.mean():.1%}\")\n",
        "\n",
        "        model_files = {\n",
        "            'CatBoost': CONFIG['paths']['model_catboost'],\n",
        "            'Logistic Regression': CONFIG['paths']['model_lr']\n",
        "        }\n",
        "\n",
        "\n",
        "        models_data = []\n",
        "        all_results = []\n",
        "\n",
        "        for model_name, model_path in model_files.items():\n",
        "            logger.info(f\"Evaluating {model_name}...\")\n",
        "\n",
        "            try:\n",
        "                name, proba, pred, threshold = load_and_evaluate_model(\n",
        "                    model_path, X_test, y_test\n",
        "                )\n",
        "\n",
        "                models_data.append((name, proba, pred))\n",
        "\n",
        "                metrics = calculate_all_metrics(y_test, pred, proba)\n",
        "\n",
        "                ci = bootstrap_metrics(\n",
        "                    y_test, pred, proba,\n",
        "                    n_bootstrap=CONFIG['bootstrap']['n_bootstraps']\n",
        "                )\n",
        "\n",
        "\n",
        "                model_results = {\n",
        "                    'Model': name,\n",
        "                    'Threshold': threshold,\n",
        "                    **metrics,\n",
        "                    **ci\n",
        "                }\n",
        "\n",
        "                all_results.append(model_results)\n",
        "\n",
        "                logger.info(f\"âœ“ {name}: AUC={metrics['AUROC']:.3f}, \"\n",
        "                           f\"AP={metrics['AUPRC']:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to evaluate {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not models_data:\n",
        "            raise RuntimeError(\"No models were successfully evaluated\")\n",
        "\n",
        "\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "\n",
        "\n",
        "        priority_cols = ['Model', 'Threshold', 'Test_Samples', 'Positive_Rate',\n",
        "                        'AUROC', 'AUPRC', 'Accuracy', 'F1', 'Precision', 'Recall',\n",
        "                        'Balanced_Accuracy', 'MCC', 'Kappa', 'Brier_Score',\n",
        "                        'Log_Loss', 'ECE', 'MCE', 'Slope', 'Intercept',\n",
        "                        'Spiegelhalter_Z', 'Spiegelhalter_p', 'HL_statistic',\n",
        "                        'HL_p_value', 'Specificity', 'NPV', 'PPV', 'FPR', 'FNR',\n",
        "                        'TN', 'FP', 'FN', 'TP']\n",
        "\n",
        "        remaining_cols = [c for c in results_df.columns if c not in priority_cols]\n",
        "        final_cols = priority_cols + remaining_cols\n",
        "\n",
        "        results_df = results_df[final_cols]\n",
        "        results_path = output_dir / \"model_comparison_results.csv\"\n",
        "        results_df.to_csv(results_path, index=False)\n",
        "        logger.info(f\"Results saved to {results_path}\")\n",
        "\n",
        "\n",
        "        plot_path = output_dir / \"model_comparison_plots.png\"\n",
        "        plot_with_ci(models_data, y_test, str(plot_path))\n",
        "\n",
        "        if len(models_data) == 2:\n",
        "            logger.info(\"Calculating effect sizes between models...\")\n",
        "\n",
        "            effect_results = compare_models_effect_size(\n",
        "                models_data[0], models_data[1], y_test,\n",
        "                n_bootstrap=CONFIG['bootstrap']['n_bootstraps']\n",
        "            )\n",
        "\n",
        "            effect_df = pd.DataFrame([effect_results])\n",
        "            effect_path = output_dir / \"effect_size_analysis.csv\"\n",
        "            effect_df.to_csv(effect_path, index=False)\n",
        "\n",
        "\n",
        "            effect_plot_path = output_dir / \"effect_sizes_plot.png\"\n",
        "            plot_effect_sizes(effect_results, str(effect_plot_path))\n",
        "\n",
        "            logger.info(\"\\n\" + \"=\"*60)\n",
        "            logger.info(\"EFFECT SIZE SUMMARY\")\n",
        "            logger.info(\"=\"*60)\n",
        "\n",
        "            logger.info(f\"AUROC - Cohen's d: {effect_results.get('AUROC_Cohens_d', 'N/A'):.3f} \"\n",
        "                       f\"({effect_results.get('AUROC_Cohens_d_interpretation', 'N/A')})\")\n",
        "            logger.info(f\"AUPRC - Cohen's d: {effect_results.get('AUPRC_Cohens_d', 'N/A'):.3f} \"\n",
        "                       f\"({effect_results.get('AUPRC_Cohens_d_interpretation', 'N/A')})\")\n",
        "\n",
        "            or_val = effect_results.get('McNemar_Odds_ratio', np.nan)\n",
        "            if not np.isnan(or_val):\n",
        "                logger.info(f\"McNemar - Odds Ratio: {or_val:.3f} \"\n",
        "                           f\"({effect_results.get('McNemar_effect_interpretation', 'N/A')})\")\n",
        "\n",
        "            if 'AUROC_observed_power' in effect_results:\n",
        "                logger.info(f\"\\nStatistical Power:\")\n",
        "                logger.info(f\"AUROC Power: {effect_results['AUROC_observed_power']:.1%} \"\n",
        "                           f\"({effect_results['AUROC_power_interpretation']})\")\n",
        "                logger.info(f\"AUPRC Power: {effect_results.get('AUPRC_observed_power', 'N/A')}\")\n",
        "\n",
        "                n_rec = effect_results.get('AUROC_recommended_n_80_power', 'N/A')\n",
        "                logger.info(f\"Recommended n for 80% power: {n_rec}\")\n",
        "\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"ANALYSIS COMPLETE\")\n",
        "        logger.info(\"=\"*60)\n",
        "        logger.info(f\"Results:      {output_dir / 'model_comparison_results.csv'}\")\n",
        "        logger.info(f\"Plots:        {output_dir / 'model_comparison_plots.png'}\")\n",
        "\n",
        "        if len(models_data) == 2:\n",
        "            logger.info(f\"Effect Sizes: {output_dir / 'effect_size_analysis.csv'}\")\n",
        "            logger.info(f\"Effect Plot:  {output_dir / 'effect_sizes_plot.png'}\")\n",
        "\n",
        "        logger.info(f\"Log file:     {output_dir / 'logs'} directory\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Fatal error in model comparison: {e}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        logger.info(\"Analysis finished\")\n",
        "        logging.shutdown()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KJMAIHrRvEmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Decision Curve Analysis**  \n",
        "Perform decision curve analysis (DCA) to evaluate clinical benefit across threshold probabilities."
      ],
      "metadata": {
        "id": "LvZxP008vIlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "os.makedirs(\"model_comparison\", exist_ok=True)\n",
        "N_BOOT = 1000\n",
        "\n",
        "CLIN_THRESH = np.linspace(0.05, 0.5, 100)\n",
        "RNG = np.random.default_rng(42)\n",
        "COLORS = {\n",
        "    \"CatBoost\": \"#0173B2\",\n",
        "    \"Logistic Regression\": \"#DE8F05\"\n",
        "}\n",
        "Y_OFFSETS = {\"CatBoost\": 0.02, \"Logistic Regression\": -0.015}\n",
        "TEXTBOX = dict(boxstyle=\"round,pad=0.14\", fc=\"white\", ec=\"gray\", alpha=0.81, linewidth=0.46)\n",
        "YOUDEN_TEXTBOX = dict(boxstyle=\"round,pad=0.09\", fc=\"lightgray\", ec=\"gray\", alpha=0.73, linewidth=0.3)\n",
        "\n",
        "\n",
        "print(\"Loading test_set.csv...\")\n",
        "\n",
        "try:\n",
        "    test_df = pd.read_csv(\"test_set.csv\")\n",
        "    if \"MDR status\" in test_df.columns:\n",
        "        test_df = test_df.rename(columns={\"MDR status\": \"outcome\"})\n",
        "    X_test = test_df.drop(\"outcome\", axis=1)\n",
        "    y_test = test_df[\"outcome\"].values\n",
        "    print(f\"âœ… Test data loaded: {len(test_df)} samples\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Error: test_set.csv not found.\")\n",
        "    exit()\n",
        "\n",
        "def get_onehot_feature_names_from_ct(column_transformer):\n",
        "    names = []\n",
        "    for nm, pipe, cols in column_transformer.transformers_:\n",
        "        if nm == \"remainder\" and pipe == \"drop\": continue\n",
        "        if nm == \"cat\" and hasattr(pipe, \"named_steps\") and \"onehot\" in pipe.named_steps:\n",
        "            cats = pipe.named_steps[\"onehot\"].categories_\n",
        "            for col, cat_list in zip(cols, cats):\n",
        "                names += [f\"{col}_{cat}\" for cat in cat_list]\n",
        "        else:\n",
        "            names += list(cols)\n",
        "    return names\n",
        "\n",
        "model_paths = {\n",
        "    \"CatBoost\": \"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\",\n",
        "    \"Logistic Regression\": \"deployment_artifacts/LogisticRegression_with_thresholds_v1.0.0.pkl\"\n",
        "}\n",
        "\n",
        "loaded_models = {}\n",
        "for name, path in model_paths.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"âš ï¸ Warning: {path} not found. Skipping {name}.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        bundle = joblib.load(path)\n",
        "        model = bundle[\"model\"]\n",
        "        pre = bundle.get(\"preprocessor\")\n",
        "        sel_feats = bundle.get(\"selected_features\")\n",
        "\n",
        "        thr = bundle.get(\"youden_j_threshold\") or bundle.get(\"metadata\", {}).get(\"threshold\", 0.5)\n",
        "\n",
        "        if pre is None or sel_feats is None:\n",
        "            raise ValueError(\"Missing preprocessor or selected_features\")\n",
        "        loaded_models[name] = {\"model\": model, \"preprocessor\": pre, \"selected_features\": sel_feats, \"youden_threshold\": thr}\n",
        "        print(f\"âœ… {name} model loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading {name}: {e}\")\n",
        "\n",
        "if not loaded_models:\n",
        "    print(\"âŒ No models loaded. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "def get_predictions(model_info, X_raw):\n",
        "    pre = model_info[\"preprocessor\"]\n",
        "    sel_feats = model_info[\"selected_features\"]\n",
        "\n",
        "    X_trans = pre.transform(X_raw)\n",
        "    feat_names = get_onehot_feature_names_from_ct(pre)\n",
        "\n",
        "\n",
        "    if hasattr(X_trans, \"toarray\"): X_trans = X_trans.toarray()\n",
        "    X_df = pd.DataFrame(X_trans, columns=feat_names, index=X_raw.index)\n",
        "\n",
        "    missing = [f for f in sel_feats if f not in X_df.columns]\n",
        "    if missing: raise ValueError(f\"Missing transformed columns: {missing[:3]}...\")\n",
        "\n",
        "    X_eval = X_df[sel_feats]\n",
        "    mdl = model_info[\"model\"]\n",
        "\n",
        "    try:\n",
        "        proba = mdl.predict_proba(X_eval)[:, 1]\n",
        "    except AttributeError:\n",
        "        scores = mdl.decision_function(X_eval)\n",
        "        proba = 1 / (1 + np.exp(-scores))\n",
        "    return proba\n",
        "\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions, youden_thresholds = {}, {}\n",
        "for name, info in loaded_models.items():\n",
        "    try:\n",
        "        predictions[name] = get_predictions(info, X_test)\n",
        "        youden_thresholds[name] = info[\"youden_threshold\"]\n",
        "        print(f\"âœ… {name} predictions generated (Youden={youden_thresholds[name]:.3f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed for {name}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_net_benefit(y_true, y_pred_prob, thresholds):\n",
        "    n = len(y_true)\n",
        "    nb = []\n",
        "    for t in thresholds:\n",
        "        tp = ((y_pred_prob >= t) & (y_true == 1)).sum()\n",
        "        fp = ((y_pred_prob >= t) & (y_true == 0)).sum()\n",
        "\n",
        "        weight = t / (1 - t) if t < 1.0 else 0\n",
        "\n",
        "        val = (tp / n) - (fp / n) * weight\n",
        "        nb.append(val)\n",
        "    return np.array(nb)\n",
        "\n",
        "def compute_clinical_utility_indexes(y_true, y_pred_prob, thresholds):\n",
        "    results = {'NNT': [], 'FP_per_TP': []}\n",
        "    for t in thresholds:\n",
        "        pred_pos = y_pred_prob >= t\n",
        "        tp = (pred_pos & (y_true == 1)).sum()\n",
        "        fp = (pred_pos & (y_true == 0)).sum()\n",
        "\n",
        "        if tp == 0:\n",
        "            nnt, fp_per_tp = np.nan, np.nan\n",
        "        else:\n",
        "\n",
        "            nnt = np.sum(pred_pos) / tp\n",
        "            fp_per_tp = fp / tp\n",
        "\n",
        "        results['NNT'].append(nnt)\n",
        "        results['FP_per_TP'].append(fp_per_tp)\n",
        "    return {k: np.array(v) for k, v in results.items()}\n",
        "\n",
        "def bootstrap_utility_curves(y_true, y_pred_prob, thresholds, n_boot=1000, rng=None):\n",
        "    if rng is None: rng = np.random.default_rng(42)\n",
        "    n = len(y_true)\n",
        "\n",
        "\n",
        "    nnt_boot = np.full((n_boot, len(thresholds)), np.nan)\n",
        "    fppt_boot = np.full((n_boot, len(thresholds)), np.nan)\n",
        "\n",
        "    for i in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred_prob[idx]\n",
        "\n",
        "\n",
        "        if len(np.unique(yt)) < 2: continue\n",
        "\n",
        "        for j, t in enumerate(thresholds):\n",
        "            pred_pos = yp >= t\n",
        "            tp = np.sum(pred_pos & (yt == 1))\n",
        "            fp = np.sum(pred_pos & (yt == 0))\n",
        "\n",
        "            if tp > 0:\n",
        "                nnt_boot[i, j] = np.sum(pred_pos) / tp\n",
        "                fppt_boot[i, j] = fp / tp\n",
        "\n",
        "    return {\n",
        "        'nnt_mean': np.nanmean(nnt_boot, axis=0),\n",
        "        'nnt_low':  np.nanpercentile(nnt_boot, 2.5, axis=0),\n",
        "        'nnt_high': np.nanpercentile(nnt_boot, 97.5, axis=0),\n",
        "        'fppt_mean': np.nanmean(fppt_boot, axis=0),\n",
        "        'fppt_low':  np.nanpercentile(fppt_boot, 2.5, axis=0),\n",
        "        'fppt_high': np.nanpercentile(fppt_boot, 97.5, axis=0),\n",
        "    }\n",
        "\n",
        "results, utility_bands, clinical_utilities = {}, {}, {}\n",
        "\n",
        "for name, proba in predictions.items():\n",
        "    print(f\"\\nBootstrapping {name} Net Benefit and Utility Indices...\")\n",
        "    boot_mat = np.full((N_BOOT, len(CLIN_THRESH)), np.nan)\n",
        "    skip = 0\n",
        "\n",
        "    for i in tqdm(range(N_BOOT)):\n",
        "        idx = RNG.choice(len(y_test), len(y_test), replace=True)\n",
        "        ys, ps = y_test[idx], proba[idx]\n",
        "\n",
        "        if len(np.unique(ys)) < 2:\n",
        "            skip += 1\n",
        "            continue\n",
        "\n",
        "        boot_mat[i] = compute_net_benefit(ys, ps, CLIN_THRESH)\n",
        "\n",
        "    if skip: print(f\"âš ï¸ Skipped {skip}/{N_BOOT} bootstraps\")\n",
        "\n",
        "\n",
        "    valid_mask = ~np.isnan(boot_mat).all(axis=1)\n",
        "    valid = boot_mat[valid_mask]\n",
        "\n",
        "    mean_nb = np.nanmean(valid, axis=0)\n",
        "    low_ci = np.nanpercentile(valid, 2.5, axis=0)\n",
        "    high_ci = np.nanpercentile(valid, 97.5, axis=0)\n",
        "\n",
        "\n",
        "    opt_idx = int(np.nanargmax(mean_nb))\n",
        "\n",
        "    results[name] = {\n",
        "        \"thresholds\": CLIN_THRESH,\n",
        "        \"net_benefit\": mean_nb,\n",
        "        \"lower_ci\": low_ci,\n",
        "        \"upper_ci\": high_ci,\n",
        "        \"optimal_threshold\": float(CLIN_THRESH[opt_idx]),\n",
        "        \"optimal_net_benefit\": float(mean_nb[opt_idx]),\n",
        "        \"youden_threshold\": float(youden_thresholds.get(name, np.nan)),\n",
        "    }\n",
        "\n",
        "    clinical_utilities[name] = compute_clinical_utility_indexes(y_test, proba, CLIN_THRESH)\n",
        "    utility_bands[name] = bootstrap_utility_curves(y_test, proba, CLIN_THRESH, n_boot=N_BOOT, rng=RNG)\n",
        "\n",
        "\n",
        "print(\"\\nPlotting DCA and Clinical Utility Indices...\")\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 7, 'axes.titlesize': 7, 'axes.labelsize': 7,\n",
        "    'xtick.labelsize': 6, 'ytick.labelsize': 6, 'legend.fontsize': 6,\n",
        "    'axes.linewidth': 0.7, 'lines.linewidth': 1.2, 'figure.dpi': 600\n",
        "})\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3.5), dpi=600)\n",
        "ax_dca, ax_nnt, ax_fppt = axes\n",
        "\n",
        "\n",
        "prev = float(y_test.mean())\n",
        "\n",
        "treat_all = prev - (1 - prev) * (CLIN_THRESH / (1 - CLIN_THRESH))\n",
        "\n",
        "ax_dca.axhline(0, linestyle=\"-\", color=\"black\", alpha=0.7, linewidth=0.8, label=\"Treat None\")\n",
        "ax_dca.plot(CLIN_THRESH, treat_all, \"-.\", color=\"tab:red\", alpha=0.82, linewidth=1.0, label=\"Treat All\")\n",
        "\n",
        "for name, res in results.items():\n",
        "    color = COLORS.get(name, \"#333333\")\n",
        "\n",
        "    ax_dca.plot(res[\"thresholds\"], res[\"net_benefit\"], color=color, label=name, linewidth=1.2)\n",
        "    ax_dca.fill_between(res[\"thresholds\"], res[\"lower_ci\"], res[\"upper_ci\"], color=color, alpha=0.14)\n",
        "\n",
        "\n",
        "    ot, nb_opt = res[\"optimal_threshold\"], res[\"optimal_net_benefit\"]\n",
        "\n",
        "    if 0 < ot < 0.5:\n",
        "        ax_dca.axvline(ot, linestyle=\":\", color=color, alpha=0.85, linewidth=0.8)\n",
        "        ax_dca.text(ot + 0.004, nb_opt + 0.012 + Y_OFFSETS.get(name, 0),\n",
        "                   f\"Opt: {ot:.2f}\", color=color, fontsize=6.2, bbox=TEXTBOX)\n",
        "\n",
        "ax_dca.set_xlabel('Threshold probability', fontsize=7)\n",
        "ax_dca.set_ylabel('Net benefit', fontsize=7)\n",
        "ax_dca.set_xlim(CLIN_THRESH[0], CLIN_THRESH[-1])\n",
        "\n",
        "\n",
        "y_max = max([max(res[\"upper_ci\"]) for res in results.values()] + [max(treat_all)])\n",
        "ax_dca.set_ylim(-0.05, y_max + 0.05)\n",
        "\n",
        "ax_dca.set_title(f\"Decision Curve Analysis\\n(n={len(y_test):,}, prevalence={prev:.1%})\", pad=9)\n",
        "ax_dca.legend(loc=\"upper right\", frameon=False, fontsize=6)\n",
        "ax_dca.grid(True, alpha=0.13, lw=0.4, linestyle='--')\n",
        "ax_dca.spines['top'].set_visible(False)\n",
        "ax_dca.spines['right'].set_visible(False)\n",
        "\n",
        "\n",
        "for name in results.keys():\n",
        "    color = COLORS.get(name, \"#333333\")\n",
        "    ax_nnt.plot(CLIN_THRESH, utility_bands[name]['nnt_mean'], color=color, label=name)\n",
        "    ax_nnt.fill_between(CLIN_THRESH, utility_bands[name]['nnt_low'], utility_bands[name]['nnt_high'], color=color, alpha=0.13)\n",
        "\n",
        "ax_nnt.set_xlabel(\"Threshold probability\", fontsize=7)\n",
        "ax_nnt.set_ylabel(\"Number Needed to Evaluate\", fontsize=7)\n",
        "ax_nnt.set_title(\"Number Needed to Evaluate\\n(to find 1 positive)\", pad=9, fontsize=7)\n",
        "ax_nnt.grid(True, alpha=0.14, lw=0.4, linestyle='--')\n",
        "ax_nnt.spines['top'].set_visible(False)\n",
        "ax_nnt.spines['right'].set_visible(False)\n",
        "\n",
        "\n",
        "for name in results.keys():\n",
        "    color = COLORS.get(name, \"#333333\")\n",
        "    ax_fppt.plot(CLIN_THRESH, utility_bands[name]['fppt_mean'], color=color, label=name)\n",
        "    ax_fppt.fill_between(CLIN_THRESH, utility_bands[name]['fppt_low'], utility_bands[name]['fppt_high'], color=color, alpha=0.13)\n",
        "\n",
        "ax_fppt.set_xlabel(\"Threshold probability\", fontsize=7)\n",
        "ax_fppt.set_ylabel(\"FP per TP\", fontsize=7)\n",
        "ax_fppt.set_title(\"False Positives per True Positive\", pad=9, fontsize=7)\n",
        "ax_fppt.grid(True, alpha=0.14, lw=0.4, linestyle='--')\n",
        "ax_fppt.spines['top'].set_visible(False)\n",
        "ax_fppt.spines['right'].set_visible(False)\n",
        "\n",
        "plt.tight_layout(pad=0.5)\n",
        "plt.savefig(\"model_comparison/dca_comprehensive_plot.png\", dpi=600, bbox_inches=\"tight\", facecolor='white')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "dca_results_df = pd.DataFrame({'threshold': CLIN_THRESH})\n",
        "for name in results:\n",
        "    dca_results_df[f'{name}_net_benefit'] = results[name]['net_benefit']\n",
        "    dca_results_df[f'{name}_lower_ci'] = results[name]['lower_ci']\n",
        "    dca_results_df[f'{name}_upper_ci'] = results[name]['upper_ci']\n",
        "    dca_results_df[f'{name}_NNT_mean'] = utility_bands[name]['nnt_mean']\n",
        "    dca_results_df[f'{name}_FPPT_mean'] = utility_bands[name]['fppt_mean']\n",
        "\n",
        "dca_results_df.to_csv('model_comparison/dca_results_with_utility.csv', index=False)\n",
        "print(\"âœ… Done.\")"
      ],
      "metadata": {
        "id": "Nd4zPZAE0Dfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Model Interpretability: SHAP Analysis**  \n",
        "Apply SHAP values to explain global and local feature contributions to model predictions."
      ],
      "metadata": {
        "id": "d72yFux-4Yk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import joblib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import norm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if \"DISPLAY\" not in os.environ:\n",
        "    matplotlib.use(\"Agg\")\n",
        "\n",
        "@dataclass\n",
        "class SHAPConfig:\n",
        "    model_bundle_path: str\n",
        "    test_data_path: str\n",
        "    output_dir: str = \"shap_results\"\n",
        "    n_bootstrap: int = 200\n",
        "    sample_size: Optional[int] = None\n",
        "    top_features: int = 10\n",
        "    n_force_plots: int = 5\n",
        "    n_dependence_plots: int = 5\n",
        "    random_state: int = 42\n",
        "    plot_dpi: int = 600\n",
        "    plot_style: str = \"seaborn-v0_8-whitegrid\"\n",
        "    cache_shap: bool = True\n",
        "    cache_dir: str = \"shap_cache\"\n",
        "    plot_color: str = \"#0173B2\"\n",
        "    error_color: str = \"#DE8F05\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not Path(self.model_bundle_path).exists():\n",
        "            raise FileNotFoundError(f\"Model bundle not found: {self.model_bundle_path}\")\n",
        "        if not Path(self.test_data_path).exists():\n",
        "            raise FileNotFoundError(f\"Test data not found: {self.test_data_path}\")\n",
        "\n",
        "        output_path = Path(self.output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        (output_path / \"force_plots\").mkdir(exist_ok=True)\n",
        "        (output_path / \"tables\").mkdir(exist_ok=True)\n",
        "        (output_path / \"bootstrap\").mkdir(exist_ok=True)\n",
        "\n",
        "        if self.cache_shap:\n",
        "            Path(self.cache_dir).mkdir(exist_ok=True)\n",
        "\n",
        "class SHAPAnalyzer:\n",
        "\n",
        "    def __init__(self, config: SHAPConfig):\n",
        "        self.config = config\n",
        "        self.rng = np.random.default_rng(config.random_state)\n",
        "        self.results = {}\n",
        "\n",
        "\n",
        "        try:\n",
        "            plt.style.use(config.plot_style)\n",
        "        except OSError:\n",
        "            plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "        plt.rcParams.update({\n",
        "            \"font.family\": \"DejaVu Sans\",\n",
        "            \"font.size\": 14,\n",
        "            \"axes.titlesize\": 18,\n",
        "            \"axes.labelsize\": 16,\n",
        "            \"xtick.labelsize\": 12,\n",
        "            \"ytick.labelsize\": 12,\n",
        "            \"figure.dpi\": 600,\n",
        "            \"savefig.bbox\": \"tight\"\n",
        "        })\n",
        "\n",
        "\n",
        "    def _get_short_names_map(self) -> Dict[str, str]:\n",
        "        return {\n",
        "            \"Institution Type_Hospital\": \"Setting: Hospital\",\n",
        "            \"Institution Type_Lab\": \"Setting: Lab\",\n",
        "            \"Gender_M\": \"Male\",\n",
        "            \"Gender_F\": \"Female\",\n",
        "            \"Bacteria type_E. coli\": \"E. coli\",\n",
        "            \"Bacteria type_Klebsiella Spp\": \"Klebsiella\",\n",
        "            \"Bacteria type_Pseudomonas Spp\": \"Pseudomonas\",\n",
        "            \"Healthcare Sector_Governmental\": \"Sector: Gov\",\n",
        "            \"Healthcare Sector_Private\": \"Sector: Private\",\n",
        "        }\n",
        "\n",
        "    def load_model_bundle(self) -> Dict:\n",
        "        bundle_path = Path(self.config.model_bundle_path)\n",
        "        try:\n",
        "            bundle = joblib.load(bundle_path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model bundle: {e}\") from e\n",
        "        return bundle\n",
        "\n",
        "    def extract_base_model(self, model):\n",
        "        if isinstance(model, CalibratedClassifierCV):\n",
        "            if hasattr(model, 'calibrated_classifiers_'):\n",
        "                return model.calibrated_classifiers_[0].estimator\n",
        "            return model.base_estimator\n",
        "        return model\n",
        "\n",
        "    def get_feature_names(self, preprocessor) -> List[str]:\n",
        "        try:\n",
        "            return list(preprocessor.get_feature_names_out())\n",
        "        except AttributeError:\n",
        "            return self._get_feature_names_fallback(preprocessor)\n",
        "\n",
        "    def _get_feature_names_fallback(self, column_transformer) -> List[str]:\n",
        "        names = []\n",
        "        for name, transformer, columns in column_transformer.transformers_:\n",
        "            if name == \"remainder\" and transformer == \"drop\": continue\n",
        "            if (name == \"cat\" and hasattr(transformer, \"named_steps\") and\n",
        "                \"onehot\" in transformer.named_steps):\n",
        "                encoder = transformer.named_steps[\"onehot\"]\n",
        "                for col, categories in zip(columns, encoder.categories_):\n",
        "                    names.extend([f\"{col}_{cat}\" for cat in categories])\n",
        "            else:\n",
        "                names.extend(list(columns))\n",
        "        return names\n",
        "\n",
        "    def clean_feature_names(self, feature_names: List[str]) -> List[str]:\n",
        "        cleaned = []\n",
        "        for name in feature_names:\n",
        "            if \"__\" in name: name = name.split(\"__\")[-1]\n",
        "            cleaned.append(name)\n",
        "        return cleaned\n",
        "\n",
        "    def load_and_preprocess_data(self, bundle: Dict) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        logger.info(\"Loading test data...\")\n",
        "        test_df = pd.read_csv(self.config.test_data_path)\n",
        "        target_col = \"MDR status\" if \"MDR status\" in test_df.columns else \"outcome\"\n",
        "\n",
        "        X_raw = test_df.drop(columns=[target_col])\n",
        "        y = test_df[target_col]\n",
        "\n",
        "        preprocessor = bundle['preprocessor']\n",
        "        X_trans = preprocessor.transform(X_raw)\n",
        "        if hasattr(X_trans, 'toarray'): X_trans = X_trans.toarray()\n",
        "\n",
        "        raw_feature_names = self.get_feature_names(preprocessor)\n",
        "        cleaned_names = self.clean_feature_names(raw_feature_names)\n",
        "        feature_names = cleaned_names if X_trans.shape[1] == len(cleaned_names) else raw_feature_names\n",
        "\n",
        "        X_full = pd.DataFrame(X_trans, columns=feature_names, index=X_raw.index)\n",
        "        selected_features = bundle['selected_features']\n",
        "        X_selected = X_full[selected_features]\n",
        "        return X_selected, y\n",
        "\n",
        "    def compute_shap_values(self, bundle, X: pd.DataFrame, use_cache: bool = True) -> Tuple[np.ndarray, float, float]:\n",
        "        full_model = bundle['model']\n",
        "        if use_cache and self.config.cache_shap:\n",
        "            cache_file = self._get_cache_path(full_model, X)\n",
        "            if cache_file.exists():\n",
        "                logger.info(f\"Loading cached SHAP values from {cache_file}\")\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "\n",
        "        base_model = self.extract_base_model(full_model)\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(base_model)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"TreeExplainer failed, falling back to KernelExplainer: {e}\")\n",
        "            explainer = shap.KernelExplainer(full_model.predict_proba, X.iloc[:100])\n",
        "\n",
        "        shap_values = explainer.shap_values(X)\n",
        "        base_expected_value = explainer.expected_value\n",
        "\n",
        "        if isinstance(shap_values, list):\n",
        "            if len(shap_values) == 2:\n",
        "                shap_values = shap_values[1]\n",
        "                if isinstance(base_expected_value, (list, np.ndarray)):\n",
        "                    base_expected_value = base_expected_value[1]\n",
        "            else:\n",
        "                shap_values = shap_values if len(shap_values) > 0 else shap_values\n",
        "\n",
        "        shap_values = np.asarray(shap_values, dtype=np.float32)\n",
        "        base_expected_value = float(base_expected_value)\n",
        "\n",
        "        calibrated_expected_value = np.nan\n",
        "        if hasattr(full_model, \"predict_proba\"):\n",
        "            probs = full_model.predict_proba(X)[:, 1]\n",
        "            calibrated_expected_value = float(np.mean(probs))\n",
        "\n",
        "        if use_cache and self.config.cache_shap:\n",
        "            with open(self._get_cache_path(full_model, X), 'wb') as f:\n",
        "                pickle.dump((shap_values, base_expected_value, calibrated_expected_value), f)\n",
        "\n",
        "        return shap_values, base_expected_value, calibrated_expected_value\n",
        "\n",
        "    def _get_cache_path(self, model, X: pd.DataFrame) -> Path:\n",
        "        import hashlib\n",
        "        from pandas.util import hash_pandas_object\n",
        "        model_hash = hashlib.md5(str(type(model)).encode()).hexdigest()[:8]\n",
        "        data_hash = str(hash_pandas_object(X).sum())[:8]\n",
        "        return Path(self.config.cache_dir) / f\"shap_{model_hash}_{data_hash}.pkl\"\n",
        "\n",
        "    def bootstrap_shap_importance(self, shap_values: np.ndarray, sample_size: Optional[int] = None) -> Dict:\n",
        "        n_samples, n_features = shap_values.shape\n",
        "        if sample_size is None: sample_size = min(n_samples, self.config.sample_size or n_samples)\n",
        "\n",
        "        logger.info(f\"Bootstrapping SHAP importance (n={self.config.n_bootstrap})\")\n",
        "        feat_imp_boot = np.zeros((self.config.n_bootstrap, n_features), dtype=np.float32)\n",
        "        shap_mean_boot = np.zeros((self.config.n_bootstrap, n_features), dtype=np.float32)\n",
        "        all_indices = self.rng.integers(0, n_samples, size=(self.config.n_bootstrap, sample_size))\n",
        "\n",
        "        for i in tqdm(range(self.config.n_bootstrap), desc=\"Bootstrapping\"):\n",
        "            indices = all_indices[i]\n",
        "            sample_shap = shap_values[indices]\n",
        "            feat_imp_boot[i] = np.mean(np.abs(sample_shap), axis=0)\n",
        "            shap_mean_boot[i] = np.mean(sample_shap, axis=0)\n",
        "\n",
        "        return {\n",
        "            'feature_importance_mean': np.mean(feat_imp_boot, axis=0),\n",
        "            'feature_importance_ci_lower': np.percentile(feat_imp_boot, 2.5, axis=0),\n",
        "            'feature_importance_ci_upper': np.percentile(feat_imp_boot, 97.5, axis=0),\n",
        "            'shap_mean_mean': np.mean(shap_mean_boot, axis=0),\n",
        "            'n_bootstrap': self.config.n_bootstrap\n",
        "        }\n",
        "\n",
        "    def plot_bootstrap_importance(self, bootstrap_results: Dict, feature_names: List[str], top_n: Optional[int] = None):\n",
        "        if top_n is None: top_n = self.config.top_features\n",
        "        mean_importance = bootstrap_results['feature_importance_mean']\n",
        "        sorted_idx = np.argsort(mean_importance)[::-1][:top_n]\n",
        "\n",
        "        plot_names = [feature_names[i] for i in sorted_idx]\n",
        "        plot_means = mean_importance[sorted_idx]\n",
        "        plot_ci_lower = bootstrap_results['feature_importance_ci_lower'][sorted_idx]\n",
        "        plot_ci_upper = bootstrap_results['feature_importance_ci_upper'][sorted_idx]\n",
        "        plot_signed = bootstrap_results['shap_mean_mean'][sorted_idx]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, max(6, top_n * 0.6)), dpi=self.config.plot_dpi)\n",
        "        y_pos = np.arange(len(plot_names))\n",
        "        xerr = [plot_means - plot_ci_lower, plot_ci_upper - plot_means]\n",
        "\n",
        "        bars = ax.barh(y_pos, plot_means, xerr=xerr, color=self.config.plot_color,\n",
        "                      alpha=0.8, ecolor=self.config.error_color, capsize=6, error_kw={'linewidth': 2})\n",
        "\n",
        "        max_val = np.max(plot_ci_upper)\n",
        "        for i, (bar, mean_val) in enumerate(zip(bars, plot_means)):\n",
        "            sign_char = \"+\" if plot_signed[i] > 0 else \"-\"\n",
        "            label_text = f'{mean_val:.4f} {sign_char}'\n",
        "            ax.text(plot_ci_upper[i] + (max_val * 0.02), bar.get_y() + bar.get_height()/2,\n",
        "                    label_text, va='center', ha='left', fontsize=12,\n",
        "                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(plot_names)\n",
        "        ax.set_xlabel('Mean Absolute SHAP Value (95% CI)')\n",
        "        ax.set_title(f'Top {top_n} Feature Importance (Bootstrap N={bootstrap_results[\"n_bootstrap\"]})', pad=20)\n",
        "        ax.set_xlim(0, max_val * 1.25)\n",
        "        ax.invert_yaxis()\n",
        "        ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "        plt.savefig(Path(self.config.output_dir) / \"bootstrap_feature_importance.png\", bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_force_plots(self, shap_values: np.ndarray, expected_value: float, X: pd.DataFrame, y: pd.Series):\n",
        "        output_dir = Path(self.config.output_dir) / \"force_plots\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        n_samples = min(self.config.n_force_plots, X.shape[0])\n",
        "\n",
        "        logger.info(f\"Generating {n_samples} High-Resolution Force Plots...\")\n",
        "\n",
        "        for idx in range(n_samples):\n",
        "            try:\n",
        "\n",
        "                plt.figure(figsize=(24, 6), dpi=600)\n",
        "\n",
        "                shap.force_plot(\n",
        "                    expected_value,\n",
        "                    shap_values[idx],\n",
        "                    X.iloc[idx],\n",
        "                    matplotlib=True,\n",
        "                    show=False,\n",
        "                    text_rotation=30,\n",
        "                    contribution_threshold=0.02\n",
        "                )\n",
        "\n",
        "\n",
        "                status = \"MDR\" if int(y.iloc[idx]) == 1 else \"Non-MDR\"\n",
        "                plt.title(f\"Force Plot (Sample {idx}) - True Label: {status}\", fontsize=20, pad=40, weight='bold')\n",
        "\n",
        "                plt.tight_layout()\n",
        "\n",
        "                save_path = output_dir / f\"sample_{idx}_force_plot.png\"\n",
        "                plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to generate static force plot for sample {idx}: {e}\")\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "            try:\n",
        "                plot_html = shap.force_plot(\n",
        "                    expected_value, shap_values[idx], X.iloc[idx],\n",
        "                    matplotlib=False, link=\"logit\"\n",
        "                )\n",
        "                shap.save_html(str(output_dir / f\"sample_{idx}_prob.html\"), plot_html)\n",
        "            except Exception: pass\n",
        "\n",
        "        logger.info(f\"âœ… Saved High-Res force plots to {output_dir}\")\n",
        "\n",
        "    def plot_shap_summary(self, shap_values: np.ndarray, X: pd.DataFrame):\n",
        "        output_dir = Path(self.config.output_dir)\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        shap.summary_plot(shap_values, X, show=False, max_display=15, plot_size=(14, 10))\n",
        "        plt.title('SHAP Feature Importance Summary', fontsize=20, pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / \"shap_summary_beeswarm.png\", dpi=600, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        shap.summary_plot(shap_values, X, plot_type='bar', show=False, max_display=15, plot_size=(14, 10))\n",
        "        plt.title('Mean Absolute SHAP Values', fontsize=20, pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / \"shap_summary_bar.png\", dpi=600, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def run_analysis(self):\n",
        "        logger.info(\"Starting SHAP analysis pipeline\")\n",
        "        try:\n",
        "            bundle = self.load_model_bundle()\n",
        "            X, y = self.load_and_preprocess_data(bundle)\n",
        "\n",
        "            shap_values, base_ev, calib_ev = self.compute_shap_values(bundle, X, use_cache=self.config.cache_shap)\n",
        "\n",
        "            bootstrap_results = self.bootstrap_shap_importance(shap_values)\n",
        "            mapping = self._get_short_names_map()\n",
        "            X_display = X.rename(columns=mapping)\n",
        "            display_names = list(X_display.columns)\n",
        "\n",
        "            pd.DataFrame(shap_values, columns=X.columns, index=X.index).to_csv(\n",
        "                Path(self.config.output_dir) / \"tables\" / \"shap_values_raw.csv\")\n",
        "\n",
        "            self.plot_bootstrap_importance(bootstrap_results, display_names)\n",
        "            self.plot_shap_summary(shap_values, X_display)\n",
        "            self.save_force_plots(shap_values, base_ev, X_display, y)\n",
        "\n",
        "            logger.info(f\"Analysis complete! Results saved to {self.config.output_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {e}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    config = SHAPConfig(\n",
        "        model_bundle_path=\"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\",\n",
        "        test_data_path=\"test_set.csv\",\n",
        "        output_dir=\"shap_analysis_results\",\n",
        "        n_bootstrap=200,\n",
        "        top_features=10,\n",
        "        n_force_plots=5,\n",
        "        cache_shap=True,\n",
        "        plot_style=\"seaborn-v0_8-whitegrid\"\n",
        "    )\n",
        "\n",
        "    analyzer = SHAPAnalyzer(config)\n",
        "    analyzer.run_analysis()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "02i3Eva-4cvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Model Interpretability: LIME Analysis**  \n",
        "Use LIME to provide local explanations of individual predictions and validate model behavior."
      ],
      "metadata": {
        "id": "PxV8645Q5VM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "MODEL_BUNDLE_PATH = \"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\"\n",
        "TEST_DATA_PATH = \"test_set.csv\"\n",
        "OUTPUT_DIR = \"lime_explanations\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"DejaVu Sans\",\n",
        "    \"font.weight\": \"normal\",\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"figure.dpi\": 600,\n",
        "    \"axes.linewidth\": 1.0,\n",
        "})\n",
        "\n",
        "\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(TEST_DATA_PATH)\n",
        "if \"MDR status\" not in df.columns:\n",
        "    raise ValueError(\"'MDR status' column missing in test data.\")\n",
        "X_raw = df.drop(columns=[\"MDR status\"])\n",
        "y_test = df[\"MDR status\"].astype(int)\n",
        "\n",
        "print(\"Loading model bundle...\")\n",
        "bundle = joblib.load(MODEL_BUNDLE_PATH)\n",
        "model = bundle[\"model\"]\n",
        "preprocessor = bundle.get(\"preprocessor\")\n",
        "selected_features = bundle.get(\"selected_features\")\n",
        "\n",
        "def get_onehot_feature_names_from_ct(ct):\n",
        "    names = []\n",
        "    for nm, pipe, cols in ct.transformers_:\n",
        "        if nm == \"remainder\" and pipe == \"drop\": continue\n",
        "        if nm == \"cat\" and hasattr(pipe, \"named_steps\") and \"onehot\" in pipe.named_steps:\n",
        "            cats = pipe.named_steps[\"onehot\"].categories_\n",
        "            for col, cat_list in zip(cols, cats):\n",
        "                names += [f\"{col}_{cat}\" for cat in cat_list]\n",
        "        else: names += list(cols)\n",
        "    return names\n",
        "\n",
        "X_trans = preprocessor.transform(X_raw)\n",
        "all_feat_names = get_onehot_feature_names_from_ct(preprocessor)\n",
        "X_full = pd.DataFrame(X_trans, columns=all_feat_names, index=X_raw.index)\n",
        "X_use = X_full[selected_features].copy().astype(np.float64)\n",
        "feature_names = list(X_use.columns)\n",
        "\n",
        "rename_map = {\n",
        "    \"Institution Type_Hospital\": \"Setting: Hospital\",\n",
        "    \"Institution Type_Lab\": \"Setting: Lab\",\n",
        "    \"Gender_M\": \"Male\", \"Gender_F\": \"Female\",\n",
        "    \"Bacteria type_E. coli\": \"Pathogen: E. coli\",\n",
        "    \"Bacteria type_Klebsiella Spp\": \"Pathogen: Klebsiella\",\n",
        "    \"Bacteria type_Pseudomonas Spp\": \"Pathogen: Pseudomonas\",\n",
        "    \"Healthcare Sector_Governmental\": \"Sector: Gov\",\n",
        "    \"Healthcare Sector_Private\": \"Sector: Private\",\n",
        "}\n",
        "\n",
        "display_names = []\n",
        "for n in feature_names:\n",
        "    clean_n = str(n)\n",
        "    if clean_n in rename_map: clean_n = rename_map[clean_n]\n",
        "    display_names.append(clean_n)\n",
        "\n",
        "_seen, dedup_names = {}, []\n",
        "for n in display_names:\n",
        "    if n in _seen:\n",
        "        _seen[n] += 1\n",
        "        dedup_names.append(f\"{n}_{_seen[n]}\")\n",
        "    else:\n",
        "        _seen[n] = 0\n",
        "        dedup_names.append(n)\n",
        "display_names = dedup_names\n",
        "\n",
        "def predict_fn(X_array):\n",
        "    X_arr = np.asarray(X_array, dtype=np.float64)\n",
        "    if X_arr.ndim == 1: X_arr = X_arr.reshape(1, -1)\n",
        "    try: probs = model.predict_proba(X_arr)\n",
        "    except:\n",
        "        X_df = pd.DataFrame(X_arr, columns=feature_names)\n",
        "        probs = model.predict_proba(X_df)\n",
        "\n",
        "    probs = np.asarray(probs, dtype=np.float64)\n",
        "    if probs.ndim == 1: probs = probs.reshape(-1, 1)\n",
        "    if probs.shape[1] == 1:\n",
        "        p1 = probs[:, 0]\n",
        "        probs = np.column_stack([1.0 - p1, p1])\n",
        "    return probs\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_use.to_numpy(dtype=np.float64),\n",
        "    feature_names=display_names,\n",
        "    class_names=['Non-MDR', 'MDR'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_indices = []\n",
        "for cls in [0, 1]:\n",
        "    idxs = np.where(y_test.values == cls)[0]\n",
        "    k = min(3, len(idxs))\n",
        "    if k > 0: sample_indices.extend(np.random.choice(idxs, size=k, replace=False))\n",
        "\n",
        "sample_indices = list(dict.fromkeys(sample_indices))\n",
        "\n",
        "for idx in sample_indices:\n",
        "    try:\n",
        "        x_row = X_use.iloc[idx].to_numpy(dtype=np.float64)\n",
        "        exp = explainer.explain_instance(\n",
        "            data_row=x_row,\n",
        "            predict_fn=predict_fn,\n",
        "            num_features=min(10, X_use.shape[1]),\n",
        "            top_labels=1\n",
        "        )\n",
        "        top_label = int(np.asarray(exp.top_labels))\n",
        "\n",
        "\n",
        "        fig = exp.as_pyplot_figure(label=top_label)\n",
        "\n",
        "        fig.set_size_inches(8, 5)\n",
        "\n",
        "        plt.title(f\"Sample {idx} (True: {'MDR' if int(y_test.iloc[idx]) == 1 else 'Non-MDR'})\",\n",
        "                  fontsize=14, pad=15, weight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        png_path = os.path.join(OUTPUT_DIR, f\"sample_{idx}.png\")\n",
        "        plt.savefig(png_path, dpi=600, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        print(f\"âœ… Saved Clear LIME plot: {png_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to explain sample {idx}: {str(e)}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ LIME analysis completed!\")"
      ],
      "metadata": {
        "id": "PaCb9oGH5V7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. SHAP Analysis on LIME-selected Samples**  \n",
        "Perform SHAP analysis specifically on cases identified by LIME for deeper interpretability."
      ],
      "metadata": {
        "id": "mS_XOYa75hof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "MODEL_BUNDLE_PATH = \"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\"\n",
        "TEST_DATA_PATH = \"test_set.csv\"\n",
        "\n",
        "RENAME_MAP = {\n",
        "    \"Institution Type_Hospital\": \"Setting: Hospital\",\n",
        "    \"Institution Type_Lab\": \"Setting: Lab\",\n",
        "    \"Gender_M\": \"Male\",\n",
        "    \"Gender_F\": \"Female\",\n",
        "    \"Bacteria type_E. coli\": \"Pathogen: E. coli\",\n",
        "    \"Bacteria type_Klebsiella Spp\": \"Pathogen: Klebsiella\",\n",
        "    \"Bacteria type_Pseudomonas Spp\": \"Pathogen: Pseudomonas\",\n",
        "    \"Healthcare Sector_Governmental\": \"Sector: Gov\",\n",
        "    \"Healthcare Sector_Private\": \"Sector: Private\",\n",
        "}\n",
        "\n",
        "def get_onehot_feature_names_from_ct(ct):\n",
        "    names = []\n",
        "    for nm, pipe, cols in ct.transformers_:\n",
        "        if nm == \"remainder\" and pipe == \"drop\":\n",
        "            continue\n",
        "        if nm == \"cat\" and hasattr(pipe, \"named_steps\") and \"onehot\" in pipe.named_steps:\n",
        "            cats = pipe.named_steps[\"onehot\"].categories_\n",
        "            for col, cat_list in zip(cols, cats):\n",
        "                names += [f\"{col}_{cat}\" for cat in cat_list]\n",
        "        else:\n",
        "            names += list(cols)\n",
        "    return names\n",
        "\n",
        "def extract_base_model(model):\n",
        "    if isinstance(model, CalibratedClassifierCV):\n",
        "        return model.calibrated_classifiers_[0].estimator\n",
        "    return model\n",
        "\n",
        "def compute_shap_values(model, X_df):\n",
        "    base_model = extract_base_model(model)\n",
        "    explainer = shap.TreeExplainer(base_model)\n",
        "    shap_values = explainer.shap_values(X_df)\n",
        "    expected_value = explainer.expected_value\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        if len(shap_values) == 2:\n",
        "            shap_values = shap_values[1]\n",
        "            if isinstance(expected_value, (list, tuple, np.ndarray)):\n",
        "                expected_value = expected_value[1]\n",
        "        else:\n",
        "            shap_values = shap_values if len(shap_values) > 0 else shap_values\n",
        "\n",
        "    shap_values = np.asarray(shap_values)\n",
        "    expected_value = float(np.asarray(expected_value))\n",
        "    return shap_values, expected_value\n",
        "\n",
        "def generate_shap_for_lime_samples(\n",
        "    bundle_path=MODEL_BUNDLE_PATH,\n",
        "    test_path=TEST_DATA_PATH,\n",
        "    lime_sample_indices=None,\n",
        "    output_dir=\"explanations\"\n",
        "):\n",
        "\n",
        "    if lime_sample_indices is None:\n",
        "        lime_sample_indices = []\n",
        "\n",
        "    df = pd.read_csv(test_path)\n",
        "    if \"MDR status\" not in df.columns:\n",
        "        raise ValueError(\"'MDR status' column missing from test_set.csv.\")\n",
        "    X_raw = df.drop(columns=[\"MDR status\"])\n",
        "\n",
        "    bundle = joblib.load(bundle_path)\n",
        "    model = bundle[\"model\"]\n",
        "    pre = bundle.get(\"preprocessor\")\n",
        "    sel_feats = bundle.get(\"selected_features\")\n",
        "\n",
        "    if pre is None or sel_feats is None:\n",
        "        raise KeyError(\"Bundle missing 'preprocessor' or 'selected_features'\")\n",
        "\n",
        "    X_trans = pre.transform(X_raw)\n",
        "    all_names = get_onehot_feature_names_from_ct(pre)\n",
        "    X_full = pd.DataFrame(X_trans, columns=all_names, index=X_raw.index)\n",
        "\n",
        "    missing = [f for f in sel_feats if f not in X_full.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing transformed columns: {missing}\")\n",
        "\n",
        "    X = X_full[sel_feats].copy()\n",
        "    short_feature_names = [RENAME_MAP.get(col, col) for col in X.columns]\n",
        "    shap_values, expected_value = compute_shap_values(model, X)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.family\": \"DejaVu Sans\",\n",
        "        \"font.weight\": \"normal\",\n",
        "        \"font.size\": 12,\n",
        "        \"axes.titlesize\": 14,\n",
        "        \"axes.labelsize\": 12,\n",
        "        \"xtick.labelsize\": 11,\n",
        "        \"ytick.labelsize\": 11,\n",
        "        \"figure.dpi\": 600,\n",
        "        \"axes.linewidth\": 1.0,\n",
        "    })\n",
        "\n",
        "    for i, sample_idx in enumerate(lime_sample_indices):\n",
        "        try:\n",
        "            row_X = X.loc[sample_idx] if sample_idx in X.index else X.iloc[sample_idx]\n",
        "            pos = X.index.get_loc(sample_idx) if sample_idx in X.index else sample_idx\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "\n",
        "            shap.plots.waterfall(\n",
        "                shap.Explanation(\n",
        "                    values=shap_values[pos],\n",
        "                    base_values=expected_value,\n",
        "                    data=row_X,\n",
        "                    feature_names=short_feature_names\n",
        "                ),\n",
        "                show=False,\n",
        "                max_display=10\n",
        "            )\n",
        "\n",
        "            plt.title(f\"SHAP for sample {sample_idx}\", fontsize=16, pad=20, weight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            png_path = os.path.join(output_dir, f\"shap_sample_{sample_idx}.png\")\n",
        "\n",
        "\n",
        "            plt.savefig(png_path, dpi=600, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"âœ… Saved High-Res waterfall plot: {png_path}\")\n",
        "\n",
        "\n",
        "            force_plot = shap.plots.force(\n",
        "                expected_value,\n",
        "                shap_values[pos],\n",
        "                row_X,\n",
        "                feature_names=short_feature_names,\n",
        "                matplotlib=False\n",
        "            )\n",
        "            html_path = os.path.join(output_dir, f\"shap_force_{sample_idx}.html\")\n",
        "            shap.save_html(html_path, force_plot)\n",
        "            print(f\"âœ… Saved force plot: {html_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing sample {sample_idx}: {e}\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ SHAP analysis complete for all LIME samples\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    lime_sample_indices = [9, 103, 46, 28, 252, 322]\n",
        "    generate_shap_for_lime_samples(\n",
        "        bundle_path=MODEL_BUNDLE_PATH,\n",
        "        test_path=TEST_DATA_PATH,\n",
        "        lime_sample_indices=lime_sample_indices,\n",
        "        output_dir=\"explanations\"\n",
        "    )"
      ],
      "metadata": {
        "id": "fFYkZx-25l4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. Subgroup / Fairness Performance Analysis**  \n",
        "Evaluate model performance across patient subgroups to assess fairness and potential biases."
      ],
      "metadata": {
        "id": "fQqElxCk5uA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "from itertools import combinations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, precision_score, recall_score, f1_score,\n",
        "    accuracy_score, average_precision_score\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "log_filename = f'subgroup_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_filename),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 20,\n",
        "    'axes.labelsize': 16,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14,\n",
        "    'figure.dpi': 600,\n",
        "    'lines.linewidth': 2,\n",
        "    'lines.markersize': 8,\n",
        "    'savefig.bbox': 'tight'\n",
        "})\n",
        "# ----------------------------------------\n",
        "\n",
        "class HybridFeatureSelector:\n",
        "    def __init__(self, selected_features=None):\n",
        "        self.selected_features_ = selected_features if selected_features is not None else []\n",
        "    def fit(self, X, y=None): return self\n",
        "    def transform(self, X): return X[self.selected_features_]\n",
        "    def fit_transform(self, X, y=None): return self.fit(X, y).transform(X)\n",
        "\n",
        "def get_feature_names_from_column_transformer(ct):\n",
        "    names = []\n",
        "    for nm, pipe, cols in ct.transformers_:\n",
        "        if nm == 'remainder' and pipe == 'drop': continue\n",
        "        if nm == 'cat' and hasattr(pipe, 'named_steps') and 'onehot' in pipe.named_steps:\n",
        "            cats = pipe.named_steps['onehot'].categories_\n",
        "            for c, cl in zip(cols, cats): names += [f\"{c}_{v}\" for v in cl]\n",
        "        else: names += list(cols)\n",
        "    return names\n",
        "\n",
        "def load_group_specific_thresholds(filepath):\n",
        "    logging.info(f\"ðŸ“‹ Loading group-specific thresholds from {filepath}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        group_thresholds = {\n",
        "            row['group'].strip(): row['youden_j']\n",
        "            for _, row in df.iterrows()\n",
        "        }\n",
        "        logging.info(f\"âœ… Found Youden thresholds for {len(group_thresholds)} groups.\")\n",
        "        return group_thresholds\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"âš ï¸ Could not load thresholds: {e}\")\n",
        "        return {}\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    logging.info(\"ðŸ“‚ Loading test data and model...\")\n",
        "    test_set = pd.read_csv(\"test_set.csv\")\n",
        "    X_test = test_set.drop(columns=[\"MDR status\"])\n",
        "    y_true = test_set[\"MDR status\"]\n",
        "    bundle = joblib.load(\"deployment_artifacts/CatBoost_with_thresholds_v1.0.0.pkl\")\n",
        "    return test_set, X_test, y_true, bundle\n",
        "\n",
        "def preprocess_and_predict(X_test, bundle):\n",
        "    logging.info(\"ðŸ”§ Preprocessing data...\")\n",
        "    model = bundle['model']\n",
        "    preprocessor = bundle.get('preprocessor')\n",
        "    selected_features = bundle.get('selected_features')\n",
        "\n",
        "    X_processed = preprocessor.transform(X_test)\n",
        "    all_features = get_feature_names_from_column_transformer(preprocessor)\n",
        "    X_processed_df = pd.DataFrame(X_processed, columns=all_features)\n",
        "    X_final = X_processed_df[selected_features] if selected_features else X_processed_df\n",
        "    y_proba = model.predict_proba(X_final)[:, 1]\n",
        "    return y_proba, X_processed_df\n",
        "\n",
        "def define_subgroups_from_preprocessed_data(test_data_with_probs, preprocessed_df):\n",
        "    logging.info(\"ðŸ” Defining subgroups...\")\n",
        "    combined_df = pd.concat([test_data_with_probs.reset_index(drop=True), preprocessed_df.reset_index(drop=True)], axis=1)\n",
        "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='last')]\n",
        "\n",
        "    subgroups = {'All': combined_df}\n",
        "\n",
        "    if 'Age' in combined_df.columns:\n",
        "        age_median = combined_df['Age'].median()\n",
        "        subgroups['Age < Median'] = combined_df[combined_df['Age'] < age_median]\n",
        "        subgroups['Age >= Median'] = combined_df[combined_df['Age'] >= age_median]\n",
        "\n",
        "    categorical_prefixes = ['Gender', 'Institution Type', 'Healthcare Sector', 'Bacteria type']\n",
        "    non_redundant_cols = [\"_F\", \"_Lab\", \"_Private\"]\n",
        "\n",
        "    for prefix in categorical_prefixes:\n",
        "        cols = [col for col in combined_df.columns if col.startswith(prefix) and combined_df[col].isin([0, 1]).all()]\n",
        "        for col in cols:\n",
        "            if prefix == 'Bacteria type' or any(sub in col for sub in non_redundant_cols):\n",
        "                if \"Gender_F\" in col: name_1, name_0 = \"Female\", \"Male\"\n",
        "                elif \"Institution Type_Lab\" in col: name_1, name_0 = \"Setting: Lab\", \"Setting: Hospital\"\n",
        "                elif \"Healthcare Sector_Private\" in col: name_1, name_0 = \"Sector: Private\", \"Sector: Gov\"\n",
        "                elif \"Bacteria type\" in col:\n",
        "                    clean_bac = col.replace(\"Bacteria type_\", \"\").replace(\" Spp\", \"\")\n",
        "                    name_1, name_0 = f\"Pathogen: {clean_bac}\", f\"Non-{clean_bac}\"\n",
        "                else: name_1, name_0 = f\"{col} = 1\", f\"{col} = 0\"\n",
        "\n",
        "                subgroups[name_1] = combined_df[combined_df[col] == 1]\n",
        "                subgroups[name_0] = combined_df[combined_df[col] == 0]\n",
        "\n",
        "    return {name: group for name, group in subgroups.items() if len(group) > 0}\n",
        "\n",
        "def bootstrap_ci(y_true, y_scores, metric_fn, n_boot=1000, alpha=0.05, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    scores = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = resample(np.arange(len(y_true)), replace=True)\n",
        "        if len(np.unique(y_true[idx])) < 2 and metric_fn.__name__ in ['roc_auc_score', 'average_precision_score']: continue\n",
        "        try: scores.append(metric_fn(y_true[idx], y_scores[idx]))\n",
        "        except: continue\n",
        "    if not scores: return np.nan, np.nan, np.nan\n",
        "    return np.mean(scores), np.percentile(scores, 100 * alpha / 2), np.percentile(scores, 100 * (1 - alpha / 2))\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob, n_boot=1000):\n",
        "    if len(y_true) == 0: return {}\n",
        "    results = {}\n",
        "    y_true, y_pred, y_prob = np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
        "    metric_defs = {\"AUROC\": (roc_auc_score, y_prob), \"PR_AUC\": (average_precision_score, y_prob),\n",
        "                   \"Accuracy\": (accuracy_score, y_pred), \"F1\": (lambda yt, yp: f1_score(yt, yp, zero_division=0), y_pred),\n",
        "                   \"Recall\": (lambda yt, yp: recall_score(yt, yp, zero_division=0), y_pred)}\n",
        "    for name, (fn, scores) in metric_defs.items():\n",
        "        mean, low, high = bootstrap_ci(y_true, scores, fn, n_boot=n_boot)\n",
        "        results[name], results[f\"{name}_lowCI\"], results[f\"{name}_highCI\"] = mean, low, high\n",
        "    return results\n",
        "\n",
        "def apply_thresholds_and_compute_metrics(subgroups, bundle, group_specific_thresholds):\n",
        "    logging.info(\"ðŸ“Š Computing metrics for each subgroup...\")\n",
        "    global_threshold = bundle.get('youden_j_threshold', 0.5)\n",
        "    metrics_dict, predicted_subgroups = {}, {}\n",
        "\n",
        "    for name, df in subgroups.items():\n",
        "        if df.empty: continue\n",
        "        cleaned_name = name.strip()\n",
        "        threshold = group_specific_thresholds.get(cleaned_name, global_threshold)\n",
        "\n",
        "        df = df.copy()\n",
        "        df['predicted'] = (df['prob'] >= threshold).astype(int)\n",
        "        metrics = compute_metrics(df['MDR status'], df['predicted'], df['prob'])\n",
        "        metrics.update({'Threshold': threshold, 'N_samples': len(df)})\n",
        "        metrics_dict[name] = metrics\n",
        "        predicted_subgroups[name] = df\n",
        "\n",
        "    return metrics_dict, predicted_subgroups\n",
        "\n",
        "METRIC_COLORS = {\"AUROC\": \"blue\", \"Accuracy\": \"green\", \"Recall\": \"purple\"}\n",
        "\n",
        "def _get_ci_columns(metric, df):\n",
        "    low_cols = [f\"{metric}_lowCI\", f\"{metric}_low_ci\", f\"{metric}_lo\"]\n",
        "    high_cols = [f\"{metric}_highCI\", f\"{metric}_high_ci\", f\"{metric}_hi\"]\n",
        "    low_col = next((c for c in low_cols if c in df.columns), None)\n",
        "    high_col = next((c for c in high_cols if c in df.columns), None)\n",
        "    if not low_col or not high_col: raise KeyError(f\"Missing CI columns for {metric}\")\n",
        "    return low_col, high_col\n",
        "\n",
        "def plot_subgroup_metrics(df, metrics_to_plot, output_dir, single_plots=False):\n",
        "    if df.empty: return\n",
        "    if \"Subgroup\" not in df.columns: df[\"Subgroup\"] = df.index\n",
        "    df = df.sort_values(\"Subgroup\", ascending=False)\n",
        "\n",
        "    if single_plots:\n",
        "        for metric in metrics_to_plot:\n",
        "            try:\n",
        "                lo_col, hi_col = _get_ci_columns(metric, df)\n",
        "\n",
        "                plt.figure(figsize=(14, 12))\n",
        "\n",
        "                xerr = [df[metric] - df[lo_col], df[hi_col] - df[metric]]\n",
        "                plt.errorbar(df[metric], df[\"Subgroup\"], xerr=xerr, fmt=\"o\",\n",
        "                             capsize=8, elinewidth=3, markersize=10,\n",
        "                             color=METRIC_COLORS.get(metric, 'gray'))\n",
        "\n",
        "                plt.axvline(0.5, color=\"red\", linestyle=\"--\", alpha=0.6, linewidth=2)\n",
        "                plt.title(f\"Subgroup Analysis: {metric} with 95% CI\", pad=20)\n",
        "                plt.grid(True, linestyle='--', alpha=0.5)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(output_dir / f\"subgroup_analysis_{metric}.png\")\n",
        "                plt.close()\n",
        "            except KeyError: pass\n",
        "    else:\n",
        "\n",
        "        plt.figure(figsize=(16, 14))\n",
        "\n",
        "        offsets = np.linspace(-0.25, 0.25, len(metrics_to_plot))\n",
        "        for i, metric in enumerate(metrics_to_plot):\n",
        "            try:\n",
        "                lo_col, hi_col = _get_ci_columns(metric, df)\n",
        "                plt.errorbar(df[metric], np.arange(len(df)) + offsets[i],\n",
        "                             xerr=[df[metric] - df[lo_col], df[hi_col] - df[metric]],\n",
        "                             fmt=\"o\", capsize=8, elinewidth=3, markersize=10,\n",
        "                             color=METRIC_COLORS.get(metric, 'gray'), label=metric)\n",
        "            except KeyError: pass\n",
        "\n",
        "        plt.yticks(np.arange(len(df)), df[\"Subgroup\"])\n",
        "        plt.axvline(0.5, color=\"red\", linestyle=\"--\", alpha=0.6, linewidth=2)\n",
        "        plt.title(\"Subgroup Analysis: Model Performance with 95% CI\", pad=20)\n",
        "        plt.xlabel(\"Performance Metric Value\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "\n",
        "        plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / \"subgroup_analysis_combined.png\")\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    output_dir = Path(\"subgroup_analysis_results\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    plots_dir = output_dir / \"plots\"\n",
        "    plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        threshold_file = \"Threshold_results/reports/CatBoost_subgroups_optimal_thresholds_summary.csv\"\n",
        "        group_thresholds = load_group_specific_thresholds(threshold_file)\n",
        "\n",
        "        test_set, X_test, y_true, bundle = load_and_prepare_data()\n",
        "        y_proba, X_processed = preprocess_and_predict(X_test, bundle)\n",
        "\n",
        "        test_set_with_probs = test_set.copy()\n",
        "        test_set_with_probs['prob'] = y_proba\n",
        "\n",
        "        subgroups = define_subgroups_from_preprocessed_data(test_set_with_probs, X_processed)\n",
        "        metrics_dict, predicted_subgroups = apply_thresholds_and_compute_metrics(subgroups, bundle, group_thresholds)\n",
        "\n",
        "        metrics_df = pd.DataFrame(metrics_dict).T.round(4)\n",
        "        metrics_df.to_csv(output_dir / \"subgroup_metrics_results.csv\")\n",
        "\n",
        "        plot_subgroup_metrics(metrics_df, [\"AUROC\", \"Accuracy\", \"Recall\"], plots_dir, single_plots=True)\n",
        "        plot_subgroup_metrics(metrics_df, [\"AUROC\", \"Accuracy\", \"Recall\"], plots_dir, single_plots=False)\n",
        "\n",
        "        print(\"âœ… Analysis Complete. High-resolution plots saved.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "D-S2FyQf5x_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}